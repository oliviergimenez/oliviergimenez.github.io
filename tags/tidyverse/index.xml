<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tidyverse | Olivier Gimenez</title>
    <link>https://oliviergimenez.github.io/tags/tidyverse/</link>
      <atom:link href="https://oliviergimenez.github.io/tags/tidyverse/index.xml" rel="self" type="application/rss+xml" />
    <description>tidyverse</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>¬© Olivier Gimenez 2020</copyright><lastBuildDate>Mon, 23 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://oliviergimenez.github.io/img/flyfshing.jpg</url>
      <title>tidyverse</title>
      <link>https://oliviergimenez.github.io/tags/tidyverse/</link>
    </image>
    
    <item>
      <title>Mortalit√© en France et tidyverse</title>
      <link>https://oliviergimenez.github.io/blog/procrastination_coulmont/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/procrastination_coulmont/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Procrastination... Je me suis &amp;quot;amus√©&amp;quot; √† reproduire cette g√©niale figure de &lt;a href=&#34;https://twitter.com/coulmont?ref_src=twsrc%5Etfw&#34;&gt;@coulmont&lt;/a&gt; sur le jour des d√©c√®s en üá´üá∑ en fonction de l&amp;#39;√¢ge et du temps avec &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; et le &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; &lt;a href=&#34;https://t.co/W8QNBX5WlT&#34;&gt;https://t.co/W8QNBX5WlT&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ggplot2?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ggplot2&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/dplyr?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dplyr&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/lubridate?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#lubridate&lt;/a&gt; &lt;a href=&#34;https://t.co/wOthJOBzBd&#34;&gt;https://t.co/wOthJOBzBd&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1209172238009864193_src=twsrc%5Etfw&#34;&gt;December 23, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt; </description>
    </item>
    
    <item>
      <title>Workshop on reproducible science</title>
      <link>https://oliviergimenez.github.io/blog/cesabworkshop/</link>
      <pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/cesabworkshop/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This week we host our first winter school on ¬´Reproducible Research in Numerical Ecology ¬ª co-organised by &lt;a href=&#34;https://twitter.com/hashtag/GDR_Ecostat?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#GDR_Ecostat&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/hashtag/CESAB?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#CESAB&lt;/a&gt; &lt;a href=&#34;https://twitter.com/FRBiodiv?ref_src=twsrc%5Etfw&#34;&gt;@FRBiodiv&lt;/a&gt;. Many thanks to our incredible list of speakers : Nicolas Casajus, St√©phane Dray, &lt;a href=&#34;https://twitter.com/GueryLorelei?ref_src=twsrc%5Etfw&#34;&gt;@GueryLorelei&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/oaggimenez?ref_src=twsrc%5Etfw&#34;&gt;@oaggimenez&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/FGuilhaumon?ref_src=twsrc%5Etfw&#34;&gt;@FGuilhaumon&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/NinaSchiett?ref_src=twsrc%5Etfw&#34;&gt;@NinaSchiett&lt;/a&gt; !! &lt;a href=&#34;https://t.co/lRKlVXtYym&#34;&gt;pic.twitter.com/lRKlVXtYym&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nicolas Mouquet (@NicolasMouquet) &lt;a href=&#34;https://twitter.com/NicolasMouquet/status/1201490464945389568?ref_src=twsrc%5Etfw&#34;&gt;December 2,2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt; </description>
    </item>
    
    <item>
      <title>Introduction to spatial analyses in R</title>
      <link>https://oliviergimenez.github.io/blog/intro_spatial/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/intro_spatial/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;fr&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üë©
üíªüó∫Ô∏èüë®
üíª The slides of my introduction to &lt;a href=&#34;https://twitter.com/hashtag/GIS?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#GIS&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/hashtag/mapping?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#mapping&lt;/a&gt; in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; using the &lt;a href=&#34;https://twitter.com/hashtag/sf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#sf&lt;/a&gt; üì¶ and brown üêª distribution in the &lt;a href=&#34;https://twitter.com/hashtag/pyrenees?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#pyrenees&lt;/a&gt; as a case study &lt;a href=&#34;https://t.co/SKQOCzbxHn&#34;&gt;https://t.co/SKQOCzbxHn&lt;/a&gt; - raw material on &lt;a href=&#34;https://twitter.com/hashtag/github?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#github&lt;/a&gt; &lt;a href=&#34;https://t.co/dHoMz6I2Kp&#34;&gt;https://t.co/dHoMz6I2Kp&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rspatial?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rspatial&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/spatial?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#spatial&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ggplot2?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ggplot2&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/dataviz?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dataviz&lt;/a&gt; &lt;a href=&#34;https://t.co/22eD1Y55d3&#34;&gt;pic.twitter.com/22eD1Y55d3&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1093882504560414721?ref_src=twsrc%5Etfw&#34;&gt;8 f√©vrier 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script asyncc=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to the Tidyverse</title>
      <link>https://oliviergimenez.github.io/blog/intro_tidyverse/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/intro_tidyverse/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;fr&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My introduction to the &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; for our lab meeting to manipulate and visualise data in &lt;a href=&#34;https://twitter.com/hashtag/rstat?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstat&lt;/a&gt; &lt;a href=&#34;https://t.co/As9bkXY9GZ&#34;&gt;https://t.co/As9bkXY9GZ&lt;/a&gt;. Feel free to steal and modify this material for your own use. Be advised, this is work in progress &amp;amp; a mix of üá¨üáß/üá´üá∑üòã Comments welcome! &lt;a href=&#34;https://twitter.com/hashtag/datascience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#datascience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/davaviz?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#davaviz&lt;/a&gt; &lt;a href=&#34;https://t.co/2vrrdbzuWh&#34;&gt;pic.twitter.com/2vrrdbzuWh&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1085492126404751360?ref_src=twsrc%5Etfw&#34;&gt;16 janvier 2019&lt;/a&gt;&lt;/blockote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Scientific research is all about networking</title>
      <link>https://oliviergimenez.github.io/blog/network_ecology/</link>
      <pubDate>Wed, 02 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/network_ecology/</guid>
      <description>&lt;p&gt;I read 
&lt;a href=&#34;http://coulmont.com/blog/2018/12/02/sociologue-reseau-theses-2018/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this awesome post&lt;/a&gt; (in French) by 
&lt;a href=&#34;http://coulmont.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baptiste Coulmont&lt;/a&gt;, professor in sociology, who explored the French academic network in sociology. Coulmont used the composition of PhD commitees to determine academic links between colleagues. The approach very appealing because it uses public data available from the website 
&lt;a href=&#34;www.these.fr&#34;&gt;these.fr&lt;/a&gt;. Here, I used Coulmont&amp;rsquo;s &lt;code&gt;R&lt;/code&gt; code to produce the French academic network in ecology. This was a nice opportunity to illustrate how to work in the &lt;code&gt;tidyverse&lt;/code&gt; and to do some 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Web_scraping&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web scraping&lt;/a&gt; using the &lt;code&gt;rvest&lt;/code&gt; package.&lt;/p&gt;
&lt;h2 id=&#34;get-the-data&#34;&gt;Get the data&lt;/h2&gt;
&lt;p&gt;Load the packages we need:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(RCurl)
library(tidyverse)
library(lubridate)
library(scales)
library(hrbrthemes)
library(data.table)
# devtools::install_github(&amp;quot;privefl/bigreadr&amp;quot;)
library(bigreadr)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now prepare the URL requests. The total number of PhDs is around
88000 on the period 2015-2018. Because the website uses slices of 1000 on each page, we proceed
in sequence:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;i &amp;lt;- 1:88
i &amp;lt;- i*1000
URL &amp;lt;-paste0(&amp;quot;http://www.theses.fr/?q=&amp;amp;fq=dateSoutenance:[2015-01-01T23:59:59Z%2BTO%2B2018-12-31T23:59:59Z]&amp;amp;checkedfacets=&amp;amp;start=&amp;quot;,i,&amp;quot;&amp;amp;sort=none&amp;amp;status=&amp;amp;access=&amp;amp;prevision=&amp;amp;filtrepersonne=&amp;amp;zone1=titreRAs&amp;amp;val1=&amp;amp;op1=AND&amp;amp;zone2=auteurs&amp;amp;val2=&amp;amp;op2=AND&amp;amp;zone3=etabSoutenances&amp;amp;val3=&amp;amp;zone4=dateSoutenance&amp;amp;val4a=&amp;amp;val4b=&amp;amp;type=&amp;amp;lng=&amp;amp;checkedfacets=&amp;amp;format=csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, the search can be done by hand directly from the

&lt;a href=&#34;www.theses.fr&#34;&gt;theses.fr&lt;/a&gt; website. [Fran√ßois-Xavier Coudert]
(&lt;a href=&#34;https://www.coudert.name/&#34;&gt;https://www.coudert.name/&lt;/a&gt;) also provides 
&lt;a href=&#34;https://twitter.com/fxcoudert/status/1069188451898138624&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the search results for the
2015-2018
period&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We proceed with the requests, and store everything in a csv file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;j &amp;lt;- 1
SERP &amp;lt;- 1
for(j in 1:length(URL)){ # loop over the slices
  SERP[j] &amp;lt;- getURL(URL[j])
  write.csv(SERP,&amp;quot;SERP_2.csv&amp;quot;,append=F)
}
rm(SERP,i,j,URL)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We keep only the PhDs in the field (Discipline) of ecology. This is basically the only change I have made to Coulmont&amp;rsquo;s neat code.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;theses &amp;lt;- read.csv(&amp;quot;SERP_2.csv&amp;quot;,sep=&amp;quot;;&amp;quot;,quote=&amp;quot;&amp;quot;,skip=1,stringsAsFactors = F)
#theses %&amp;gt;% 
#  pull(X..Discipline..) %&amp;gt;% 
#  unique()

ecology &amp;lt;- theses %&amp;gt;% filter(grepl(&amp;quot;ecologie&amp;quot;,X..Discipline..,ignore.case=T)) %&amp;gt;% # keep PhDs with Displine == ecologie
  filter(X..Date.de.soutenance..!=&amp;quot;&amp;quot;) %&amp;gt;% # remove PhDs with missing dates of defense
  filter(X..Statut..==&amp;quot;soutenue&amp;quot;) # keep only PhDs that have been defended
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the id of all PhDs in ecology defended during the period 2015-2018. We
will use the id to get the composition of all PhD commitees. Getting this composition
requires scraping the web page of each PhD, and to get the
ID of each PhD. For doing so, we use the &lt;code&gt;rvest&lt;/code&gt; package (see the 
&lt;a href=&#34;https://masalmon.eu/tags/rvest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;excellent posts&lt;/a&gt;
by Ma√´lle Salmon for examples).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(rvest)
identifiants &amp;lt;- ecology$X..Identifiant.de.la.these.. # get PhD ids
reseau_total &amp;lt;- data_frame(noms_jury=&amp;quot;&amp;quot;,
                           liens_jury=&amp;quot;&amp;quot;,
                           these=&amp;quot;&amp;quot;,
                           directeurs=&amp;quot;&amp;quot;,
                           liens_directeurs=&amp;quot;&amp;quot;)

for (i in 1:length(identifiants)) {
  
  # get info on current PhD
  data_theses_eco &amp;lt;- read_html( paste0(&amp;quot;http://www.theses.fr/&amp;quot;,identifiants[i]) ) 

  # get name PhD supervisor for 
  directeurs &amp;lt;- bind_cols(
    directeurs = data_theses_eco  %&amp;gt;%
      html_nodes(&amp;quot;div .donnees-ombre p&amp;quot;) %&amp;gt;%
      .[[1]] %&amp;gt;%
      html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;%
      html_text()
    ,
    liens_directeurs = data_theses_eco  %&amp;gt;%
      html_nodes(&amp;quot;div .donnees-ombre p&amp;quot;) %&amp;gt;%
      .[[1]] %&amp;gt;%
      html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;%
      html_attr(name=&amp;quot;href&amp;quot;)
  ) %&amp;gt;% mutate(  these = identifiants[i] )
  
  # get names of people in commitees
    jury &amp;lt;- bind_cols( 
    noms_jury = data_theses_eco %&amp;gt;%
      html_nodes(&amp;quot;div .donnees p a&amp;quot;) %&amp;gt;%
      html_text()
    ,
    liens_jury = data_theses_eco %&amp;gt;%
      html_nodes(&amp;quot;div .donnees p a&amp;quot;) %&amp;gt;%
      html_attr(name=&amp;quot;href&amp;quot;)
  ) %&amp;gt;% mutate(  these = identifiants[i] )
    
  # put all together
    reseau &amp;lt;- jury %&amp;gt;% left_join(directeurs,by=&amp;quot;these&amp;quot;) 
    reseau_total &amp;lt;- bind_rows(reseau_total,reseau)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;build-the-network&#34;&gt;Build the network&lt;/h2&gt;
&lt;p&gt;Load the packages we need, and the data we got at the previous step:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(igraph)
library(ggraph)
library(ggrepel)
load(&#39;reseau_total.RData&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coulmont defined a weighted link between two colleagues &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;j&lt;/em&gt; as
follows: 3 if &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;j&lt;/em&gt; are both supervisors, 2 if &lt;em&gt;i&lt;/em&gt; is a supervisor
and &lt;em&gt;j&lt;/em&gt; a PhD commitee member and 1 if both &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;j&lt;/em&gt; are PhD commitee
members. A colleague may accumulate several weights.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;directions_theses &amp;lt;- reseau_total %&amp;gt;% 
  select(these,directeurs) %&amp;gt;% 
  unique() %&amp;gt;% 
  group_by(these) %&amp;gt;% 
  mutate(N=n()) %&amp;gt;%
  filter(N==2) %&amp;gt;% # keep co-supervision w/ 2 supervisors 
  mutate(rang=rank(directeurs)) %&amp;gt;% 
  spread(key=rang,value=directeurs) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  select(nom1=`1`,nom2=`2`) %&amp;gt;% 
  mutate(poids=3)

directions_jury &amp;lt;- reseau_total %&amp;gt;% 
  select(nom1=noms_jury,nom2=directeurs) %&amp;gt;% 
  filter( nom1 != &amp;quot;&amp;quot;) %&amp;gt;%
  mutate(poids=2) %&amp;gt;%
  group_by(nom1,nom2) %&amp;gt;% 
  summarize(poids=sum(poids))

jury_jury &amp;lt;- reseau_total %&amp;gt;% 
  select(noms_jury,these) %&amp;gt;% 
  unique() %&amp;gt;% 
  filter(noms_jury!=&amp;quot;&amp;quot;)

g_j &amp;lt;-  graph_from_data_frame(jury_jury,directed=F)
V(g_j)$type &amp;lt;- V(g_j)$name %in% jury_jury$noms_jury
g_j_1 &amp;lt;- bipartite_projection(g_j,which=&amp;quot;true&amp;quot;)
jurys &amp;lt;- as_long_data_frame(g_j_1) %&amp;gt;%
  select(nom1=`ver[el[, 1], ]`, nom2=`ver2[el[, 2], ]`, poids=weight)

reseau_petit &amp;lt;- bind_rows(directions_theses,directions_jury,jurys) %&amp;gt;%
  group_by(nom1,nom2) %&amp;gt;% 
  summarize(poids=sum(poids)) # data.frame from which the network will be created
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each node in the network has a size proportional to its 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Betweenness_centrality&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;betweenness&lt;/a&gt;
score. We also determine communities using the 
&lt;a href=&#34;http://arxiv.org/abs/physics/0512106&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;walktrap
algorithm&lt;/a&gt; that will be colored differently. The width of an edge is
proportional to the strength of the link between the two corresponding
nodes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;g &amp;lt;- graph_from_data_frame(reseau_petit, directed=F) # create network from data.frame
g &amp;lt;- simplify(g,edge.attr.comb = sum)
V(g)$degres &amp;lt;-  degree(g)
V(g)$label &amp;lt;- gsub(&amp;quot;^\\S+\\s+(.+)$&amp;quot;,&amp;quot;\\1&amp;quot;,V(g)$name)
V(g)$communaute &amp;lt;- as.character(cluster_walktrap(g, steps=15)$membership) # determine communities
V(g)$closeness &amp;lt;- (5*closeness(g))^10
V(g)$btwns &amp;lt;- betweenness(g) # network metric betweeness
V(g)$eigen_centr &amp;lt;- eigen_centrality(g)$vector
g &amp;lt;- delete_edges(g, which(E(g)$poids&amp;lt;5) ) # delete edges with weight &amp;lt;= 4
V(g)$cluster_number &amp;lt;- clusters(g)$membership # to which community you belong
g &amp;lt;- induced_subgraph(g, V(g)$cluster_number== which( max(clusters(g)$csize) == clusters(g)$csize) )
E(g)$weight &amp;lt;- 1/E(g)$poids # width of edge proportional to weight
V(g)$label &amp;lt;- ifelse(V(g)$degres&amp;lt;20,&amp;quot;&amp;quot;,V(g)$label) # do not display all names
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;plot-the-network&#34;&gt;Plot the network&lt;/h2&gt;
&lt;p&gt;We now plot the network. For clarity, we only indicate the names of
colleagues who were part of several phD commitees.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ggraph(g,layout=&amp;quot;igraph&amp;quot;,algorithm=&amp;quot;fr&amp;quot;) + 
  geom_edge_link(aes(width=.1*poids), alpha=.1, 
                 end_cap = circle(5, &#39;mm&#39;), 
                 start_cap = circle(5, &#39;mm&#39;)) +
  geom_node_point(aes(size=eigen_centr), color=&amp;quot;white&amp;quot;,alpha=1) +
  geom_node_point(aes(color=communaute,size=eigen_centr), alpha=.5) +
  scale_size_area(max_size = 20) +
  geom_node_text(aes(label=label),size=3,repel=T,box.padding = 0.15) +
  labs(title=&amp;quot;R√©seaux des √©cologues&amp;quot;,
       subtitle=&amp;quot;Soutenances de th√®ses entre 2015 et 2018&amp;quot;,
       caption=&amp;quot;Sources : theses.fr \n Code par B. Coulmont, modifi√© par O. Gimenez&amp;quot;) +
  theme_graph(foreground = &#39;white&#39;, fg_text_colour = &#39;white&#39;,
              base_family = &amp;quot;Helvetica&amp;quot;) +
  theme(legend.position=&amp;quot;none&amp;quot;,
        text=element_text(size=16,family=&amp;quot;Helvetica&amp;quot;),
        plot.margin = unit(c(0.2, 0.2, 0.2, 0.2), units=&amp;quot;line&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/ecolnetwork.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# save
ggsave(filename = &amp;quot;ecology_network.pdf&amp;quot;,width=30,height = 20)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I played around the defaults Coulmont used to build and plot the network. It helps in getting a better understanding of the network and the links between colleagues working in ecology. Overall, I indeed feel very much connected to my colleagues in Montpellier, Lyon and Grenoble. I should probably go out of my comfort zone and interact even more with my colleagues from La Rochelle, Marseille and Aix-en-Provence üòÉ&lt;/p&gt;
&lt;p&gt;As always, data and code are available from 
&lt;a href=&#34;https://github.com/oliviergimenez/phd-in-ecology-network/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
