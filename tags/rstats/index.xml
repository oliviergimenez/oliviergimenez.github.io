<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>rstats | Olivier Gimenez</title><link>https://oliviergimenez.github.io/tags/rstats/</link><atom:link href="https://oliviergimenez.github.io/tags/rstats/index.xml" rel="self" type="application/rss+xml"/><description>rstats</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Olivier Gimenez 2021</copyright><lastBuildDate>Mon, 20 Sep 2021 00:00:00 +0000</lastBuildDate><image><url>https://oliviergimenez.github.io/img/flyfishing.jpg</url><title>rstats</title><link>https://oliviergimenez.github.io/tags/rstats/</link></image><item><title>Draft chapter on Bayes stats and MCMC in R</title><link>https://oliviergimenez.github.io/blog/draft-bayesmcmc/</link><pubDate>Mon, 20 Sep 2021 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/draft-bayesmcmc/</guid><description>&lt;p>I have a draft chapter on Bayes stats and MCMC at &lt;a href="https://oliviergimenez.github.io/banana-book/crashcourse.html">https://oliviergimenez.github.io/banana-book/crashcourse.html&lt;/a> I&amp;rsquo;d love your feedback about what is confusing and what is missing ðŸ˜‡ #rstats&lt;/p></description></item><item><title>Bayesian analysis of capture-recapture data with hidden Markov models - Theory and case studies in R</title><link>https://oliviergimenez.github.io/blog/banana-book/</link><pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/banana-book/</guid><description>&lt;p>So, I&amp;rsquo;m writing a book ðŸ¤¯ It''&amp;rsquo;s called &amp;ldquo;Bayesian analysis of capture-recapture data with hidden Markov models - Theory and case studies in R&amp;rdquo;.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">So, I&amp;#39;m writing a book ðŸ¤¯ It&amp;#39;s called &amp;#39;Bayesian analysis of capture-recapture data w hidden Markov models - Theory and case studies in R&amp;#39;. Online version here &lt;a href="https://t.co/Crgx5NX70s">https://t.co/Crgx5NX70s&lt;/a> to be published w &lt;a href="https://twitter.com/CRCPress?ref_src=twsrc%5Etfw">@CRCPress&lt;/a> Will share chapter drafts ðŸ˜‡ &amp;amp; random thoughts along the way &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> &lt;a href="https://t.co/ms6dIjvY7m">pic.twitter.com/ms6dIjvY7m&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1439664049776644101?ref_src=twsrc%5Etfw">September 19, 2021&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Experimenting with machine learning in R with tidymodels and the Kaggle titanic dataset</title><link>https://oliviergimenez.github.io/blog/learning-machine-learning/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/learning-machine-learning/</guid><description>&lt;p>I would like to familiarize myself with machine learning (ML) techniques in &lt;code>R&lt;/code>. So I have been reading and learning by doing. I thought I&amp;rsquo;d share my experience for others who&amp;rsquo;d like to give it a try&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h1 id="first-version-august-13-2021-updated-august-23-2021">First version August 13, 2021, updated August 23, 2021&lt;/h1>
&lt;p>Since my first post, Iâ€™ve been reading notebooks shared by folks who
ranked high in the challenge, and added two features that they used.
Eventually, these new predictors did not help (I must be doing something
wrong). I also explored some other ML algorithms. Last, I tuned the
parameters more efficiently with a clever grid-search algorithm. All in
all, I slightly improved my score, but most importantly, I now have a
clean template for further use.&lt;/p>
&lt;h1 id="motivation">Motivation&lt;/h1>
&lt;p>All material available from GitHub at
&lt;a href="https://github.com/oliviergimenez/learning-machine-learning">https://github.com/oliviergimenez/learning-machine-learning&lt;/a>.&lt;/p>
&lt;p>The two great books Iâ€™m using are:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>
&lt;a href="https://www.statlearning.com/" target="_blank" rel="noopener">An Introduction to Statistical Learning with Applications in
R&lt;/a> by Gareth James, Daniela Witten,
Trevor Hastie and Robert Tibshirani&lt;/p>
&lt;/li>
&lt;li>
&lt;p>
&lt;a href="https://www.tmwr.org/" target="_blank" rel="noopener">Tidy models in R&lt;/a> by Max Kuhn and Julia
Silge&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>I also recommend checking out the material (codes, screencasts) shared
by
&lt;a href="http://varianceexplained.org/r/sliced-ml/" target="_blank" rel="noopener">David Robinson&lt;/a> and
&lt;a href="https://juliasilge.com/" target="_blank" rel="noopener">Julia Silge&lt;/a> from whom I picked some useful
tricks that I put to use below.&lt;/p>
&lt;p>To try things, Iâ€™ve joined the
&lt;a href="https://en.wikipedia.org/wiki/Kaggle" target="_blank" rel="noopener">Kaggle&lt;/a> online community which
gathers folks with lots of experience in ML from whom you can learn.
Kaggle also hosts public datasets that can be used for playing around.&lt;/p>
&lt;p>I use the &lt;code>tidymodels&lt;/code> metapackage that contains a suite of packages for
modeling and machine learning using &lt;code>tidyverse&lt;/code> principles. Check out
all possibilities
&lt;a href="https://www.tidymodels.org/find/" target="_blank" rel="noopener">here&lt;/a>, and parsnip
models in particular
&lt;a href="https://www.tidymodels.org/find/parsnip/" target="_blank" rel="noopener">there&lt;/a>.&lt;/p>
&lt;p>Letâ€™s start with the famous
&lt;a href="https://www.kaggle.com/c/titanic/overview" target="_blank" rel="noopener">Titanic
dataset&lt;/a>. We need to predict
if a passenger survived the sinking of the Titanic (1) or not (0). A
dataset is provided for training our models (train.csv). Another dataset
is provided (test.csv) for which we do not know the answer. We will
predict survival for each passenger, submit our answer to Kaggle and see
how well we did compared to other folks. The metric for comparison is
the percentage of passengers we correctly predict â€“ aka as accuracy.&lt;/p>
&lt;p>First things first, letâ€™s load some packages to get us started.&lt;/p>
&lt;pre>&lt;code>library(tidymodels) # metapackage for ML
library(tidyverse) # metapackage for data manipulation and visulaisation
library(stacks) # stack ML models for better perfomance
theme_set(theme_light())
doParallel::registerDoParallel(cores = 4) # parallel computations
&lt;/code>&lt;/pre>
&lt;h1 id="data">Data&lt;/h1>
&lt;p>Read in training data.&lt;/p>
&lt;pre>&lt;code>rawdata &amp;lt;- read_csv(&amp;quot;dat/titanic/train.csv&amp;quot;)
glimpse(rawdata)
## Rows: 891
## Columns: 12
## $ PassengerId &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20â€¦
## $ Survived &amp;lt;dbl&amp;gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, â€¦
## $ Pclass &amp;lt;dbl&amp;gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, â€¦
## $ Name &amp;lt;chr&amp;gt; &amp;quot;Braund, Mr. Owen Harris&amp;quot;, &amp;quot;Cumings, Mrs. John Bradley (Florence Brigâ€¦
## $ Sex &amp;lt;chr&amp;gt; &amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;male&amp;quot;,â€¦
## $ Age &amp;lt;dbl&amp;gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55, 2, NA, â€¦
## $ SibSp &amp;lt;dbl&amp;gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, â€¦
## $ Parch &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, â€¦
## $ Ticket &amp;lt;chr&amp;gt; &amp;quot;A/5 21171&amp;quot;, &amp;quot;PC 17599&amp;quot;, &amp;quot;STON/O2. 3101282&amp;quot;, &amp;quot;113803&amp;quot;, &amp;quot;373450&amp;quot;, &amp;quot;330â€¦
## $ Fare &amp;lt;dbl&amp;gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 1â€¦
## $ Cabin &amp;lt;chr&amp;gt; NA, &amp;quot;C85&amp;quot;, NA, &amp;quot;C123&amp;quot;, NA, NA, &amp;quot;E46&amp;quot;, NA, NA, NA, &amp;quot;G6&amp;quot;, &amp;quot;C103&amp;quot;, NA, Nâ€¦
## $ Embarked &amp;lt;chr&amp;gt; &amp;quot;S&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;Q&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;,â€¦
naniar::miss_var_summary(rawdata)
## # A tibble: 12 Ã— 3
## variable n_miss pct_miss
## &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Cabin 687 77.1
## 2 Age 177 19.9
## 3 Embarked 2 0.224
## 4 PassengerId 0 0
## 5 Survived 0 0
## 6 Pclass 0 0
## 7 Name 0 0
## 8 Sex 0 0
## 9 SibSp 0 0
## 10 Parch 0 0
## 11 Ticket 0 0
## 12 Fare 0 0
&lt;/code>&lt;/pre>
&lt;p>After some data exploration (not shown), I decided to take care of
missing values, gather the two family variables in a single variable,
and create a variable title.&lt;/p>
&lt;pre>&lt;code># Get most frequent port of embarkation
uniqx &amp;lt;- unique(na.omit(rawdata$Embarked))
mode_embarked &amp;lt;- as.character(fct_drop(uniqx[which.max(tabulate(match(rawdata$Embarked, uniqx)))]))
# Build function for data cleaning and handling NAs
process_data &amp;lt;- function(tbl){
tbl %&amp;gt;%
mutate(class = case_when(Pclass == 1 ~ &amp;quot;first&amp;quot;,
Pclass == 2 ~ &amp;quot;second&amp;quot;,
Pclass == 3 ~ &amp;quot;third&amp;quot;),
class = as_factor(class),
gender = factor(Sex),
fare = Fare,
age = Age,
ticket = Ticket,
alone = if_else(SibSp + Parch == 0, &amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;), # alone variable
alone = as_factor(alone),
port = factor(Embarked), # rename embarked as port
title = str_extract(Name, &amp;quot;[A-Za-z]+\\.&amp;quot;), # title variable
title = fct_lump(title, 4)) %&amp;gt;% # keep only most frequent levels of title
mutate(port = ifelse(is.na(port), mode_embarked, port), # deal w/ NAs in port (replace by mode)
port = as_factor(port)) %&amp;gt;%
group_by(title) %&amp;gt;%
mutate(median_age_title = median(age, na.rm = T)) %&amp;gt;%
ungroup() %&amp;gt;%
mutate(age = if_else(is.na(age), median_age_title, age)) %&amp;gt;% # deal w/ NAs in age (replace by median in title)
mutate(ticketfreq = ave(1:nrow(.), FUN = length),
fareadjusted = fare / ticketfreq) %&amp;gt;%
mutate(familyage = SibSp + Parch + 1 + age/70)
}
# Process the data
dataset &amp;lt;- rawdata %&amp;gt;%
process_data() %&amp;gt;%
mutate(survived = as_factor(if_else(Survived == 1, &amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;))) %&amp;gt;%
mutate(survived = relevel(survived, ref = &amp;quot;yes&amp;quot;)) %&amp;gt;% # first event is survived = yes
select(survived, class, gender, age, alone, port, title, fareadjusted, familyage)
# Have a look again
glimpse(dataset)
## Rows: 891
## Columns: 9
## $ survived &amp;lt;fct&amp;gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yâ€¦
## $ class &amp;lt;fct&amp;gt; third, first, third, first, third, third, first, third, third, seconâ€¦
## $ gender &amp;lt;fct&amp;gt; male, female, female, female, male, male, male, male, female, femaleâ€¦
## $ age &amp;lt;dbl&amp;gt; 22, 38, 26, 35, 35, 30, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55, 2, 30,â€¦
## $ alone &amp;lt;fct&amp;gt; no, no, yes, no, yes, yes, yes, no, no, no, no, yes, yes, no, yes, yâ€¦
## $ port &amp;lt;fct&amp;gt; 3, 1, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3, 3, 2,â€¦
## $ title &amp;lt;fct&amp;gt; Mr., Mrs., Miss., Mrs., Mr., Mr., Mr., Master., Mrs., Mrs., Miss., Mâ€¦
## $ fareadjusted &amp;lt;dbl&amp;gt; 0.008136925, 0.080003704, 0.008894501, 0.059595960, 0.009034792, 0.0â€¦
## $ familyage &amp;lt;dbl&amp;gt; 2.314286, 2.542857, 1.371429, 2.500000, 1.500000, 1.428571, 1.771429â€¦
naniar::miss_var_summary(dataset)
## # A tibble: 9 Ã— 3
## variable n_miss pct_miss
## &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 survived 0 0
## 2 class 0 0
## 3 gender 0 0
## 4 age 0 0
## 5 alone 0 0
## 6 port 0 0
## 7 title 0 0
## 8 fareadjusted 0 0
## 9 familyage 0 0
&lt;/code>&lt;/pre>
&lt;p>Letâ€™s apply the same treatment to the test dataset.&lt;/p>
&lt;pre>&lt;code>rawdata &amp;lt;- read_csv(&amp;quot;dat/titanic/test.csv&amp;quot;)
holdout &amp;lt;- rawdata %&amp;gt;%
process_data() %&amp;gt;%
select(PassengerId, class, gender, age, alone, port, title, fareadjusted, familyage)
glimpse(holdout)
## Rows: 418
## Columns: 9
## $ PassengerId &amp;lt;dbl&amp;gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905â€¦
## $ class &amp;lt;fct&amp;gt; third, third, second, third, third, third, third, second, third, thiâ€¦
## $ gender &amp;lt;fct&amp;gt; male, female, male, male, female, male, female, male, female, male, â€¦
## $ age &amp;lt;dbl&amp;gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0, 28.5, 46â€¦
## $ alone &amp;lt;fct&amp;gt; yes, no, yes, yes, no, yes, yes, no, yes, no, yes, yes, no, no, no, â€¦
## $ port &amp;lt;fct&amp;gt; 2, 3, 2, 3, 3, 3, 2, 3, 1, 3, 3, 3, 3, 3, 3, 1, 2, 1, 3, 1, 1, 3, 3,â€¦
## $ title &amp;lt;fct&amp;gt; Mr., Mrs., Mr., Mr., Mrs., Mr., Miss., Mr., Mrs., Mr., Mr., Mr., Mrsâ€¦
## $ fareadjusted &amp;lt;dbl&amp;gt; 0.018730144, 0.016746411, 0.023175837, 0.020723684, 0.029395933, 0.0â€¦
## $ familyage &amp;lt;dbl&amp;gt; 1.492857, 2.671429, 1.885714, 1.385714, 3.314286, 1.200000, 1.428571â€¦
naniar::miss_var_summary(holdout)
## # A tibble: 9 Ã— 3
## variable n_miss pct_miss
## &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 fareadjusted 1 0.239
## 2 PassengerId 0 0
## 3 class 0 0
## 4 gender 0 0
## 5 age 0 0
## 6 alone 0 0
## 7 port 0 0
## 8 title 0 0
## 9 familyage 0 0
&lt;/code>&lt;/pre>
&lt;h1 id="exploratory-data-analysis">Exploratory data analysis&lt;/h1>
&lt;pre>&lt;code>skimr::skim(dataset)
&lt;/code>&lt;/pre>
&lt;table>
&lt;caption>Data summary&lt;/caption>
&lt;tbody>
&lt;tr class="odd">
&lt;td style="text-align: left;">Name&lt;/td>
&lt;td style="text-align: left;">dataset&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td style="text-align: left;">Number of rows&lt;/td>
&lt;td style="text-align: left;">891&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td style="text-align: left;">Number of columns&lt;/td>
&lt;td style="text-align: left;">9&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td style="text-align: left;">_______________________&lt;/td>
&lt;td style="text-align: left;">&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td style="text-align: left;">Column type frequency:&lt;/td>
&lt;td style="text-align: left;">&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td style="text-align: left;">factor&lt;/td>
&lt;td style="text-align: left;">6&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td style="text-align: left;">numeric&lt;/td>
&lt;td style="text-align: left;">3&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td style="text-align: left;">________________________&lt;/td>
&lt;td style="text-align: left;">&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td style="text-align: left;">Group variables&lt;/td>
&lt;td style="text-align: left;">None&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Data summary&lt;/p>
&lt;p>&lt;strong>Variable type: factor&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th style="text-align: left;">skim_variable&lt;/th>
&lt;th style="text-align: right;">n_missing&lt;/th>
&lt;th style="text-align: right;">complete_rate&lt;/th>
&lt;th style="text-align: left;">ordered&lt;/th>
&lt;th style="text-align: right;">n_unique&lt;/th>
&lt;th style="text-align: left;">top_counts&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td style="text-align: left;">survived&lt;/td>
&lt;td style="text-align: right;">0&lt;/td>
&lt;td style="text-align: right;">1&lt;/td>
&lt;td style="text-align: left;">FALSE&lt;/td>
&lt;td style="text-align: right;">2&lt;/td>
&lt;td style="text-align: left;">no: 549, yes: 342&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td style="text-align: left;">class&lt;/td>
&lt;td style="text-align: right;">0&lt;/td>
&lt;td style="text-align: right;">1&lt;/td>
&lt;td style="text-align: left;">FALSE&lt;/td>
&lt;td style="text-align: right;">3&lt;/td>
&lt;td style="text-align: left;">thi: 491, fir: 216, sec: 184&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td style="text-align: left;">gender&lt;/td>
&lt;td style="text-align: right;">0&lt;/td>
&lt;td style="text-align: right;">1&lt;/td>
&lt;td style="text-align: left;">FALSE&lt;/td>
&lt;td style="text-align: right;">2&lt;/td>
&lt;td style="text-align: left;">mal: 577, fem: 314&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td style="text-align: left;">alone&lt;/td>
&lt;td style="text-align: right;">0&lt;/td>
&lt;td style="text-align: right;">1&lt;/td>
&lt;td style="text-align: left;">FALSE&lt;/td>
&lt;td style="text-align: right;">2&lt;/td>
&lt;td style="text-align: left;">yes: 537, no: 354&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td style="text-align: left;">port&lt;/td>
&lt;td style="text-align: right;">0&lt;/td>
&lt;td style="text-align: right;">1&lt;/td>
&lt;td style="text-align: left;">FALSE&lt;/td>
&lt;td style="text-align: right;">4&lt;/td>
&lt;td style="text-align: left;">3: 644, 1: 168, 2: 77, S: 2&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td style="text-align: left;">title&lt;/td>
&lt;td style="text-align: right;">0&lt;/td>
&lt;td style="text-align: right;">1&lt;/td>
&lt;td style="text-align: left;">FALSE&lt;/td>
&lt;td style="text-align: right;">5&lt;/td>
&lt;td style="text-align: left;">Mr.: 517, Mis: 182, Mrs: 125, Mas: 40&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Variable type: numeric&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th style="text-align: left;">skim_variable&lt;/th>
&lt;th style="text-align: right;">n_missing&lt;/th>
&lt;th style="text-align: right;">complete_rate&lt;/th>
&lt;th style="text-align: right;">mean&lt;/th>
&lt;th style="text-align: right;">sd&lt;/th>
&lt;th style="text-align: right;">p0&lt;/th>
&lt;th style="text-align: right;">p25&lt;/th>
&lt;th style="text-align: right;">p50&lt;/th>
&lt;th style="text-align: right;">p75&lt;/th>
&lt;th style="text-align: right;">p100&lt;/th>
&lt;th style="text-align: left;">hist&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td style="text-align: left;">age&lt;/td>
&lt;td style="text-align: right;">0&lt;/td>
&lt;td style="text-align: right;">1&lt;/td>
&lt;td style="text-align: right;">29.39&lt;/td>
&lt;td style="text-align: right;">13.26&lt;/td>
&lt;td style="text-align: right;">0.42&lt;/td>
&lt;td style="text-align: right;">21.00&lt;/td>
&lt;td style="text-align: right;">30.00&lt;/td>
&lt;td style="text-align: right;">35.00&lt;/td>
&lt;td style="text-align: right;">80.00&lt;/td>
&lt;td style="text-align: left;">â–‚â–‡â–ƒâ–â–&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td style="text-align: left;">fareadjusted&lt;/td>
&lt;td style="text-align: right;">0&lt;/td>
&lt;td style="text-align: right;">1&lt;/td>
&lt;td style="text-align: right;">0.04&lt;/td>
&lt;td style="text-align: right;">0.06&lt;/td>
&lt;td style="text-align: right;">0.00&lt;/td>
&lt;td style="text-align: right;">0.01&lt;/td>
&lt;td style="text-align: right;">0.02&lt;/td>
&lt;td style="text-align: right;">0.03&lt;/td>
&lt;td style="text-align: right;">0.58&lt;/td>
&lt;td style="text-align: left;">â–‡â–â–â–â–&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td style="text-align: left;">familyage&lt;/td>
&lt;td style="text-align: right;">0&lt;/td>
&lt;td style="text-align: right;">1&lt;/td>
&lt;td style="text-align: right;">2.32&lt;/td>
&lt;td style="text-align: right;">1.57&lt;/td>
&lt;td style="text-align: right;">1.07&lt;/td>
&lt;td style="text-align: right;">1.41&lt;/td>
&lt;td style="text-align: right;">1.57&lt;/td>
&lt;td style="text-align: right;">2.62&lt;/td>
&lt;td style="text-align: right;">11.43&lt;/td>
&lt;td style="text-align: left;">â–‡â–â–â–â–&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Letâ€™s explore the data.&lt;/p>
&lt;pre>&lt;code>dataset %&amp;gt;%
count(survived)
## # A tibble: 2 Ã— 2
## survived n
## &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt;
## 1 yes 342
## 2 no 549
dataset %&amp;gt;%
group_by(gender) %&amp;gt;%
summarize(n = n(),
n_surv = sum(survived == &amp;quot;yes&amp;quot;),
pct_surv = n_surv / n)
## # A tibble: 2 Ã— 4
## gender n n_surv pct_surv
## &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 female 314 233 0.742
## 2 male 577 109 0.189
dataset %&amp;gt;%
group_by(title) %&amp;gt;%
summarize(n = n(),
n_surv = sum(survived == &amp;quot;yes&amp;quot;),
pct_surv = n_surv / n) %&amp;gt;%
arrange(desc(pct_surv))
## # A tibble: 5 Ã— 4
## title n n_surv pct_surv
## &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Mrs. 125 99 0.792
## 2 Miss. 182 127 0.698
## 3 Master. 40 23 0.575
## 4 Other 27 12 0.444
## 5 Mr. 517 81 0.157
dataset %&amp;gt;%
group_by(class, gender) %&amp;gt;%
summarize(n = n(),
n_surv = sum(survived == &amp;quot;yes&amp;quot;),
pct_surv = n_surv / n) %&amp;gt;%
arrange(desc(pct_surv))
## # A tibble: 6 Ã— 5
## # Groups: class [3]
## class gender n n_surv pct_surv
## &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 first female 94 91 0.968
## 2 second female 76 70 0.921
## 3 third female 144 72 0.5
## 4 first male 122 45 0.369
## 5 second male 108 17 0.157
## 6 third male 347 47 0.135
&lt;/code>&lt;/pre>
&lt;p>Some informative graphs.&lt;/p>
&lt;pre>&lt;code>dataset %&amp;gt;%
group_by(class, gender) %&amp;gt;%
summarize(n = n(),
n_surv = sum(survived == &amp;quot;yes&amp;quot;),
pct_surv = n_surv / n) %&amp;gt;%
mutate(class = fct_reorder(class, pct_surv)) %&amp;gt;%
ggplot(aes(pct_surv, class, fill = class, color = class)) +
geom_col(position = position_dodge()) +
scale_x_continuous(labels = percent) +
labs(x = &amp;quot;% in category that survived&amp;quot;, fill = NULL, color = NULL, y = NULL) +
facet_wrap(~gender)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-7-1.png" alt="">&lt;/p>
&lt;pre>&lt;code>dataset %&amp;gt;%
mutate(age = cut(age, breaks = c(0, 20, 40, 60, 80))) %&amp;gt;%
group_by(age, gender) %&amp;gt;%
summarize(n = n(),
n_surv = sum(survived == &amp;quot;yes&amp;quot;),
pct_surv = n_surv / n) %&amp;gt;%
mutate(age = fct_reorder(age, pct_surv)) %&amp;gt;%
ggplot(aes(pct_surv, age, fill = age, color = age)) +
geom_col(position = position_dodge()) +
scale_x_continuous(labels = percent) +
labs(x = &amp;quot;% in category that survived&amp;quot;, fill = NULL, color = NULL, y = NULL) +
facet_wrap(~gender)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-7-2.png" alt="">&lt;/p>
&lt;pre>&lt;code>dataset %&amp;gt;%
ggplot(aes(fareadjusted, group = survived, color = survived, fill = survived)) +
geom_histogram(alpha = .4, position = position_dodge()) +
labs(x = &amp;quot;fare&amp;quot;, y = NULL, color = &amp;quot;survived?&amp;quot;, fill = &amp;quot;survived?&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-7-3.png" alt="">&lt;/p>
&lt;pre>&lt;code>dataset %&amp;gt;%
ggplot(aes(familyage, group = survived, color = survived, fill = survived)) +
geom_histogram(alpha = .4, position = position_dodge()) +
labs(x = &amp;quot;family aged&amp;quot;, y = NULL, color = &amp;quot;survived?&amp;quot;, fill = &amp;quot;survived?&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-7-4.png" alt="">&lt;/p>
&lt;h1 id="trainingtesting-datasets">Training/testing datasets&lt;/h1>
&lt;p>Split our dataset in two, one dataset for training and the other one for
testing. We will use an additionnal splitting step for cross-validation.&lt;/p>
&lt;pre>&lt;code>set.seed(2021)
spl &amp;lt;- initial_split(dataset, strata = &amp;quot;survived&amp;quot;)
train &amp;lt;- training(spl)
test &amp;lt;- testing(spl)
train_5fold &amp;lt;- train %&amp;gt;%
vfold_cv(5)
&lt;/code>&lt;/pre>
&lt;h1 id="gradient-boosting-algorithms---xgboost">Gradient boosting algorithms - xgboost&lt;/h1>
&lt;p>Letâ€™s start with
&lt;a href="https://en.wikipedia.org/wiki/XGBoost" target="_blank" rel="noopener">gradient boosting
methods&lt;/a> which are very popular
in the ML community.&lt;/p>
&lt;h2 id="tuning">Tuning&lt;/h2>
&lt;p>Set up defaults.&lt;/p>
&lt;pre>&lt;code>mset &amp;lt;- metric_set(accuracy) # metric is accuracy
control &amp;lt;- control_grid(save_workflow = TRUE,
save_pred = TRUE,
extract = extract_model) # grid for tuning
&lt;/code>&lt;/pre>
&lt;p>First a recipe.&lt;/p>
&lt;pre>&lt;code>xg_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code>&lt;/pre>
&lt;p>Then specify a gradient boosting model.&lt;/p>
&lt;pre>&lt;code>xg_model &amp;lt;- boost_tree(mode = &amp;quot;classification&amp;quot;, # binary response
trees = tune(),
mtry = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune(),
min_n = tune()) # parameters to be tuned
&lt;/code>&lt;/pre>
&lt;p>Now set our workflow.&lt;/p>
&lt;pre>&lt;code>xg_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(xg_model) %&amp;gt;%
add_recipe(xg_rec)
&lt;/code>&lt;/pre>
&lt;p>Use cross-validation to evaluate our model with different param config.&lt;/p>
&lt;pre>&lt;code>xg_tune &amp;lt;- xg_wf %&amp;gt;%
tune_grid(train_5fold,
metrics = mset,
control = control,
grid = crossing(trees = 1000,
mtry = c(3, 5, 8), # finalize(mtry(), train)
tree_depth = c(5, 10, 15),
learn_rate = c(0.01, 0.005),
loss_reduction = c(0.01, 0.1, 1),
min_n = c(2, 10, 25)))
&lt;/code>&lt;/pre>
&lt;p>Visualize the results.&lt;/p>
&lt;pre>&lt;code>autoplot(xg_tune) + theme_light()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-14-1.png" alt="">&lt;/p>
&lt;p>Collect metrics.&lt;/p>
&lt;pre>&lt;code>xg_tune %&amp;gt;%
collect_metrics() %&amp;gt;%
arrange(desc(mean))
## # A tibble: 162 Ã— 12
## mtry trees min_n tree_depth learn_rate loss_reduction .metric .estimator mean n
## &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1 3 1000 2 5 0.01 0.01 accuracy binary 0.849 5
## 2 8 1000 2 5 0.01 0.01 accuracy binary 0.847 5
## 3 8 1000 2 5 0.01 0.1 accuracy binary 0.846 5
## 4 3 1000 2 15 0.01 0.1 accuracy binary 0.844 5
## 5 5 1000 2 10 0.01 1 accuracy binary 0.844 5
## 6 3 1000 2 5 0.01 0.1 accuracy binary 0.844 5
## 7 5 1000 2 10 0.01 0.1 accuracy binary 0.843 5
## 8 3 1000 2 10 0.01 0.1 accuracy binary 0.843 5
## 9 5 1000 2 5 0.01 0.01 accuracy binary 0.843 5
## 10 5 1000 2 5 0.01 0.1 accuracy binary 0.843 5
## # â€¦ with 152 more rows, and 2 more variables: std_err &amp;lt;dbl&amp;gt;, .config &amp;lt;chr&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>The tuning takes some time. There are other ways to explore the
parameter space more efficiently. For example, we will use the function
&lt;a href="https://dials.tidymodels.org/reference/grid_max_entropy.html" target="_blank" rel="noopener">&lt;code>dials::grid_max_entropy()&lt;/code>&lt;/a>
in the last section about ensemble modelling. Here, I will use
&lt;a href="https://search.r-project.org/CRAN/refmans/finetune/html/tune_race_anova.html" target="_blank" rel="noopener">&lt;code>finetune::tune_race_anova&lt;/code>&lt;/a>.&lt;/p>
&lt;pre>&lt;code>library(finetune)
xg_tune &amp;lt;-
xg_wf %&amp;gt;%
tune_race_anova(
train_5fold,
grid = 50,
param_info = xg_model %&amp;gt;% parameters(),
metrics = metric_set(accuracy),
control = control_race(verbose_elim = TRUE))
&lt;/code>&lt;/pre>
&lt;p>Visualize the results.&lt;/p>
&lt;pre>&lt;code>autoplot(xg_tune)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-17-1.png" alt="">&lt;/p>
&lt;p>Collect metrics.&lt;/p>
&lt;pre>&lt;code>xg_tune %&amp;gt;%
collect_metrics() %&amp;gt;%
arrange(desc(mean))
## # A tibble: 50 Ã— 12
## mtry trees min_n tree_depth learn_rate loss_reduction .metric .estimator mean n
## &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1 6 856 4 13 1.12e- 2 2.49e- 8 accuracy binary 0.837 5
## 2 10 1952 6 7 3.36e- 2 2.07e+ 0 accuracy binary 0.829 5
## 3 3 896 2 5 1.73e- 5 6.97e- 8 accuracy binary 0.826 5
## 4 14 1122 4 6 1.16e- 6 3.44e+ 0 accuracy binary 0.815 4
## 5 7 939 8 4 2.88e- 5 1.50e- 4 accuracy binary 0.813 4
## 6 7 17 10 7 4.54e- 6 6.38e- 3 accuracy binary 0.813 4
## 7 8 92 9 11 3.60e- 3 3.01e-10 accuracy binary 0.811 4
## 8 13 1407 15 4 1.48e- 2 9.68e- 4 accuracy binary 0.807 3
## 9 4 658 11 9 9.97e-10 1.27e- 5 accuracy binary 0.805 3
## 10 2 628 14 9 1.84e- 6 9.44e- 5 accuracy binary 0.798 3
## # â€¦ with 40 more rows, and 2 more variables: std_err &amp;lt;dbl&amp;gt;, .config &amp;lt;chr&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="fit-model">Fit model&lt;/h2>
&lt;p>Use best config to fit model to training data.&lt;/p>
&lt;pre>&lt;code>xg_fit &amp;lt;- xg_wf %&amp;gt;%
finalize_workflow(select_best(xg_tune)) %&amp;gt;%
fit(train)
## [23:39:25] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
&lt;/code>&lt;/pre>
&lt;p>Check out accuracy on testing dataset to see if we overfitted.&lt;/p>
&lt;pre>&lt;code>xg_fit %&amp;gt;%
augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
accuracy(survived, .pred_class)
## # A tibble: 1 Ã— 3
## .metric .estimator .estimate
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 accuracy binary 0.812
&lt;/code>&lt;/pre>
&lt;p>Check out important features (aka predictors).&lt;/p>
&lt;pre>&lt;code>importances &amp;lt;- xgboost::xgb.importance(model = extract_fit_engine(xg_fit))
importances %&amp;gt;%
mutate(Feature = fct_reorder(Feature, Gain)) %&amp;gt;%
ggplot(aes(Gain, Feature)) +
geom_col()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-21-1.png" alt="">&lt;/p>
&lt;h2 id="make-predictions">Make predictions&lt;/h2>
&lt;p>Now weâ€™re ready to predict survival for the holdout dataset and submit
to Kaggle. Note that I use the whole dataset, not just the training
dataset.&lt;/p>
&lt;pre>&lt;code>xg_wf %&amp;gt;%
finalize_workflow(select_best(xg_tune)) %&amp;gt;%
fit(dataset) %&amp;gt;%
augment(holdout) %&amp;gt;%
select(PassengerId, Survived = .pred_class) %&amp;gt;%
mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
write_csv(&amp;quot;output/titanic/xgboost.csv&amp;quot;)
## [23:39:28] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
&lt;/code>&lt;/pre>
&lt;p>I got and accuracy of 0.74162. Cool. Letâ€™s train a random forest model
now.&lt;/p>
&lt;h1 id="random-forests">Random forests&lt;/h1>
&lt;p>Letâ€™s continue with
&lt;a href="https://en.wikipedia.org/wiki/Random_forest" target="_blank" rel="noopener">random forest
methods&lt;/a>.&lt;/p>
&lt;h2 id="tuning-1">Tuning&lt;/h2>
&lt;p>First a recipe.&lt;/p>
&lt;pre>&lt;code>rf_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code>&lt;/pre>
&lt;p>Then specify a random forest model.&lt;/p>
&lt;pre>&lt;code>rf_model &amp;lt;- rand_forest(mode = &amp;quot;classification&amp;quot;, # binary response
engine = &amp;quot;ranger&amp;quot;, # by default
mtry = tune(),
trees = tune(),
min_n = tune()) # parameters to be tuned
&lt;/code>&lt;/pre>
&lt;p>Now set our workflow.&lt;/p>
&lt;pre>&lt;code>rf_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(rf_model) %&amp;gt;%
add_recipe(rf_rec)
&lt;/code>&lt;/pre>
&lt;p>Use cross-validation to evaluate our model with different param config.&lt;/p>
&lt;pre>&lt;code>rf_tune &amp;lt;-
rf_wf %&amp;gt;%
tune_race_anova(
train_5fold,
grid = 50,
param_info = rf_model %&amp;gt;% parameters(),
metrics = metric_set(accuracy),
control = control_race(verbose_elim = TRUE))
&lt;/code>&lt;/pre>
&lt;p>Visualize the results.&lt;/p>
&lt;pre>&lt;code>autoplot(rf_tune)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-27-1.png" alt="">&lt;/p>
&lt;p>Collect metrics.&lt;/p>
&lt;pre>&lt;code>rf_tune %&amp;gt;%
collect_metrics() %&amp;gt;%
arrange(desc(mean))
## # A tibble: 50 Ã— 9
## mtry trees min_n .metric .estimator mean n std_err .config
## &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 14 1554 5 accuracy binary 0.837 5 0.00656 Preprocessor1_Model49
## 2 4 774 18 accuracy binary 0.837 5 0.0133 Preprocessor1_Model50
## 3 5 1736 8 accuracy binary 0.834 5 0.0111 Preprocessor1_Model46
## 4 8 1322 5 accuracy binary 0.832 5 0.00713 Preprocessor1_Model41
## 5 2 1078 30 accuracy binary 0.831 5 0.00727 Preprocessor1_Model12
## 6 4 1892 14 accuracy binary 0.831 5 0.00886 Preprocessor1_Model39
## 7 8 962 7 accuracy binary 0.829 5 0.00742 Preprocessor1_Model43
## 8 7 946 4 accuracy binary 0.826 5 0.00452 Preprocessor1_Model23
## 9 7 1262 3 accuracy binary 0.826 5 0.00710 Preprocessor1_Model47
## 10 9 544 9 accuracy binary 0.825 5 0.00673 Preprocessor1_Model11
## # â€¦ with 40 more rows
&lt;/code>&lt;/pre>
&lt;h2 id="fit-model-1">Fit model&lt;/h2>
&lt;p>Use best config to fit model to training data.&lt;/p>
&lt;pre>&lt;code>rf_fit &amp;lt;- rf_wf %&amp;gt;%
finalize_workflow(select_best(rf_tune)) %&amp;gt;%
fit(train)
&lt;/code>&lt;/pre>
&lt;p>Check out accuracy on testing dataset to see if we overfitted.&lt;/p>
&lt;pre>&lt;code>rf_fit %&amp;gt;%
augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
accuracy(survived, .pred_class)
## # A tibble: 1 Ã— 3
## .metric .estimator .estimate
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 accuracy binary 0.786
&lt;/code>&lt;/pre>
&lt;p>Check out important features (aka predictors).&lt;/p>
&lt;pre>&lt;code>library(vip)
finalize_model(
x = rf_model,
parameters = select_best(rf_tune)) %&amp;gt;%
set_engine(&amp;quot;ranger&amp;quot;, importance = &amp;quot;permutation&amp;quot;) %&amp;gt;%
fit(survived ~ ., data = juice(prep(rf_rec))) %&amp;gt;%
vip(geom = &amp;quot;point&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-31-1.png" alt="">&lt;/p>
&lt;h2 id="make-predictions-1">Make predictions&lt;/h2>
&lt;p>Now weâ€™re ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p>
&lt;pre>&lt;code>rf_wf %&amp;gt;%
finalize_workflow(select_best(rf_tune)) %&amp;gt;%
fit(dataset) %&amp;gt;%
augment(holdout) %&amp;gt;%
select(PassengerId, Survived = .pred_class) %&amp;gt;%
mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
write_csv(&amp;quot;output/titanic/randomforest.csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>I got and accuracy of 0.77990, a bit better than gradient boosting.&lt;/p>
&lt;p>Letâ€™s continue with
&lt;a href="https://en.wikipedia.org/wiki/Catboost" target="_blank" rel="noopener">cat boosting
methods&lt;/a>.&lt;/p>
&lt;h1 id="gradient-boosting-algorithms---catboost">Gradient boosting algorithms - catboost&lt;/h1>
&lt;h2 id="tuning-2">Tuning&lt;/h2>
&lt;p>Set up defaults.&lt;/p>
&lt;pre>&lt;code>mset &amp;lt;- metric_set(accuracy) # metric is accuracy
control &amp;lt;- control_grid(save_workflow = TRUE,
save_pred = TRUE,
extract = extract_model) # grid for tuning
&lt;/code>&lt;/pre>
&lt;p>First a recipe.&lt;/p>
&lt;pre>&lt;code>cb_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code>&lt;/pre>
&lt;p>Then specify a cat boosting model.&lt;/p>
&lt;pre>&lt;code>library(treesnip)
cb_model &amp;lt;- boost_tree(mode = &amp;quot;classification&amp;quot;,
engine = &amp;quot;catboost&amp;quot;,
mtry = tune(),
trees = tune(),
min_n = tune(),
tree_depth = tune(),
learn_rate = tune()) # parameters to be tuned
&lt;/code>&lt;/pre>
&lt;p>Now set our workflow.&lt;/p>
&lt;pre>&lt;code>cb_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(cb_model) %&amp;gt;%
add_recipe(cb_rec)
&lt;/code>&lt;/pre>
&lt;p>Use cross-validation to evaluate our model with different param config.&lt;/p>
&lt;pre>&lt;code>cb_tune &amp;lt;- cb_wf %&amp;gt;%
tune_race_anova(
train_5fold,
grid = 30,
param_info = cb_model %&amp;gt;% parameters(),
metrics = metric_set(accuracy),
control = control_race(verbose_elim = TRUE))
&lt;/code>&lt;/pre>
&lt;p>Visualize the results.&lt;/p>
&lt;pre>&lt;code>autoplot(cb_tune)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-38-1.png" alt="">&lt;/p>
&lt;p>Collect metrics.&lt;/p>
&lt;pre>&lt;code>cb_tune %&amp;gt;%
collect_metrics() %&amp;gt;%
arrange(desc(mean))
## # A tibble: 30 Ã— 11
## mtry trees min_n tree_depth learn_rate .metric .estimator mean n std_err .config
## &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 1 1787 25 7 6.84e- 3 accuracy binary 0.835 5 0.0125 Preprocâ€¦
## 2 12 1885 18 15 2.06e- 3 accuracy binary 0.831 5 0.00602 Preprocâ€¦
## 3 13 1278 5 6 2.10e- 2 accuracy binary 0.826 5 0.00431 Preprocâ€¦
## 4 3 1681 22 5 5.42e- 3 accuracy binary 0.825 5 0.00507 Preprocâ€¦
## 5 9 303 2 8 9.94e- 2 accuracy binary 0.820 4 0.0120 Preprocâ€¦
## 6 11 1201 24 12 3.77e- 2 accuracy binary 0.812 3 0.00868 Preprocâ€¦
## 7 11 634 35 11 1.30e- 3 accuracy binary 0.805 3 0.0242 Preprocâ€¦
## 8 7 1648 4 7 2.38e- 4 accuracy binary 0.737 3 0.0115 Preprocâ€¦
## 9 1 1427 27 10 1.74e- 6 accuracy binary 0.378 3 0.0152 Preprocâ€¦
## 10 2 940 19 3 3.20e-10 accuracy binary 0.378 3 0.0152 Preprocâ€¦
## # â€¦ with 20 more rows
&lt;/code>&lt;/pre>
&lt;h2 id="fit-model-2">Fit model&lt;/h2>
&lt;p>Use best config to fit model to training data.&lt;/p>
&lt;pre>&lt;code>cb_fit &amp;lt;- cb_wf %&amp;gt;%
finalize_workflow(select_best(cb_tune)) %&amp;gt;%
fit(train)
&lt;/code>&lt;/pre>
&lt;p>Check out accuracy on testing dataset to see if we overfitted.&lt;/p>
&lt;pre>&lt;code>cb_fit %&amp;gt;%
augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
accuracy(survived, .pred_class)
## # A tibble: 1 Ã— 3
## .metric .estimator .estimate
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 accuracy binary 0.808
&lt;/code>&lt;/pre>
&lt;h2 id="make-predictions-2">Make predictions&lt;/h2>
&lt;p>Now weâ€™re ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p>
&lt;pre>&lt;code>cb_wf %&amp;gt;%
finalize_workflow(select_best(cb_tune)) %&amp;gt;%
fit(dataset) %&amp;gt;%
augment(holdout) %&amp;gt;%
select(PassengerId, Survived = .pred_class) %&amp;gt;%
mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
write_csv(&amp;quot;output/titanic/catboost.csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>I got and accuracy of 0.76076. Cool.&lt;/p>
&lt;h1 id="regularization-methods">Regularization methods&lt;/h1>
&lt;p>Letâ€™s continue with
&lt;a href="https://en.wikipedia.org/wiki/Elastic_net_regularization" target="_blank" rel="noopener">elastic net
regularization&lt;/a>.&lt;/p>
&lt;h2 id="tuning-3">Tuning&lt;/h2>
&lt;p>First a recipe.&lt;/p>
&lt;pre>&lt;code>en_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
step_normalize(all_numeric_predictors()) %&amp;gt;% # normalize
step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code>&lt;/pre>
&lt;p>Then specify a regularization model. We tune parameter mixture, with
ridge regression for mixture = 0, and lasso for mixture = 1.&lt;/p>
&lt;pre>&lt;code>en_model &amp;lt;- logistic_reg(penalty = tune(),
mixture = tune()) %&amp;gt;% # param to be tuned
set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;% # elastic net
set_mode(&amp;quot;classification&amp;quot;) # binary response
&lt;/code>&lt;/pre>
&lt;p>Now set our workflow.&lt;/p>
&lt;pre>&lt;code>en_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(en_model) %&amp;gt;%
add_recipe(en_rec)
&lt;/code>&lt;/pre>
&lt;p>Use cross-validation to evaluate our model with different param config.&lt;/p>
&lt;pre>&lt;code>en_tune &amp;lt;- en_wf %&amp;gt;%
tune_grid(train_5fold,
metrics = mset,
control = control,
grid = crossing(penalty = 10 ^ seq(-8, -.5, .5),
mixture = seq(0, 1, length.out = 10)))
&lt;/code>&lt;/pre>
&lt;p>Visualize the results.&lt;/p>
&lt;pre>&lt;code>autoplot(en_tune)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-47-1.png" alt="">&lt;/p>
&lt;p>Collect metrics.&lt;/p>
&lt;pre>&lt;code>en_tune %&amp;gt;%
collect_metrics() %&amp;gt;%
arrange(desc(mean))
## # A tibble: 160 Ã— 8
## penalty mixture .metric .estimator mean n std_err .config
## &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 0.00000001 0.111 accuracy binary 0.831 5 0.0112 Preprocessor1_Model017
## 2 0.0000000316 0.111 accuracy binary 0.831 5 0.0112 Preprocessor1_Model018
## 3 0.0000001 0.111 accuracy binary 0.831 5 0.0112 Preprocessor1_Model019
## 4 0.000000316 0.111 accuracy binary 0.831 5 0.0112 Preprocessor1_Model020
## 5 0.000001 0.111 accuracy binary 0.831 5 0.0112 Preprocessor1_Model021
## 6 0.00000316 0.111 accuracy binary 0.831 5 0.0112 Preprocessor1_Model022
## 7 0.00001 0.111 accuracy binary 0.831 5 0.0112 Preprocessor1_Model023
## 8 0.0000316 0.111 accuracy binary 0.831 5 0.0112 Preprocessor1_Model024
## 9 0.0001 0.111 accuracy binary 0.831 5 0.0112 Preprocessor1_Model025
## 10 0.000316 0.111 accuracy binary 0.831 5 0.0112 Preprocessor1_Model026
## # â€¦ with 150 more rows
&lt;/code>&lt;/pre>
&lt;h2 id="fit-model-3">Fit model&lt;/h2>
&lt;p>Use best config to fit model to training data.&lt;/p>
&lt;pre>&lt;code>en_fit &amp;lt;- en_wf %&amp;gt;%
finalize_workflow(select_best(en_tune)) %&amp;gt;%
fit(train)
&lt;/code>&lt;/pre>
&lt;p>Check out accuracy on testing dataset to see if we overfitted.&lt;/p>
&lt;pre>&lt;code>en_fit %&amp;gt;%
augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
accuracy(survived, .pred_class)
## # A tibble: 1 Ã— 3
## .metric .estimator .estimate
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 accuracy binary 0.826
&lt;/code>&lt;/pre>
&lt;p>Check out important features (aka predictors).&lt;/p>
&lt;pre>&lt;code>library(broom)
en_fit$fit$fit$fit %&amp;gt;%
tidy() %&amp;gt;%
filter(lambda &amp;gt;= select_best(en_tune)$penalty) %&amp;gt;%
filter(lambda == min(lambda),
term != &amp;quot;(Intercept)&amp;quot;) %&amp;gt;%
mutate(term = fct_reorder(term, estimate)) %&amp;gt;%
ggplot(aes(estimate, term, fill = estimate &amp;gt; 0)) +
geom_col() +
theme(legend.position = &amp;quot;none&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-51-1.png" alt="">&lt;/p>
&lt;h2 id="make-predictions-3">Make predictions&lt;/h2>
&lt;p>Now weâ€™re ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p>
&lt;pre>&lt;code>en_wf %&amp;gt;%
finalize_workflow(select_best(en_tune)) %&amp;gt;%
fit(dataset) %&amp;gt;%
augment(holdout) %&amp;gt;%
select(PassengerId, Survived = .pred_class) %&amp;gt;%
mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
write_csv(&amp;quot;output/titanic/regularization.csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>I got and accuracy of 0.76315.&lt;/p>
&lt;h1 id="logistic-regression">Logistic regression&lt;/h1>
&lt;p>And what about a good old-fashioned logistic regression (not a ML algo)?&lt;/p>
&lt;p>First a recipe.&lt;/p>
&lt;pre>&lt;code>logistic_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
step_normalize(all_numeric_predictors()) %&amp;gt;% # normalize
step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code>&lt;/pre>
&lt;p>Then specify a logistic regression.&lt;/p>
&lt;pre>&lt;code>logistic_model &amp;lt;- logistic_reg() %&amp;gt;% # no param to be tuned
set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;% # elastic net
set_mode(&amp;quot;classification&amp;quot;) # binary response
&lt;/code>&lt;/pre>
&lt;p>Now set our workflow.&lt;/p>
&lt;pre>&lt;code>logistic_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(logistic_model) %&amp;gt;%
add_recipe(logistic_rec)
&lt;/code>&lt;/pre>
&lt;p>Fit model.&lt;/p>
&lt;pre>&lt;code>logistic_fit &amp;lt;- logistic_wf %&amp;gt;%
fit(train)
&lt;/code>&lt;/pre>
&lt;p>Inspect significant features (aka predictors).&lt;/p>
&lt;pre>&lt;code>tidy(logistic_fit, exponentiate = TRUE) %&amp;gt;%
filter(p.value &amp;lt; 0.05)
## # A tibble: 6 Ã— 5
## term estimate std.error statistic p.value
## &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age 1.35 0.152 1.98 0.0473
## 2 familyage 2.97 0.223 4.88 0.00000105
## 3 class_first 0.136 0.367 -5.43 0.0000000559
## 4 class_second 0.386 0.298 -3.19 0.00145
## 5 title_Mr. 51.0 0.684 5.75 0.00000000912
## 6 title_Other 54.7 0.991 4.04 0.0000538
&lt;/code>&lt;/pre>
&lt;p>Same thing, but graphically.&lt;/p>
&lt;pre>&lt;code>library(broom)
logistic_fit %&amp;gt;%
tidy() %&amp;gt;%
mutate(term = fct_reorder(term, estimate)) %&amp;gt;%
ggplot(aes(estimate, term, fill = estimate &amp;gt; 0)) +
geom_col() +
theme(legend.position = &amp;quot;none&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-58-1.png" alt="">&lt;/p>
&lt;p>Check out accuracy on testing dataset to see if we overfitted.&lt;/p>
&lt;pre>&lt;code>logistic_fit %&amp;gt;%
augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
accuracy(survived, .pred_class)
## # A tibble: 1 Ã— 3
## .metric .estimator .estimate
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 accuracy binary 0.821
&lt;/code>&lt;/pre>
&lt;p>Confusion matrix.&lt;/p>
&lt;pre>&lt;code>logistic_fit %&amp;gt;%
augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
conf_mat(survived, .pred_class)
## Truth
## Prediction yes no
## yes 59 13
## no 27 125
&lt;/code>&lt;/pre>
&lt;p>ROC curve.&lt;/p>
&lt;pre>&lt;code>logistic_fit %&amp;gt;%
augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
roc_curve(truth = survived, estimate = .pred_yes) %&amp;gt;%
autoplot()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-61-1.png" alt="">&lt;/p>
&lt;p>Now weâ€™re ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p>
&lt;pre>&lt;code>logistic_wf %&amp;gt;%
fit(dataset) %&amp;gt;%
augment(holdout) %&amp;gt;%
select(PassengerId, Survived = .pred_class) %&amp;gt;%
mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
write_csv(&amp;quot;output/titanic/logistic.csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>I got and accuracy of 0.76076. Oldies but goodies!&lt;/p>
&lt;h1 id="neural-networks">Neural networks&lt;/h1>
&lt;p>We go on with
&lt;a href="https://en.wikipedia.org/wiki/Artificial_neural_network" target="_blank" rel="noopener">neural
networks&lt;/a>.&lt;/p>
&lt;h2 id="tuning-4">Tuning&lt;/h2>
&lt;p>Set up defaults.&lt;/p>
&lt;pre>&lt;code>mset &amp;lt;- metric_set(accuracy) # metric is accuracy
control &amp;lt;- control_grid(save_workflow = TRUE,
save_pred = TRUE,
extract = extract_model) # grid for tuning
&lt;/code>&lt;/pre>
&lt;p>First a recipe.&lt;/p>
&lt;pre>&lt;code>nn_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
step_normalize(all_numeric_predictors()) %&amp;gt;%
step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code>&lt;/pre>
&lt;p>Then specify a neural network.&lt;/p>
&lt;pre>&lt;code>nn_model &amp;lt;- mlp(epochs = tune(),
hidden_units = tune(),
dropout = tune()) %&amp;gt;% # param to be tuned
set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;% # binary response var
set_engine(&amp;quot;keras&amp;quot;, verbose = 0)
&lt;/code>&lt;/pre>
&lt;p>Now set our workflow.&lt;/p>
&lt;pre>&lt;code>nn_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(nn_model) %&amp;gt;%
add_recipe(nn_rec)
&lt;/code>&lt;/pre>
&lt;p>Use cross-validation to evaluate our model with different param config.&lt;/p>
&lt;pre>&lt;code>nn_tune &amp;lt;- nn_wf %&amp;gt;%
tune_race_anova(
train_5fold,
grid = 30,
param_info = nn_model %&amp;gt;% parameters(),
metrics = metric_set(accuracy),
control = control_race(verbose_elim = TRUE))
&lt;/code>&lt;/pre>
&lt;p>Visualize the results.&lt;/p>
&lt;pre>&lt;code>autoplot(nn_tune)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-68-1.png" alt="">&lt;/p>
&lt;p>Collect metrics.&lt;/p>
&lt;pre>&lt;code>nn_tune %&amp;gt;%
collect_metrics() %&amp;gt;%
arrange(desc(mean))
## # A tibble: 30 Ã— 9
## hidden_units dropout epochs .metric .estimator mean n std_err .config
## &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 10 0.484 405 accuracy binary 0.838 5 0.0150 Preprocessor1_Modelâ€¦
## 2 7 0.0597 220 accuracy binary 0.834 5 0.00881 Preprocessor1_Modelâ€¦
## 3 4 0.536 629 accuracy binary 0.834 5 0.0154 Preprocessor1_Modelâ€¦
## 4 9 0.198 768 accuracy binary 0.832 5 0.0146 Preprocessor1_Modelâ€¦
## 5 6 0.752 822 accuracy binary 0.832 5 0.0112 Preprocessor1_Modelâ€¦
## 6 9 0.406 293 accuracy binary 0.831 5 0.0145 Preprocessor1_Modelâ€¦
## 7 4 0.00445 871 accuracy binary 0.831 5 0.0150 Preprocessor1_Modelâ€¦
## 8 4 0.293 353 accuracy binary 0.831 5 0.0117 Preprocessor1_Modelâ€¦
## 9 10 0.675 935 accuracy binary 0.831 5 0.0141 Preprocessor1_Modelâ€¦
## 10 7 0.128 580 accuracy binary 0.831 5 0.0151 Preprocessor1_Modelâ€¦
## # â€¦ with 20 more rows
&lt;/code>&lt;/pre>
&lt;h2 id="fit-model-4">Fit model&lt;/h2>
&lt;p>Use best config to fit model to training data.&lt;/p>
&lt;pre>&lt;code>nn_fit &amp;lt;- nn_wf %&amp;gt;%
finalize_workflow(select_best(nn_tune)) %&amp;gt;%
fit(train)
&lt;/code>&lt;/pre>
&lt;p>Check out accuracy on testing dataset to see if we overfitted.&lt;/p>
&lt;pre>&lt;code>nn_fit %&amp;gt;%
augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
accuracy(survived, .pred_class)
## # A tibble: 1 Ã— 3
## .metric .estimator .estimate
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 accuracy binary 0.817
&lt;/code>&lt;/pre>
&lt;h2 id="make-predictions-4">Make predictions&lt;/h2>
&lt;p>Now weâ€™re ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p>
&lt;pre>&lt;code>nn_wf %&amp;gt;%
finalize_workflow(select_best(nn_tune)) %&amp;gt;%
fit(train) %&amp;gt;%
augment(holdout) %&amp;gt;%
select(PassengerId, Survived = .pred_class) %&amp;gt;%
mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
write_csv(&amp;quot;output/titanic/nn.csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>I got and accuracy of 0.78708. My best score so far.&lt;/p>
&lt;h1 id="support-vector-machines">Support vector machines&lt;/h1>
&lt;p>We go on with
&lt;a href="https://en.wikipedia.org/wiki/Support-vector_machine" target="_blank" rel="noopener">support vector
machines&lt;/a>.&lt;/p>
&lt;h2 id="tuning-5">Tuning&lt;/h2>
&lt;p>Set up defaults.&lt;/p>
&lt;pre>&lt;code>mset &amp;lt;- metric_set(accuracy) # metric is accuracy
control &amp;lt;- control_grid(save_workflow = TRUE,
save_pred = TRUE,
extract = extract_model) # grid for tuning
&lt;/code>&lt;/pre>
&lt;p>First a recipe.&lt;/p>
&lt;pre>&lt;code>svm_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
# remove any zero variance predictors
step_zv(all_predictors()) %&amp;gt;%
# remove any linear combinations
step_lincomb(all_numeric()) %&amp;gt;%
step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code>&lt;/pre>
&lt;p>Then specify a svm.&lt;/p>
&lt;pre>&lt;code>svm_model &amp;lt;- svm_rbf(cost = tune(),
rbf_sigma = tune()) %&amp;gt;% # param to be tuned
set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;% # binary response var
set_engine(&amp;quot;kernlab&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Now set our workflow.&lt;/p>
&lt;pre>&lt;code>svm_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(svm_model) %&amp;gt;%
add_recipe(svm_rec)
&lt;/code>&lt;/pre>
&lt;p>Use cross-validation to evaluate our model with different param config.&lt;/p>
&lt;pre>&lt;code>svm_tune &amp;lt;- svm_wf %&amp;gt;%
tune_race_anova(
train_5fold,
grid = 30,
param_info = svm_model %&amp;gt;% parameters(),
metrics = metric_set(accuracy),
control = control_race(verbose_elim = TRUE))
&lt;/code>&lt;/pre>
&lt;p>Visualize the results.&lt;/p>
&lt;pre>&lt;code>autoplot(svm_tune)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-78-1.png" alt="">&lt;/p>
&lt;p>Collect metrics.&lt;/p>
&lt;pre>&lt;code>svm_tune %&amp;gt;%
collect_metrics() %&amp;gt;%
arrange(desc(mean))
## # A tibble: 30 Ã— 8
## cost rbf_sigma .metric .estimator mean n std_err .config
## &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 17.7 0.00183 accuracy binary 0.829 5 0.0101 Preprocessor1_Model05
## 2 0.741 0.0506 accuracy binary 0.823 5 0.0129 Preprocessor1_Model14
## 3 1.77 0.0429 accuracy binary 0.820 5 0.0143 Preprocessor1_Model08
## 4 0.229 0.0197 accuracy binary 0.808 3 0.00626 Preprocessor1_Model23
## 5 1.15 0.00285 accuracy binary 0.795 3 0.00867 Preprocessor1_Model28
## 6 0.00182 0.00000203 accuracy binary 0.613 3 0.0171 Preprocessor1_Model01
## 7 0.0477 0.714 accuracy binary 0.613 3 0.0171 Preprocessor1_Model02
## 8 6.60 0.0000294 accuracy binary 0.613 3 0.0171 Preprocessor1_Model03
## 9 0.00254 0.000636 accuracy binary 0.613 3 0.0171 Preprocessor1_Model04
## 10 0.00544 0.0000000647 accuracy binary 0.613 3 0.0171 Preprocessor1_Model06
## # â€¦ with 20 more rows
&lt;/code>&lt;/pre>
&lt;h2 id="fit-model-5">Fit model&lt;/h2>
&lt;p>Use best config to fit model to training data.&lt;/p>
&lt;pre>&lt;code>svm_fit &amp;lt;- svm_wf %&amp;gt;%
finalize_workflow(select_best(svm_tune)) %&amp;gt;%
fit(train)
&lt;/code>&lt;/pre>
&lt;p>Check out accuracy on testing dataset to see if we overfitted.&lt;/p>
&lt;pre>&lt;code>svm_fit %&amp;gt;%
augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
accuracy(survived, .pred_class)
## # A tibble: 1 Ã— 3
## .metric .estimator .estimate
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 accuracy binary 0.826
&lt;/code>&lt;/pre>
&lt;h2 id="make-predictions-5">Make predictions&lt;/h2>
&lt;p>Now weâ€™re ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p>
&lt;pre>&lt;code>svm_wf %&amp;gt;%
finalize_workflow(select_best(svm_tune)) %&amp;gt;%
fit(dataset) %&amp;gt;%
augment(holdout) %&amp;gt;%
select(PassengerId, Survived = .pred_class) %&amp;gt;%
mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
write_csv(&amp;quot;output/titanic/svm.csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>I got and accuracy of 0.77511.&lt;/p>
&lt;h1 id="decision-trees">Decision trees&lt;/h1>
&lt;p>We go on with
&lt;a href="https://en.wikipedia.org/wiki/Decision_tree" target="_blank" rel="noopener">decision
trees&lt;/a>.&lt;/p>
&lt;h2 id="tuning-6">Tuning&lt;/h2>
&lt;p>Set up defaults.&lt;/p>
&lt;pre>&lt;code>mset &amp;lt;- metric_set(accuracy) # metric is accuracy
control &amp;lt;- control_grid(save_workflow = TRUE,
save_pred = TRUE,
extract = extract_model) # grid for tuning
&lt;/code>&lt;/pre>
&lt;p>First a recipe.&lt;/p>
&lt;pre>&lt;code>dt_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
step_zv(all_predictors()) %&amp;gt;%
step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code>&lt;/pre>
&lt;p>Then specify a decision tree model.&lt;/p>
&lt;pre>&lt;code>library(baguette)
dt_model &amp;lt;- bag_tree(cost_complexity = tune(),
tree_depth = tune(),
min_n = tune()) %&amp;gt;% # param to be tuned
set_engine(&amp;quot;rpart&amp;quot;, times = 25) %&amp;gt;% # nb bootstraps
set_mode(&amp;quot;classification&amp;quot;) # binary response var
&lt;/code>&lt;/pre>
&lt;p>Now set our workflow.&lt;/p>
&lt;pre>&lt;code>dt_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(dt_model) %&amp;gt;%
add_recipe(dt_rec)
&lt;/code>&lt;/pre>
&lt;p>Use cross-validation to evaluate our model with different param config.&lt;/p>
&lt;pre>&lt;code>dt_tune &amp;lt;- dt_wf %&amp;gt;%
tune_race_anova(
train_5fold,
grid = 30,
param_info = dt_model %&amp;gt;% parameters(),
metrics = metric_set(accuracy),
control = control_race(verbose_elim = TRUE))
&lt;/code>&lt;/pre>
&lt;p>Visualize the results.&lt;/p>
&lt;pre>&lt;code>autoplot(dt_tune)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-88-1.png" alt="">&lt;/p>
&lt;p>Collect metrics.&lt;/p>
&lt;pre>&lt;code>dt_tune %&amp;gt;%
collect_metrics() %&amp;gt;%
arrange(desc(mean))
## # A tibble: 30 Ã— 9
## cost_complexity tree_depth min_n .metric .estimator mean n std_err .config
## &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 3.02e- 7 13 7 accuracy binary 0.832 5 0.0130 Preprocessor1_â€¦
## 2 3.43e- 3 14 4 accuracy binary 0.829 5 0.0145 Preprocessor1_â€¦
## 3 5.75e- 5 5 7 accuracy binary 0.828 5 0.0126 Preprocessor1_â€¦
## 4 2.13e- 6 14 5 accuracy binary 0.828 5 0.0108 Preprocessor1_â€¦
## 5 1.34e- 5 5 35 accuracy binary 0.823 5 0.00654 Preprocessor1_â€¦
## 6 4.47e- 5 13 10 accuracy binary 0.823 5 0.0120 Preprocessor1_â€¦
## 7 1.42e- 2 4 25 accuracy binary 0.822 5 0.0121 Preprocessor1_â€¦
## 8 4.54e-10 10 36 accuracy binary 0.822 5 0.0147 Preprocessor1_â€¦
## 9 8.10e- 8 11 32 accuracy binary 0.822 5 0.0143 Preprocessor1_â€¦
## 10 3.43e- 4 11 21 accuracy binary 0.820 5 0.0207 Preprocessor1_â€¦
## # â€¦ with 20 more rows
&lt;/code>&lt;/pre>
&lt;h2 id="fit-model-6">Fit model&lt;/h2>
&lt;p>Use best config to fit model to training data.&lt;/p>
&lt;pre>&lt;code>dt_fit &amp;lt;- dt_wf %&amp;gt;%
finalize_workflow(select_best(dt_tune)) %&amp;gt;%
fit(train)
&lt;/code>&lt;/pre>
&lt;p>Check out accuracy on testing dataset to see if we overfitted.&lt;/p>
&lt;pre>&lt;code>dt_fit %&amp;gt;%
augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
accuracy(survived, .pred_class)
## # A tibble: 1 Ã— 3
## .metric .estimator .estimate
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 accuracy binary 0.808
&lt;/code>&lt;/pre>
&lt;h2 id="make-predictions-6">Make predictions&lt;/h2>
&lt;p>Now weâ€™re ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p>
&lt;pre>&lt;code>dt_wf %&amp;gt;%
finalize_workflow(select_best(dt_tune)) %&amp;gt;%
fit(dataset) %&amp;gt;%
augment(holdout) %&amp;gt;%
select(PassengerId, Survived = .pred_class) %&amp;gt;%
mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
write_csv(&amp;quot;output/titanic/dt.csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>I got and accuracy of 0.76794.&lt;/p>
&lt;h1 id="stacked-ensemble-modelling">Stacked ensemble modelling&lt;/h1>
&lt;p>Letâ€™s do some ensemble modelling with all algo but logistic and
catboost. Tune again with a probability-based metric. Start with
xgboost.&lt;/p>
&lt;pre>&lt;code>library(finetune)
library(stacks)
# xgboost
xg_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
xg_model &amp;lt;- boost_tree(mode = &amp;quot;classification&amp;quot;, # binary response
trees = tune(),
mtry = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune(),
min_n = tune()) # parameters to be tuned
xg_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(xg_model) %&amp;gt;%
add_recipe(xg_rec)
xg_grid &amp;lt;- grid_latin_hypercube(
trees(),
finalize(mtry(), train),
tree_depth(),
learn_rate(),
loss_reduction(),
min_n(),
size = 30)
xg_tune &amp;lt;- xg_wf %&amp;gt;%
tune_grid(
resamples = train_5fold,
grid = xg_grid,
metrics = metric_set(roc_auc),
control = control_stack_grid())
&lt;/code>&lt;/pre>
&lt;p>Then random forests.&lt;/p>
&lt;pre>&lt;code># random forest
rf_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;%
step_dummy(all_nominal_predictors())
rf_model &amp;lt;- rand_forest(mode = &amp;quot;classification&amp;quot;, # binary response
engine = &amp;quot;ranger&amp;quot;, # by default
mtry = tune(),
trees = tune(),
min_n = tune()) # parameters to be tuned
rf_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(rf_model) %&amp;gt;%
add_recipe(rf_rec)
rf_grid &amp;lt;- grid_latin_hypercube(
finalize(mtry(), train),
trees(),
min_n(),
size = 30)
rf_tune &amp;lt;- rf_wf %&amp;gt;%
tune_grid(
resamples = train_5fold,
grid = rf_grid,
metrics = metric_set(roc_auc),
control = control_stack_grid())
&lt;/code>&lt;/pre>
&lt;p>Regularisation methods (between ridge and lasso).&lt;/p>
&lt;pre>&lt;code># regularization methods
en_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
step_normalize(all_numeric_predictors()) %&amp;gt;% # normalize
step_dummy(all_nominal_predictors())
en_model &amp;lt;- logistic_reg(penalty = tune(),
mixture = tune()) %&amp;gt;% # param to be tuned
set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;% # elastic net
set_mode(&amp;quot;classification&amp;quot;) # binary response
en_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(en_model) %&amp;gt;%
add_recipe(en_rec)
en_grid &amp;lt;- grid_latin_hypercube(
penalty(),
mixture(),
size = 30)
en_tune &amp;lt;- en_wf %&amp;gt;%
tune_grid(
resamples = train_5fold,
grid = en_grid,
metrics = metric_set(roc_auc),
control = control_stack_grid())
&lt;/code>&lt;/pre>
&lt;p>Neural networks (takes time, so pick only a few values for illustration
purpose).&lt;/p>
&lt;pre>&lt;code># neural networks
nn_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
step_normalize(all_numeric_predictors()) %&amp;gt;%
step_dummy(all_nominal_predictors())
nn_model &amp;lt;- mlp(epochs = tune(),
hidden_units = 2,
dropout = tune()) %&amp;gt;% # param to be tuned
set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;% # binary response var
set_engine(&amp;quot;keras&amp;quot;, verbose = 0)
nn_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(nn_model) %&amp;gt;%
add_recipe(nn_rec)
# nn_grid &amp;lt;- grid_latin_hypercube(
# epochs(),
# hidden_units(),
# dropout(),
# size = 10)
nn_tune &amp;lt;- nn_wf %&amp;gt;%
tune_grid(
resamples = train_5fold,
grid = crossing(dropout = c(0.1, 0.2), epochs = c(250, 500, 1000)), # nn_grid
metrics = metric_set(roc_auc),
control = control_stack_grid())
#autoplot(nn_tune)
&lt;/code>&lt;/pre>
&lt;p>Support vector machines.&lt;/p>
&lt;pre>&lt;code># support vector machines
svm_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;%
step_normalize(all_numeric_predictors()) %&amp;gt;%
step_dummy(all_nominal_predictors())
svm_model &amp;lt;- svm_rbf(cost = tune(),
rbf_sigma = tune()) %&amp;gt;% # param to be tuned
set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;% # binary response var
set_engine(&amp;quot;kernlab&amp;quot;)
svm_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(svm_model) %&amp;gt;%
add_recipe(svm_rec)
svm_grid &amp;lt;- grid_latin_hypercube(
cost(),
rbf_sigma(),
size = 30)
svm_tune &amp;lt;- svm_wf %&amp;gt;%
tune_grid(
resamples = train_5fold,
grid = svm_grid,
metrics = metric_set(roc_auc),
control = control_stack_grid())
&lt;/code>&lt;/pre>
&lt;p>Last, decision trees.&lt;/p>
&lt;pre>&lt;code># decision trees
dt_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
step_impute_median(all_numeric()) %&amp;gt;%
step_zv(all_predictors()) %&amp;gt;%
step_dummy(all_nominal_predictors())
library(baguette)
dt_model &amp;lt;- bag_tree(cost_complexity = tune(),
tree_depth = tune(),
min_n = tune()) %&amp;gt;% # param to be tuned
set_engine(&amp;quot;rpart&amp;quot;, times = 25) %&amp;gt;% # nb bootstraps
set_mode(&amp;quot;classification&amp;quot;) # binary response var
dt_wf &amp;lt;-
workflow() %&amp;gt;%
add_model(dt_model) %&amp;gt;%
add_recipe(dt_rec)
dt_grid &amp;lt;- grid_latin_hypercube(
cost_complexity(),
tree_depth(),
min_n(),
size = 30)
dt_tune &amp;lt;- dt_wf %&amp;gt;%
tune_grid(
resamples = train_5fold,
grid = dt_grid,
metrics = metric_set(roc_auc),
control = control_stack_grid())
&lt;/code>&lt;/pre>
&lt;p>Get best config.&lt;/p>
&lt;pre>&lt;code>xg_best &amp;lt;- xg_tune %&amp;gt;% filter_parameters(parameters = select_best(xg_tune))
rf_best &amp;lt;- rf_tune %&amp;gt;% filter_parameters(parameters = select_best(rf_tune))
en_best &amp;lt;- en_tune %&amp;gt;% filter_parameters(parameters = select_best(en_tune))
nn_best &amp;lt;- nn_tune %&amp;gt;% filter_parameters(parameters = select_best(nn_tune))
svm_best &amp;lt;- svm_tune %&amp;gt;% filter_parameters(parameters = select_best(svm_tune))
dt_best &amp;lt;- dt_tune %&amp;gt;% filter_parameters(parameters = select_best(dt_tune))
&lt;/code>&lt;/pre>
&lt;p>Do the stacked ensemble modelling.&lt;/p>
&lt;p>Pile all models together.&lt;/p>
&lt;pre>&lt;code>blended &amp;lt;- stacks() %&amp;gt;% # initialize
add_candidates(en_best) %&amp;gt;% # add regularization model
add_candidates(xg_best) %&amp;gt;% # add gradient boosting
add_candidates(rf_best) %&amp;gt;% # add random forest
add_candidates(nn_best) %&amp;gt;% # add neural network
add_candidates(svm_best) %&amp;gt;% # add svm
add_candidates(dt_best) # add decision trees
blended
## # A data stack with 6 model definitions and 6 candidate members:
## # en_best: 1 model configuration
## # xg_best: 1 model configuration
## # rf_best: 1 model configuration
## # nn_best: 1 model configuration
## # svm_best: 1 model configuration
## # dt_best: 1 model configuration
## # Outcome: survived (factor)
&lt;/code>&lt;/pre>
&lt;p>Fit regularized model.&lt;/p>
&lt;pre>&lt;code>blended_fit &amp;lt;- blended %&amp;gt;%
blend_predictions() # fit regularized model
&lt;/code>&lt;/pre>
&lt;p>Visualise penalized model. Note that neural networks are dropped,
despite achieving best score when used in isolation. Iâ€™ll have to dig
into that.&lt;/p>
&lt;pre>&lt;code>autoplot(blended_fit)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-102-1.png" alt="">&lt;/p>
&lt;pre>&lt;code>autoplot(blended_fit, type = &amp;quot;members&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-102-2.png" alt="">&lt;/p>
&lt;pre>&lt;code>autoplot(blended_fit, type = &amp;quot;weights&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-102-3.png" alt="">&lt;/p>
&lt;p>Fit candidate members with non-zero stacking coef with full training
dataset.&lt;/p>
&lt;pre>&lt;code>blended_regularized &amp;lt;- blended_fit %&amp;gt;%
fit_members()
blended_regularized
## # A tibble: 3 Ã— 3
## member type weight
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 .pred_no_en_best_1_12 logistic_reg 2.67
## 2 .pred_no_dt_best_1_11 bag_tree 1.95
## 3 .pred_no_rf_best_1_05 rand_forest 0.922
&lt;/code>&lt;/pre>
&lt;p>Perf on testing dataset?&lt;/p>
&lt;pre>&lt;code>test %&amp;gt;%
bind_cols(predict(blended_regularized, .)) %&amp;gt;%
accuracy(survived, .pred_class)
## # A tibble: 1 Ã— 3
## .metric .estimator .estimate
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 accuracy binary 0.826
&lt;/code>&lt;/pre>
&lt;p>Now predict.&lt;/p>
&lt;pre>&lt;code>holdout %&amp;gt;%
bind_cols(predict(blended_regularized, .)) %&amp;gt;%
select(PassengerId, Survived = .pred_class) %&amp;gt;%
mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
write_csv(&amp;quot;output/titanic/stacked.csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>I got an 0.76076 accuracy.&lt;/p>
&lt;h1 id="conclusions">Conclusions&lt;/h1>
&lt;p>I covered several ML algorithms and logistic regression with the awesome
&lt;code>tidymodels&lt;/code> metapackage in &lt;code>R&lt;/code>. My scores at predicting Titanic
survivors were ok I guess. Some folks on Kaggle got a perfect accuracy,
so there is always room for improvement. Maybe better tuning, better
features (or predictors) or other algorithms would increase accuracy. Of
course, I forgot to use &lt;code>set.seed()&lt;/code> so results are not exactly
reproducible.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>This post was also published on &lt;a href="https://www.r-bloggers.com">https://www.r-bloggers.com&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Quick and dirty analysis of a Twitter social network</title><link>https://oliviergimenez.github.io/blog/twitter-social-network/</link><pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/twitter-social-network/</guid><description>&lt;p>I use &lt;code>R&lt;/code> to retrieve some data from Twitter, do some exploratory data analysis and visualisation and examine a network of followers.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>I use Twitter to get live updates of what my follow scientists are up to, to communicate about my students' awesome work and to share material that I hope is useful to some people&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Recently, I reached 5,000 followers and I thought I&amp;rsquo;d spend some time trying to know better who they/you are. To do so, I use &lt;code>R&lt;/code> to retrieve some data from Twitter using
&lt;a href="https://docs.ropensci.org/rtweet/" target="_blank" rel="noopener">&lt;code>rtweet&lt;/code>&lt;/a>, do some data exploration and visualisation using the
&lt;a href="https://www.tidyverse.org/" target="_blank" rel="noopener">&lt;code>tidyverse&lt;/code>&lt;/a> and examine my network of followers with
&lt;a href="https://tidygraph.data-imaginist.com/" target="_blank" rel="noopener">&lt;code>tidygraph&lt;/code>&lt;/a>,
&lt;a href="https://ggraph.data-imaginist.com/" target="_blank" rel="noopener">&lt;code>ggraph&lt;/code>&lt;/a> and
&lt;a href="https://igraph.org/r/" target="_blank" rel="noopener">&lt;code>igraph&lt;/code>&lt;/a>. Data and codes are available from &lt;a href="https://github.com/oliviergimenez/sna-twitter">https://github.com/oliviergimenez/sna-twitter&lt;/a>.&lt;/p>
&lt;p>To reproduce the analyses below, you will need to access a Twitter API (application programming interface) to retrieve the information about your followers. In brief, an API is an intermediary application that allows applications to talk to each other. To access the Twitter APIs, you need a developer account for which you may apply at &lt;a href="https://developer.twitter.com/en">https://developer.twitter.com/en&lt;/a>. There is a short form to fill in, and it takes less than a day to get an answer.&lt;/p>
&lt;p>Below I rely heavily on the code shared by Joe Cristian through the Algoritma Technical Blog at &lt;a href="https://algotech.netlify.app/blog/social-network-analysis-in-r/">https://algotech.netlify.app/blog/social-network-analysis-in-r/&lt;/a>. Kuddos and credits to him.&lt;/p>
&lt;h2 id="data-retrieving">Data retrieving&lt;/h2>
&lt;p>We load the &lt;code>rtweet&lt;/code> package to work with Twitter from R.&lt;/p>
&lt;pre>&lt;code class="language-r">#devtools::install_github(&amp;quot;ropensci/rtweet&amp;quot;)
library(rtweet)
&lt;/code>&lt;/pre>
&lt;p>We also load the &lt;code>tidyverse&lt;/code> for data manipulation and visualisation.&lt;/p>
&lt;pre>&lt;code class="language-r">library(tidyverse)
theme_set(theme_light(base_size = 14))
&lt;/code>&lt;/pre>
&lt;p>First we need to get credentials. The usual &lt;code>rtweet&lt;/code> sequence with &lt;code>rtweet_app&lt;/code>, &lt;code>auth_save&lt;/code> and &lt;code>auth_as&lt;/code> is supposed to work (see
&lt;a href="https://docs.ropensci.org/rtweet/articles/auth.html" target="_blank" rel="noopener">here&lt;/a>), but the Twitter API kept failing (error 401) for me. Tried a few things, in vain. I will use the deprecated &lt;code>create_token&lt;/code> function instead. You might need to change the defaults of your Twitter app from &amp;ldquo;Read only&amp;rdquo; to &amp;ldquo;Read, write and access direct messages&amp;rdquo;.&lt;/p>
&lt;p>Enter my API keys and access tokens (not shown), then authenticate.&lt;/p>
&lt;pre>&lt;code class="language-r">token &amp;lt;- create_token(
app = &amp;quot;sna-twitter-network-5k&amp;quot;,
consumer_key = api_key,
consumer_secret = api_secret_key,
access_token = access_token,
access_secret = access_token_secret)
&lt;/code>&lt;/pre>
&lt;p>Get my Twitter info with my description, number of followers, number of likes, etc.&lt;/p>
&lt;pre>&lt;code class="language-r">og &amp;lt;- lookup_users(&amp;quot;oaggimenez&amp;quot;)
str(og, max = 2)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## 'data.frame': 1 obs. of 21 variables:
## $ id : num 7.51e+17
## $ id_str : chr &amp;quot;750892662224453632&amp;quot;
## $ name : chr &amp;quot;Olivier Gimenez \U0001f596&amp;quot;
## $ screen_name : chr &amp;quot;oaggimenez&amp;quot;
## $ location : chr &amp;quot;Montpellier, France&amp;quot;
## $ description : chr &amp;quot;Scientist (he/him) @CNRS @cefemontpellier @twitthair1 â€¢ Grown statistician â€¢ Improvised ecologist â€¢ Sociologist&amp;quot;| __truncated__
## $ url : chr &amp;quot;https://t.co/l7NImYeGdY&amp;quot;
## $ protected : logi FALSE
## $ followers_count : int 5042
## $ friends_count : int 2191
## $ listed_count : int 68
## $ created_at : chr &amp;quot;Thu Jul 07 03:22:16 +0000 2016&amp;quot;
## $ favourites_count : int 18066
## $ verified : logi FALSE
## $ statuses_count : int 5545
## $ profile_image_url_https: chr &amp;quot;https://pbs.twimg.com/profile_images/1330619806396067845/mIPmR-x4_normal.jpg&amp;quot;
## $ profile_banner_url : chr &amp;quot;https://pbs.twimg.com/profile_banners/750892662224453632/1602417664&amp;quot;
## $ default_profile : logi FALSE
## $ default_profile_image : logi FALSE
## $ withheld_in_countries :List of 1
## ..$ : list()
## $ entities :List of 1
## ..$ :List of 2
## - attr(*, &amp;quot;tweets&amp;quot;)='data.frame': 1 obs. of 36 variables:
## ..$ created_at : chr &amp;quot;Thu Jul 29 16:15:52 +0000 2021&amp;quot;
## ..$ id : num 1.42e+18
## ..$ id_str : chr &amp;quot;1420780122949529601&amp;quot;
## ..$ text : chr &amp;quot;Looking forward to digging into this paper \U0001f929\U0001f9ee\U0001f60d https://t.co/ddCMraM3kj&amp;quot;
## ..$ truncated : logi FALSE
## ..$ entities :List of 1
## ..$ source : chr &amp;quot;&amp;lt;a href=\&amp;quot;http://twitter.com/download/iphone\&amp;quot; rel=\&amp;quot;nofollow\&amp;quot;&amp;gt;Twitter for iPhone&amp;lt;/a&amp;gt;&amp;quot;
## ..$ in_reply_to_status_id : logi NA
## ..$ in_reply_to_status_id_str : logi NA
## ..$ in_reply_to_user_id : logi NA
## ..$ in_reply_to_user_id_str : logi NA
## ..$ in_reply_to_screen_name : logi NA
## ..$ geo : logi NA
## ..$ coordinates :List of 1
## ..$ place :List of 1
## ..$ contributors : logi NA
## ..$ is_quote_status : logi TRUE
## ..$ quoted_status_id : num 1.42e+18
## ..$ quoted_status_id_str : chr &amp;quot;1420768369620312065&amp;quot;
## ..$ retweet_count : int 0
## ..$ favorite_count : int 8
## ..$ favorited : logi FALSE
## ..$ retweeted : logi FALSE
## ..$ possibly_sensitive : logi FALSE
## ..$ lang : chr &amp;quot;en&amp;quot;
## ..$ quoted_status :List of 1
## ..$ display_text_width : int 70
## ..$ user :List of 1
## ..$ full_text : logi NA
## ..$ favorited_by : logi NA
## ..$ display_text_range : logi NA
## ..$ retweeted_status : logi NA
## ..$ quoted_status_permalink : logi NA
## ..$ metadata : logi NA
## ..$ query : logi NA
## ..$ possibly_sensitive_appealable: logi NA
&lt;/code>&lt;/pre>
&lt;p>Now I obtain the id of my followers using &lt;code>get_followers&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-r">followers &amp;lt;- get_followers(user = &amp;quot;oaggimenez&amp;quot;,
n = og$followers_count,
retryonratelimit = T)
&lt;/code>&lt;/pre>
&lt;p>From their id, I can get the same details I got on my account using &lt;code>lookup_users&lt;/code>. This function is not vectorized, therefore I use a loop. Takes some time so I saved the results and load them.&lt;/p>
&lt;pre>&lt;code class="language-r">details_followers &amp;lt;- NULL
for (i in 1:length(followers$user_id)){
tmp &amp;lt;- try(lookup_users(followers$user_id[i], retryonratelimit = TRUE), silent = TRUE)
if (length(tmp) == 1){
next
} else {
tmp$listed_count &amp;lt;- NULL # get rid of this column which raised some format issues, we do not it anyway
details_followers &amp;lt;- bind_rows(details_followers, tmp)
}
}
save(details_followers, file = &amp;quot;details_followers.RData&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>What info do we have?&lt;/p>
&lt;pre>&lt;code class="language-r">load(&amp;quot;dat/details_followers.RData&amp;quot;)
names(details_followers)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;id&amp;quot; &amp;quot;id_str&amp;quot;
## [3] &amp;quot;name&amp;quot; &amp;quot;screen_name&amp;quot;
## [5] &amp;quot;location&amp;quot; &amp;quot;description&amp;quot;
## [7] &amp;quot;url&amp;quot; &amp;quot;protected&amp;quot;
## [9] &amp;quot;followers_count&amp;quot; &amp;quot;friends_count&amp;quot;
## [11] &amp;quot;created_at&amp;quot; &amp;quot;favourites_count&amp;quot;
## [13] &amp;quot;verified&amp;quot; &amp;quot;statuses_count&amp;quot;
## [15] &amp;quot;profile_image_url_https&amp;quot; &amp;quot;profile_banner_url&amp;quot;
## [17] &amp;quot;default_profile&amp;quot; &amp;quot;default_profile_image&amp;quot;
## [19] &amp;quot;withheld_in_countries&amp;quot; &amp;quot;entities&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>In more details.&lt;/p>
&lt;pre>&lt;code class="language-r">str(details_followers, max = 1)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## 'data.frame': 5025 obs. of 20 variables:
## $ id : num 1.03e+18 1.42e+18 1.32e+09 1.42e+18 1.36e+18 ...
## $ id_str : chr &amp;quot;1026053171024736258&amp;quot; &amp;quot;1419513064697712640&amp;quot; &amp;quot;1317282258&amp;quot; &amp;quot;1419222504367927296&amp;quot; ...
## $ name : chr &amp;quot;Dr. Zoe Nhleko&amp;quot; &amp;quot;akash Shelke&amp;quot; &amp;quot;Rilwan Ugbedeojo&amp;quot; &amp;quot;Thaana Van Dessel&amp;quot; ...
## $ screen_name : chr &amp;quot;ZoeNhleko&amp;quot; &amp;quot;akashSh88945929&amp;quot; &amp;quot;abuhrilwan&amp;quot; &amp;quot;ThaanaD&amp;quot; ...
## $ location : chr &amp;quot;United States&amp;quot; &amp;quot;&amp;quot; &amp;quot;Lagos, Nigeria&amp;quot; &amp;quot;France&amp;quot; ...
## $ description : chr &amp;quot;Wildlife ecologist with experience on African large mammals. PhD from @UF. Looking for job opportunities in eco&amp;quot;| __truncated__ &amp;quot;&amp;quot; &amp;quot;MSc, Plant Ecology | Seeking PhD Position in Community Ecology | Civic Leader | YALI RLC &amp;amp; African Presidential&amp;quot;| __truncated__ &amp;quot;MSc student majoring in Animal Ecology @WURanimal. \nFocused on Conservation Behavior &amp;amp; Human-Wildlife Coexistence.&amp;quot; ...
## $ url : chr &amp;quot;https://t.co/uaFY2L8BnL&amp;quot; NA NA NA ...
## $ protected : logi FALSE FALSE FALSE FALSE FALSE FALSE ...
## $ followers_count : int 2878 1 244 0 2856 4 112 6 69 476 ...
## $ friends_count : int 1325 223 2030 14 2575 99 344 32 73 1671 ...
## $ created_at : chr &amp;quot;Sun Aug 05 10:31:53 +0000 2018&amp;quot; &amp;quot;Mon Jul 26 04:21:18 +0000 2021&amp;quot; &amp;quot;Sat Mar 30 22:35:27 +0000 2013&amp;quot; &amp;quot;Sun Jul 25 09:06:36 +0000 2021&amp;quot; ...
## $ favourites_count : int 8003 0 1843 1 12360 0 484 0 83 713 ...
## $ verified : logi FALSE FALSE FALSE FALSE FALSE FALSE ...
## $ statuses_count : int 7261 0 670 1 2112 0 141 0 30 926 ...
## $ profile_image_url_https: chr &amp;quot;https://pbs.twimg.com/profile_images/1306353853571280896/rk9qAxoa_normal.jpg&amp;quot; &amp;quot;https://pbs.twimg.com/profile_images/1419513134142738433/erR2XRwf_normal.png&amp;quot; &amp;quot;https://pbs.twimg.com/profile_images/1413368052012433413/EyzbYfWB_normal.jpg&amp;quot; &amp;quot;https://pbs.twimg.com/profile_images/1419222962566217731/TQT96npR_normal.jpg&amp;quot; ...
## $ profile_banner_url : chr &amp;quot;https://pbs.twimg.com/profile_banners/1026053171024736258/1533465589&amp;quot; NA &amp;quot;https://pbs.twimg.com/profile_banners/1317282258/1624183687&amp;quot; &amp;quot;https://pbs.twimg.com/profile_banners/1419222504367927296/1627205254&amp;quot; ...
## $ default_profile : logi TRUE TRUE FALSE TRUE TRUE TRUE ...
## $ default_profile_image : logi FALSE FALSE FALSE FALSE FALSE FALSE ...
## $ withheld_in_countries :List of 5025
## $ entities :List of 5025
## - attr(*, &amp;quot;tweets&amp;quot;)='data.frame': 1 obs. of 36 variables:
&lt;/code>&lt;/pre>
&lt;h2 id="data-exploration-and-visualisation">Data exploration and visualisation&lt;/h2>
&lt;p>Let&amp;rsquo;s display the 100 bigger accounts that follow me. First thing I learned. It is humbling to be followed by influential individuals I admire like @MicrobiomDigest, @nathanpsmad, @FrancoisTaddei, @freakonometrics, @allison_horst, @HugePossum, @apreshill and scientific journals, institutions and societies.&lt;/p>
&lt;pre>&lt;code class="language-r">details_followers %&amp;gt;%
arrange(-followers_count) %&amp;gt;%
select(screen_name,
followers_count,
friends_count,
favourites_count) %&amp;gt;%
head(n = 100)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## screen_name followers_count friends_count favourites_count
## 1 dianefrancis1 230560 132635 70
## 2 simongerman600 218444 209489 56976
## 3 WildlifeMag 191613 23995 14697
## 4 cagrimbakirci 170164 9401 21334
## 5 BernardMarr 130900 29532 15764
## 6 ZotovMax 119625 133610 894
## 7 _atanas_ 116717 74266 102899
## 8 MicrobiomDigest 105825 33779 98199
## 9 Eliances 97488 94488 15164
## 10 PLOSBiology 89263 6646 4278
## 11 IPBES 78086 11797 53957
## 12 7wData 76131 84155 74609
## 13 biconnections 63802 52918 91970
## 14 aydintufekci43 60212 22627 99427
## 15 AliceVachet 56381 2320 182435
## 16 Pr_Logos 52482 50520 19852
## 17 DrJoeNyangon 51296 20858 1413
## 18 derekantoncich 48823 51449 347
## 19 Afro_Herper 45215 3392 117693
## 20 KumarAGarg 43461 47795 4052
## 21 GatelyMark 41500 37944 1153
## 22 overleaf 41306 7830 35289
## 23 figshare 40610 38361 10395
## 24 mlamons1 40415 29364 2498
## 25 coraman 40398 2169 16548
## 26 BritishEcolSoc 37394 1143 11685
## 27 nathanpsmad 36760 5312 43729
## 28 WildlifeRescue 34843 7976 61284
## 29 rudyagovic 34063 28147 3708
## 30 grp_resilience 34011 7998 3559
## 31 JNakev 33427 28532 29442
## 32 FrancoisTaddei 32631 17903 23894
## 33 RunEducator 31248 19848 17522
## 34 LarryLLapin 30782 34265 501
## 35 _Alex_Iaco_ 30775 26720 2390
## 36 owasow 30562 12932 36909
## 37 JEcology 30534 689 3312
## 38 valmasdel 28972 4512 4334
## 39 freakonometrics 28038 15112 31106
## 40 MethodsEcolEvol 26207 10347 2083
## 41 mmw_lmw 26014 12157 1068
## 42 drmichellelarue 25424 6281 56120
## 43 nicolasberrod 25258 6042 12990
## 44 DD_NaNa_ 24253 11051 39916
## 45 callin_bull 24106 2831 10572
## 46 INEE_CNRS 23483 1873 5089
## 47 Datascience__ 22793 10695 216
## 48 VisualPersist 22779 5599 79872
## 49 coywolfassoc 22203 19491 9470
## 50 RBGSydney 20259 16049 13435
## 51 allison_horst 19289 2865 7725
## 52 rhskraus 18429 20029 3503
## 53 fred_sallee 18144 18995 3423
## 54 thembauk 17748 2066 3260
## 55 parolesdhist 17257 7136 7497
## 56 CREAF_ecologia 16654 3201 11899
## 57 ChamboncelLea 16609 4059 9797
## 58 HugePossum 16461 12732 7806
## 59 ScientistFemale 16379 9039 2079
## 60 sophie_e_hill 16292 5811 10315
## 61 mohamma64508589 16130 14550 24584
## 62 SamuelHayat 15999 3068 9767
## 63 MathieuAvanzi 15890 1604 8526
## 64 Erky321 15859 16284 4841
## 65 ConLetters 15482 5547 1489
## 66 oceansresearch 14969 1759 7599
## 67 WCSNewsroom 14926 11224 7277
## 68 CMastication 14687 8091 31769
## 69 apreshill 14106 1607 32806
## 70 SumanMalla10 13872 11977 10946
## 71 umontpellier 13667 1360 8745
## 72 WriteNThrive 13615 13523 7631
## 73 abdirashidmd 13586 2389 18417
## 74 JackKinross 13392 11263 774
## 75 PaulREhrlich 13383 14208 1
## 76 teppofelin 13330 12410 264
## 77 ConservOptimism 13249 4306 15648
## 78 jepeteso 13137 13595 3420
## 79 gwintrob 12484 8751 43479
## 80 SallyR_ISLHE 12411 12477 28924
## 81 ActuIAFr 12321 8856 2495
## 82 Booksmag 12282 1785 332
## 83 jusoneoftoomany 12212 11049 2296
## 84 galaxyproject 12208 8757 813
## 85 thepostdoctoral 12186 12802 814
## 86 FrontMarineSci 11498 5733 5832
## 87 jobRxiv 11442 872 786
## 88 EcographyJourna 11381 1465 4938
## 89 davidghamilton1 11242 2231 16244
## 90 5amStartups 11170 12370 147062
## 91 RosmarieKatrin 11079 11159 4060
## 92 ScienceAndMaps 11043 8679 3596
## 93 joelgombin 11012 10335 10868
## 94 SteveLenhert 10990 8328 669
## 95 Naturevolve 10973 11349 7500
## 96 cookinforpeace 10655 2861 17519
## 97 DrPaulEvans1 10537 7927 3131
## 98 Ibycter 10517 4450 47359
## 99 Sciencecomptoir 10436 1268 15800
## 100 SteveAtLogikk 10421 8864 13123
&lt;/code>&lt;/pre>
&lt;p>Where do my followers come from? Second thing I learnt. Followers are folks from France (my home), USA, UK, Canada, Australia, and other European countries. I was happy to realize that I have followers from other parts of the world too, in India, South Africa, Brazil, Peru, Mexico, Nepal, Argentina, Kenya, Bangladesh, etc. and from Greece (my other home).&lt;/p>
&lt;pre>&lt;code class="language-r">details_followers %&amp;gt;%
mutate(location = str_replace(location, &amp;quot;.*France.*&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*England&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*United Kingdom&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*London&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*Scotland&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*Montpellier&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*Canada, BC.*&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*New Caledonia&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*Marseille&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*Grenoble&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*PerÃº&amp;quot;, &amp;quot;Peru&amp;quot;),
location = str_replace(location, &amp;quot;.*Martinique&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*La Rochelle&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*Grenoble, RhÃ´ne-Alpes&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*Avignon&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*Paris&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*paris&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*france&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*Bordeaux&amp;quot;, &amp;quot;France&amp;quot;),
location = str_replace(location, &amp;quot;.*Mytilene, Lesvos, Greece&amp;quot;, &amp;quot;Greece&amp;quot;),
location = str_replace(location, &amp;quot;.*Mytiline, Greece&amp;quot;, &amp;quot;Greece&amp;quot;),
location = str_replace(location, &amp;quot;.*Germany.*&amp;quot;, &amp;quot;Germany&amp;quot;),
location = str_replace(location, &amp;quot;.*Vienna, Austria&amp;quot;, &amp;quot;Austria&amp;quot;),
location = str_replace(location, &amp;quot;.*Arusha, Tanzania&amp;quot;, &amp;quot;Tanzania&amp;quot;),
location = str_replace(location, &amp;quot;.*Athens&amp;quot;, &amp;quot;Greece&amp;quot;),
location = str_replace(location, &amp;quot;.*Bangor, ME&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*Bangor, Wales&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*Baton Rouge, LA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*Lisboa, Portugal&amp;quot;, &amp;quot;Portugal&amp;quot;),
location = str_replace(location, &amp;quot;.*Oslo&amp;quot;, &amp;quot;Norway&amp;quot;),
location = str_replace(location, &amp;quot;.*Islamic Republic of Iran&amp;quot;, &amp;quot;Iran&amp;quot;),
location = str_replace(location, &amp;quot;.*Madrid, EspaÃ±a&amp;quot;, &amp;quot;Spain&amp;quot;),
location = str_replace(location, &amp;quot;.*Madrid&amp;quot;, &amp;quot;Spain&amp;quot;),
location = str_replace(location, &amp;quot;.*Madrid, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
location = str_replace(location, &amp;quot;.*Rome, Italy&amp;quot;, &amp;quot;Italy&amp;quot;),
location = str_replace(location, &amp;quot;.*Santiago, Chile&amp;quot;, &amp;quot;Chile&amp;quot;),
location = str_replace(location, &amp;quot;.*BelÃ©m, Brazil&amp;quot;, &amp;quot;Brazil&amp;quot;),
location = str_replace(location, &amp;quot;.*Belo Horizonte, Brazil&amp;quot;, &amp;quot;Brazil&amp;quot;),
location = str_replace(location, &amp;quot;.*Berlin&amp;quot;, &amp;quot;Germany&amp;quot;),
location = str_replace(location, &amp;quot;.*Mexico, ME&amp;quot;, &amp;quot;Mexico&amp;quot;),
location = str_replace(location, &amp;quot;.*Mexico City&amp;quot;, &amp;quot;Mexico&amp;quot;),
location = str_replace(location, &amp;quot;.*Santiago, Chile&amp;quot;, &amp;quot;Chile&amp;quot;),
location = str_replace(location, &amp;quot;.*Ontario&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Aberdeen&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*Cambridge&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*Bologna, Emilia Romagna&amp;quot;, &amp;quot;Italy&amp;quot;),
location = str_replace(location, &amp;quot;.*BogotÃ¡, D.C., Colombia&amp;quot;, &amp;quot;Colombia&amp;quot;),
location = str_replace(location, &amp;quot;.*MedellÃ­n, Colombia&amp;quot;, &amp;quot;Colombia&amp;quot;),
location = str_replace(location, &amp;quot;.*Bruxelles, Belgique&amp;quot;, &amp;quot;Belgium&amp;quot;),
location = str_replace(location, &amp;quot;.*Brasil&amp;quot;, &amp;quot;Brazil&amp;quot;),
location = str_replace(location, &amp;quot;.*Budweis, Czech Republic&amp;quot;, &amp;quot;Czech Republic&amp;quot;),
location = str_replace(location, &amp;quot;.*Calgary, Alberta&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Canada, BC&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Cardiff, Wales&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*Brisbane&amp;quot;, &amp;quot;Australia&amp;quot;),
location = str_replace(location, &amp;quot;.*Sydney&amp;quot;, &amp;quot;Australia&amp;quot;),
location = str_replace(location, &amp;quot;.*Queensland&amp;quot;, &amp;quot;Australia&amp;quot;),
location = str_replace(location, &amp;quot;.*Australia&amp;quot;, &amp;quot;Australia&amp;quot;),
location = str_replace(location, &amp;quot;.*Germany&amp;quot;, &amp;quot;Germany&amp;quot;),
location = str_replace(location, &amp;quot;.*Vancouver&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Ottawa, Ontario&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*QuÃ©bec, Canada&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Winnipeg, Manitoba&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*New South Wales&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Victoria&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*British Columbia&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Norway&amp;quot;, &amp;quot;Norway&amp;quot;),
location = str_replace(location, &amp;quot;.*Finland&amp;quot;, &amp;quot;Finland&amp;quot;),
location = str_replace(location, &amp;quot;.*South Africa&amp;quot;, &amp;quot;South Africa&amp;quot;),
location = str_replace(location, &amp;quot;.*Switzerland&amp;quot;, &amp;quot;Switzerland&amp;quot;),
location = str_replace(location, &amp;quot;.*CO&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*OK&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*KS&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*MS&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*CO&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*CO&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*CO&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*CO&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*WA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*MD&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*Colorado&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*Community of Valencia, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
location = str_replace(location, &amp;quot;.*Dunedin City, New Zealand&amp;quot;, &amp;quot;New Zealand&amp;quot;),
location = str_replace(location, &amp;quot;.*Rio Claro, Brasil&amp;quot;, &amp;quot;Brazil&amp;quot;),
location = str_replace(location, &amp;quot;.*Saskatoon, Saskatchewan&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Sherbrooke, QuÃ©bec&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*United Kingdom.*&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*University of Oxford&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*University of St Andrews&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*ID&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*NE&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*United States of America, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*Spain, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
location = str_replace(location, &amp;quot;.*USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*Wisconsin, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*Florida, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*Liege, Belgium&amp;quot;, &amp;quot;Belgium&amp;quot;),
location = str_replace(location, &amp;quot;.*Ghent, Belgium&amp;quot;, &amp;quot;Belgium&amp;quot;),
location = str_replace(location, &amp;quot;.*Pune, India&amp;quot;, &amp;quot;India&amp;quot;),
location = str_replace(location, &amp;quot;.*Hyderabad, India&amp;quot;, &amp;quot;India&amp;quot;),
location = str_replace(location, &amp;quot;.*Prague, Czech Republic&amp;quot;, &amp;quot;Czech Republic&amp;quot;),
location = str_replace(location, &amp;quot;.*Canada, BC, Canada&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*CA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*United States.*&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*DC&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*FL&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*GA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*HI&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*ME&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*MA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*MI&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*PA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*NC&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*MO&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*NY&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*NH&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*IL&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*NM&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*MT&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*OR&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*WY&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*WI&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*MN&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*CT&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*TX&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*VA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*OH&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*Massachusetts, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*California, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*MontrÃ©al, QuÃ©bec&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Edmonton, Alberta&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Toronto, Ontario&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Canada, Canada&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Montreal&amp;quot;, &amp;quot;Canada&amp;quot;),
location = str_replace(location, &amp;quot;.*Lisbon, Portugal&amp;quot;, &amp;quot;Portugal&amp;quot;),
location = str_replace(location, &amp;quot;.*Coimbra, Portugal&amp;quot;, &amp;quot;Portugal&amp;quot;),
location = str_replace(location, &amp;quot;.*Cork, Ireland&amp;quot;, &amp;quot;Ireland&amp;quot;),
location = str_replace(location, &amp;quot;.*Dublin City, Ireland&amp;quot;, &amp;quot;Ireland&amp;quot;),
location = str_replace(location, &amp;quot;.*Barcelona, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
location = str_replace(location, &amp;quot;.*Barcelona&amp;quot;, &amp;quot;Spain&amp;quot;),
location = str_replace(location, &amp;quot;.*Leipzig&amp;quot;, &amp;quot;Germany&amp;quot;),
location = str_replace(location, &amp;quot;.*Seville, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
location = str_replace(location, &amp;quot;.*Seville, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
location = str_replace(location, &amp;quot;.*Buenos Aires, Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;),
location = str_replace(location, &amp;quot;.*Rio de Janeiro, Brazil&amp;quot;, &amp;quot;Brazil&amp;quot;),
location = str_replace(location, &amp;quot;.*Canberra&amp;quot;, &amp;quot;Australia&amp;quot;),
location = str_remove(location, &amp;quot;Global&amp;quot;),
location = str_remove(location, &amp;quot;Earth&amp;quot;),
location = str_remove(location, &amp;quot;Worldwide&amp;quot;),
location = str_remove(location, &amp;quot;Europe&amp;quot;),
location = str_remove(location, &amp;quot; &amp;quot;),
location = str_replace(location, &amp;quot;.*Dhaka, Bangladesh&amp;quot;, &amp;quot;Bangladesh&amp;quot;),
location = str_replace(location, &amp;quot;.*Copenhagen, Denmark&amp;quot;, &amp;quot;Denmark&amp;quot;),
location = str_replace(location, &amp;quot;.*Amsterdam, The Netherlands&amp;quot;, &amp;quot;The Netherlands&amp;quot;),
location = str_replace(location, &amp;quot;.*Groningen, Nederland&amp;quot;, &amp;quot;The Netherlands&amp;quot;),
location = str_replace(location, &amp;quot;.*Wageningen, Nederland&amp;quot;, &amp;quot;The Netherlands&amp;quot;),
location = str_replace(location, &amp;quot;.*Aarhus, Denmark&amp;quot;, &amp;quot;Denmark&amp;quot;),
location = str_replace(location, &amp;quot;.*Antwerp, Belgium&amp;quot;, &amp;quot;Belgium&amp;quot;),
location = str_replace(location, &amp;quot;.*Aveiro, Portugal&amp;quot;, &amp;quot;Portugal&amp;quot;),
location = str_replace(location, &amp;quot;.*Australia, AUS&amp;quot;, &amp;quot;Australia&amp;quot;),
location = str_replace(location, &amp;quot;.*Australian National University&amp;quot;, &amp;quot;Australia&amp;quot;),
location = str_replace(location, &amp;quot;.*Auckland, New Zealand&amp;quot;, &amp;quot;New Zealand&amp;quot;),
location = str_replace(location, &amp;quot;.*Belfast, Northern Ireland&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*Ireland&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*Hobart, Tasmania&amp;quot;, &amp;quot;Australia&amp;quot;),
location = str_replace(location, &amp;quot;.*Dhaka, Bangladesh&amp;quot;, &amp;quot;Bangladesh&amp;quot;),
location = str_replace(location, &amp;quot;.*Nairobi, Kenya&amp;quot;, &amp;quot;Kenya&amp;quot;),
location = str_replace(location, &amp;quot;.*Dhaka, Bangladesh&amp;quot;, &amp;quot;Bangladesh&amp;quot;),
location = str_replace(location, &amp;quot;.*Berlin, Deutschland&amp;quot;, &amp;quot;Germany&amp;quot;),
location = str_replace(location, &amp;quot;.*Munich, Bavaria&amp;quot;, &amp;quot;Germany&amp;quot;),
location = str_replace(location, &amp;quot;.*Dehradun, India&amp;quot;, &amp;quot;India&amp;quot;),
location = str_replace(location, &amp;quot;.*Bengaluru, India&amp;quot;, &amp;quot;India&amp;quot;),
location = str_replace(location, &amp;quot;.*Berlin, Deutschland&amp;quot;, &amp;quot;Germany&amp;quot;),
location = str_replace(location, &amp;quot;.*Deutschland&amp;quot;, &amp;quot;Germany&amp;quot;),
location = str_replace(location, &amp;quot;.*Edinburgh&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*Glasgow&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
location = str_replace(location, &amp;quot;.*New York, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*Washington, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*California&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*California&amp;quot;, &amp;quot;United States of America&amp;quot;),
location = str_replace(location, &amp;quot;.*Christchurch City, New Zealand&amp;quot;, &amp;quot;New Zealand&amp;quot;),
location = str_replace(location, &amp;quot;.*Harare, Zimbabwe&amp;quot;, &amp;quot;Zimbabwe&amp;quot;),
location = str_replace(location, &amp;quot;.*Islamabad, Pakistan&amp;quot;, &amp;quot;Pakistan&amp;quot;),
location = str_replace(location, &amp;quot;.*Kolkata, India&amp;quot;, &amp;quot;India&amp;quot;),
location = str_replace(location, &amp;quot;.*Lagos, Nigeria&amp;quot;, &amp;quot;Nigeria&amp;quot;),
location = str_replace(location, &amp;quot;.*Lima, Peru&amp;quot;, &amp;quot;Peru&amp;quot;),
location = str_replace(location, &amp;quot;.*ValparaÃ­so, Chile&amp;quot;, &amp;quot;Chile&amp;quot;),
location = str_replace(location, &amp;quot;.*Uppsala, Sweden&amp;quot;, &amp;quot;Sweden&amp;quot;),
location = str_replace(location, &amp;quot;.*Uppsala, Sverige&amp;quot;, &amp;quot;Sweden&amp;quot;),
location = str_replace(location, &amp;quot;.*Stockholm, Sweden&amp;quot;, &amp;quot;Sweden&amp;quot;),
location = str_replace(location, &amp;quot;.*Stockholm&amp;quot;, &amp;quot;Sweden&amp;quot;),
location = str_replace(location, &amp;quot;.*Turin, Piedmont&amp;quot;, &amp;quot;Italy&amp;quot;),
location = str_replace(location, &amp;quot;.*University of Iceland&amp;quot;, &amp;quot;Iceland&amp;quot;),
location = str_replace(location, &amp;quot;.*University of Helsinki&amp;quot;, &amp;quot;Finland&amp;quot;),
location = str_replace(location, &amp;quot;.*TucumÃ¡n, Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;),
location = str_replace(location, &amp;quot;.*The Hague, The Netherlands&amp;quot;, &amp;quot;The Netherlands&amp;quot;),
location = str_replace(location, &amp;quot;.*UK&amp;quot;, &amp;quot;United Kingdom&amp;quot;)
) %&amp;gt;%
count(location, sort = TRUE) %&amp;gt;%
#slice(-40) %&amp;gt;%
head(n = 45) -&amp;gt; followers_by_country
followers_by_country
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## location n
## 1 1273
## 2 United States of America 606
## 3 France 571
## 4 United Kingdom 473
## 5 Canada 166
## 6 Germany 130
## 7 Australia 112
## 8 Norway 50
## 9 Switzerland 43
## 10 Spain 34
## 11 India 33
## 12 South Africa 32
## 13 Brazil 30
## 14 Portugal 27
## 15 Finland 26
## 16 Sweden 18
## 17 Belgium 17
## 18 Italy 16
## 19 The Netherlands 14
## 20 New Zealand 13
## 21 Peru 13
## 22 Denmark 11
## 23 Mexico 11
## 24 Nepal 10
## 25 Argentina 9
## 26 Kenya 9
## 27 Bangladesh 8
## 28 Chile 8
## 29 Greece 8
## 30 Colombia 6
## 31 Czech Republic 6
## 32 Singapore 6
## 33 Iceland 5
## 34 Senegal 5
## 35 Iran 4
## 36 Nigeria 4
## 37 Pakistan 4
## 38 Tanzania 4
## 39 Zimbabwe 4
## 40 3
## 41 Austria 3
## 42 Ecuador 3
## 43 Guatemala 3
## 44 Hong Kong 3
## 45 Luxembourg 3
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s have a look to the same info on a quick and dirty map. White countries do not appear in the 45 countries with most followers, or do not have any followers.&lt;/p>
&lt;pre>&lt;code class="language-r">library(rworldmap)
library(classInt)
spdf &amp;lt;- joinCountryData2Map(followers_by_country,
joinCode=&amp;quot;NAME&amp;quot;,
nameJoinColumn=&amp;quot;location&amp;quot;,
verbose=TRUE)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## 43 codes from your data successfully matched countries in the map
## 2 codes from your data failed to match with a country code in the map
## failedCodes failedCountries
## [1,] &amp;quot;&amp;quot; &amp;quot;&amp;quot;
## [2,] &amp;quot;&amp;quot; &amp;quot; &amp;quot;
## 200 codes from the map weren't represented in your data
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">classInt &amp;lt;- classIntervals(spdf$n,
n=9,
style = &amp;quot;jenks&amp;quot;)
catMethod &amp;lt;- classInt[[&amp;quot;brks&amp;quot;]]
library(RColorBrewer)
colourPalette &amp;lt;- brewer.pal(9,'RdPu')
mapParams &amp;lt;- mapCountryData(spdf,
nameColumnToPlot=&amp;quot;n&amp;quot;,
addLegend=FALSE,
catMethod = catMethod,
colourPalette=colourPalette,
mapTitle=&amp;quot;Number of followers per country&amp;quot;)
do.call(addMapLegend,
c(mapParams,
legendLabels=&amp;quot;all&amp;quot;,
legendWidth=0.5,
legendIntervals=&amp;quot;data&amp;quot;,
legendMar = 2))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-12-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;p>How many followers are verified?&lt;/p>
&lt;pre>&lt;code class="language-r">details_followers %&amp;gt;%
count(verified)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## verified n
## 1 FALSE 5009
## 2 TRUE 16
&lt;/code>&lt;/pre>
&lt;p>Who are the verified users? Scientific entities, but also some scientists.&lt;/p>
&lt;pre>&lt;code class="language-r">details_followers %&amp;gt;%
filter(verified==TRUE) %&amp;gt;%
select(name, screen_name, location, followers_count) %&amp;gt;%
arrange(-followers_count)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## name screen_name
## 1 Ã‡aÄŸrÄ± Mert BakÄ±rcÄ± cagrimbakirci
## 2 Elisabeth Bik MicrobiomDigest
## 3 ipbes IPBES
## 4 Dr. Earyn McGee, Lizard lassoer \U0001f98e Afro_Herper
## 5 figshare figshare
## 6 Nathan Peiffer-Smadja nathanpsmad
## 7 Wildlife Alliance WildlifeRescue
## 8 Arthur Charpentier freakonometrics
## 9 L'Ã©cologie au CNRS INEE_CNRS
## 10 The Royal Botanic Garden Sydney RBGSydney
## 11 CREAF CREAF_ecologia
## 12 Jean-Paul Carrera JeanCarrPaul
## 13 Deckset decksetapp
## 14 ð™³ðšžðšŒ-ðš€ðšžðšŠðš—ðš ð™½ðšðšžðš¢ðšŽðš— duc_qn
## 15 Christophe Josset \U0001f4dd CJosset
## 16 Casey Fung TheCaseyFung
## location followers_count
## 1 Texas, USA 170164
## 2 San Francisco, CA 105825
## 3 Bonn, Germany 78086
## 4 United States 45215
## 5 London 40610
## 6 Paris - London 36760
## 7 Phnom Penh, Cambodia 34843
## 8 MontrÃ©al, QuÃ©bec 28038
## 9 23483
## 10 Sydney, New South Wales 20259
## 11 Bellaterra (Barcelona) 16654
## 12 Oxford, England 5744
## 13 Your Mac 3884
## 14 Lausanne, Switzerland 2808
## 15 Paris, France 2002
## 16 Bundjalung Country | Mullum 1612
&lt;/code>&lt;/pre>
&lt;p>How old are my followers, where age is defined as the time elapsed since they created their Twitter account. Twitter was created in 2006. I created my account in July 2016 but started using it only two or three years ago I&amp;rsquo;d say. This dataviz is not too much informative I guess.&lt;/p>
&lt;pre>&lt;code class="language-r">details_followers %&amp;gt;%
mutate(month = created_at %&amp;gt;% str_split(&amp;quot; &amp;quot;) %&amp;gt;% map_chr(2),
day = created_at %&amp;gt;% str_split(&amp;quot; &amp;quot;) %&amp;gt;% map_chr(3),
year = created_at %&amp;gt;% str_split(&amp;quot; &amp;quot;) %&amp;gt;% map_chr(6),
created_at = paste0(month,&amp;quot;-&amp;quot;,year),
created_at = lubridate::my(created_at)) %&amp;gt;%
group_by(created_at) %&amp;gt;%
summarise(counts = n()) %&amp;gt;%
ggplot(aes(x = created_at, y = counts)) +
geom_line() +
labs(x = &amp;quot;date of creation&amp;quot;, y = &amp;quot;how many accounts were created this month-year&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-15-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;p>What is the distribution of the number of followers and friends. On average, 400 followers and 400 friends.&lt;/p>
&lt;pre>&lt;code class="language-r">foll &amp;lt;- details_followers %&amp;gt;%
ggplot() +
aes(x = log(followers_count)) +
geom_histogram(fill=&amp;quot;#69b3a2&amp;quot;, color=&amp;quot;#e9ecef&amp;quot;, alpha=0.9) +
labs(x = &amp;quot;log number of followers&amp;quot;, y = &amp;quot;counts&amp;quot;, title = &amp;quot;followers&amp;quot;)
friends &amp;lt;- details_followers %&amp;gt;%
ggplot() +
aes(x = log(friends_count)) +
geom_histogram(fill=&amp;quot;#69b3a2&amp;quot;, color=&amp;quot;#e9ecef&amp;quot;, alpha=0.9) +
labs(x = &amp;quot;log number of friends&amp;quot;, y = &amp;quot;&amp;quot;, title = &amp;quot;friends&amp;quot;)
library(patchwork)
foll + friends
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-16-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;h2 id="retrieving-social-links">Retrieving social links&lt;/h2>
&lt;p>To investigate the social network made of my followers, we need to gather who they follow, and their followers. Unfortunately, the Twitter API imposes some time and quantity constraints in retrieving data.&lt;/p>
&lt;p>Here, for the sake of illustration, I will focus on some accounts only. To do so, I consider accounts with between 1000 and 1500 followers, and more than 30000 tweets users have liked in their account&amp;rsquo;s lifetime (which denotes some activity). The more followers we consider, the more time it would take to gather info on them: it is approx one minute per follower on average, so one hour for sixty followers, and more than eighty hours for all my followers! With the filters I&amp;rsquo;m using, I end up with 35 followers here, this is very few to say infer meaningful about a network, but it&amp;rsquo;ll do the job for now.&lt;/p>
&lt;pre>&lt;code class="language-r">some_followers &amp;lt;- details_followers %&amp;gt;%
filter((followers_count &amp;gt; 1000 &amp;amp; followers_count &amp;lt; 1500),
favourites_count &amp;gt; 30000) # %&amp;gt;% nrow()
&lt;/code>&lt;/pre>
&lt;p>Create empty list and name it after their screen name.&lt;/p>
&lt;pre>&lt;code class="language-r">foler &amp;lt;- vector(mode = 'list', length = length(some_followers$screen_name))
names(foler) &amp;lt;- some_followers$screen_name
&lt;/code>&lt;/pre>
&lt;p>Get followers of these selected followers. Takes ages, so save and load later.&lt;/p>
&lt;pre>&lt;code class="language-r">for (i in 1:length(some_followers$screen_name)) {
message(&amp;quot;Getting followers for user #&amp;quot;, i, &amp;quot;/&amp;quot;, nrow(some_followers))
foler[[i]] &amp;lt;- get_followers(some_followers$screen_name[i],
n = some_followers$followers_count[i],
retryonratelimit = TRUE)
if(i %% 5 == 0){
message(&amp;quot;sleep for 5 minutes&amp;quot;)
Sys.sleep(5*60)
}
}
save(foler, file = &amp;quot;foler.RData&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Format the data.&lt;/p>
&lt;pre>&lt;code class="language-r">load(&amp;quot;dat/foler.RData&amp;quot;)
folerx &amp;lt;- bind_rows(foler, .id = &amp;quot;screen_name&amp;quot;)
active_fol_x &amp;lt;- some_followers %&amp;gt;% select(id_str, screen_name)
foler_join &amp;lt;- left_join(folerx, some_followers, by = &amp;quot;screen_name&amp;quot;)
algo_follower &amp;lt;- foler_join %&amp;gt;%
select(id_str, screen_name) %&amp;gt;%
setNames(c(&amp;quot;follower&amp;quot;, &amp;quot;active_user&amp;quot;)) %&amp;gt;%
na.omit()
&lt;/code>&lt;/pre>
&lt;p>Get friends of my followers. Takes ages. Again I save and load later.&lt;/p>
&lt;pre>&lt;code class="language-r">friend &amp;lt;- data.frame()
for (i in seq_along(some_followers$screen_name)) {
message(&amp;quot;Getting following for user #&amp;quot;, i ,&amp;quot;/&amp;quot;,nrow(some_followers))
kk &amp;lt;- get_friends(some_followers$screen_name[i],
n = some_followers$friends_count[i],
retryonratelimit = TRUE)
friend &amp;lt;- rbind(friend, kk)
if(i %% 15 == 0){
message(&amp;quot;sleep for 15 minutes&amp;quot;)
Sys.sleep(15*60+1)
}
}
save(friend, file = &amp;quot;friend.RData&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Format the data.&lt;/p>
&lt;pre>&lt;code class="language-r">load(&amp;quot;dat/friend.RData&amp;quot;)
all_friend &amp;lt;- friend %&amp;gt;% setNames(c(&amp;quot;screen_name&amp;quot;, &amp;quot;user_id&amp;quot;))
all_friendx &amp;lt;- left_join(all_friend, active_fol_x, by=&amp;quot;screen_name&amp;quot;)
algo_friend &amp;lt;- all_friendx %&amp;gt;% select(user_id, screen_name) %&amp;gt;%
setNames(c(&amp;quot;following&amp;quot;,&amp;quot;active_user&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>Now that we have all info on followers and friends, we&amp;rsquo;re gonna build the network of people who follow each other.&lt;/p>
&lt;pre>&lt;code class="language-r">un_active &amp;lt;- unique(algo_friend$active_user) %&amp;gt;%
data.frame(stringsAsFactors = F) %&amp;gt;%
setNames(&amp;quot;active_user&amp;quot;)
algo_mutual &amp;lt;- data.frame()
for (i in seq_along(un_active$active_user)){
aa &amp;lt;- algo_friend %&amp;gt;%
filter(active_user == un_active$active_user[i])
bb &amp;lt;- aa %&amp;gt;% filter(aa$following %in% algo_follower$follower) %&amp;gt;%
setNames(c(&amp;quot;mutual&amp;quot;,&amp;quot;active_user&amp;quot;))
algo_mutual &amp;lt;- rbind(algo_mutual,bb)
}
&lt;/code>&lt;/pre>
&lt;p>Instead of ids, we use screen names.&lt;/p>
&lt;pre>&lt;code class="language-r">detail_friend &amp;lt;- lookup_users(algo_mutual$mutual)
algo_mutual &amp;lt;- algo_mutual %&amp;gt;%
left_join(detail_friend, by = c(&amp;quot;mutual&amp;quot; = &amp;quot;id_str&amp;quot;)) %&amp;gt;%
na.omit() %&amp;gt;%
select(mutual, active_user, screen_name)
algo_mutual
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## # A tibble: 18 Ã— 3
## mutual active_user screen_name
## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
## 1 824265743575306243 g33k5p34k robbie_emmet
## 2 761735501707567104 g33k5p34k LauraBlissEco
## 3 1439272374 alexnicolharper LeafyEricScott
## 4 824265743575306243 alexnicolharper robbie_emmet
## 5 21467726 alexnicolharper g33k5p34k
## 6 4181949437 LegalizeBrain EvpokPadding
## 7 824265743575306243 Souzam7139 robbie_emmet
## 8 888890201673580548 Souzam7139 mellenmartin
## 9 824265743575306243 mellenmartin robbie_emmet
## 10 21467726 LauraBlissEco g33k5p34k
## 11 831398755920445440 JosiahParry RoelandtN42
## 12 4181949437 gau EvpokPadding
## 13 21467726 robbie_emmet g33k5p34k
## 14 4159201575 robbie_emmet alexnicolharper
## 15 888890201673580548 robbie_emmet mellenmartin
## 16 29538964 EvpokPadding gau
## 17 1439272374 lifedispersing LeafyEricScott
## 18 301687349 RoelandtN42 JosiahParry
&lt;/code>&lt;/pre>
&lt;p>Add my account to the network.&lt;/p>
&lt;pre>&lt;code class="language-r">un_active &amp;lt;- un_active %&amp;gt;%
mutate(mutual = rep(&amp;quot;oaggimenez&amp;quot;))
un_active &amp;lt;- un_active[,c(2,1)]
un_active &amp;lt;- un_active %&amp;gt;%
setNames(c(&amp;quot;active_user&amp;quot;,&amp;quot;screen_name&amp;quot;))
algo_mutual &amp;lt;- bind_rows(algo_mutual %&amp;gt;% select(-mutual), un_active)
&lt;/code>&lt;/pre>
&lt;p>For what follows, we will need packages to work with networks.&lt;/p>
&lt;pre>&lt;code class="language-r">library(igraph)
library(tidygraph)
library(ggraph)
&lt;/code>&lt;/pre>
&lt;p>Now we create the edges, nodes and build the network.&lt;/p>
&lt;pre>&lt;code class="language-r">nodes &amp;lt;- data.frame(V = unique(c(algo_mutual$screen_name,algo_mutual$active_user)),
stringsAsFactors = F)
edges &amp;lt;- algo_mutual %&amp;gt;%
setNames(c(&amp;quot;from&amp;quot;,&amp;quot;to&amp;quot;))
network_ego1 &amp;lt;- graph_from_data_frame(d = edges, vertices = nodes, directed = F) %&amp;gt;%
as_tbl_graph()
&lt;/code>&lt;/pre>
&lt;h2 id="network-metrics">Network metrics&lt;/h2>
&lt;p>Create communities using &lt;code>group_louvain()&lt;/code> algorithm, and calculate standard metrics using &lt;code>tidygraph&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-r">set.seed(123)
network_ego1 &amp;lt;- network_ego1 %&amp;gt;%
activate(nodes) %&amp;gt;%
mutate(community = as.factor(group_louvain())) %&amp;gt;%
mutate(degree_c = centrality_degree()) %&amp;gt;%
mutate(betweenness_c = centrality_betweenness(directed = F,normalized = T)) %&amp;gt;%
mutate(closeness_c = centrality_closeness(normalized = T)) %&amp;gt;%
mutate(eigen = centrality_eigen(directed = F))
network_ego_df &amp;lt;- as.data.frame(network_ego1)
&lt;/code>&lt;/pre>
&lt;p>Identify key nodes with respect to network metrics.&lt;/p>
&lt;pre>&lt;code class="language-r">network_ego_df
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## name community degree_c betweenness_c closeness_c eigen
## 1 robbie_emmet 2 8 0.0049299720 0.5384615 0.5238676
## 2 LauraBlissEco 2 3 0.0000000000 0.5147059 0.2717415
## 3 LeafyEricScott 5 3 0.0008403361 0.5223881 0.2287709
## 4 g33k5p34k 2 6 0.0024649860 0.5303030 0.4342056
## 5 EvpokPadding 3 4 0.0011204482 0.5223881 0.2336052
## 6 mellenmartin 2 4 0.0000000000 0.5223881 0.3371889
## 7 RoelandtN42 4 3 0.0000000000 0.5147059 0.2050991
## 8 alexnicolharper 2 5 0.0019607843 0.5303030 0.3942456
## 9 gau 3 3 0.0000000000 0.5147059 0.2133909
## 10 JosiahParry 4 3 0.0000000000 0.5147059 0.2050991
## 11 genius_c137 1 1 0.0000000000 0.5072464 0.1454399
## 12 Zjbb 1 1 0.0000000000 0.5072464 0.1454399
## 13 SJRAfloat 1 1 0.0000000000 0.5072464 0.1454399
## 14 BMPARMA17622540 1 1 0.0000000000 0.5072464 0.1454399
## 15 maryam_adeli 1 1 0.0000000000 0.5072464 0.1454399
## 16 waywardaf 1 1 0.0000000000 0.5072464 0.1454399
## 17 SusyVF6 1 1 0.0000000000 0.5072464 0.1454399
## 18 Sinalo_NM 1 1 0.0000000000 0.5072464 0.1454399
## 19 Leila_Lula 1 1 0.0000000000 0.5072464 0.1454399
## 20 CARThorpe 1 1 0.0000000000 0.5072464 0.1454399
## 21 j_wilson_white 1 1 0.0000000000 0.5072464 0.1454399
## 22 LegalizeBrain 3 2 0.0000000000 0.5147059 0.1794154
## 23 JaishriJuice 1 1 0.0000000000 0.5072464 0.1454399
## 24 jaguaretepy 1 1 0.0000000000 0.5072464 0.1454399
## 25 Souzam7139 2 3 0.0000000000 0.5223881 0.2706719
## 26 LaLince_ 1 1 0.0000000000 0.5072464 0.1454399
## 27 Nina_Ella_ 1 1 0.0000000000 0.5072464 0.1454399
## 28 samcox 1 1 0.0000000000 0.5072464 0.1454399
## 29 TAdamsBio42 1 1 0.0000000000 0.5072464 0.1454399
## 30 brubakerl 1 1 0.0000000000 0.5072464 0.1454399
## 31 robanhk 1 1 0.0000000000 0.5072464 0.1454399
## 32 InesCCarv 1 1 0.0000000000 0.5072464 0.1454399
## 33 groundhog0202 1 1 0.0000000000 0.5072464 0.1454399
## 34 zenmart 1 1 0.0000000000 0.5072464 0.1454399
## 35 lifedispersing 5 2 0.0000000000 0.5147059 0.1787123
## 36 oaggimenez 1 35 0.9685154062 1.0000000 1.0000000
&lt;/code>&lt;/pre>
&lt;p>Get users with highest values of each metric.&lt;/p>
&lt;pre>&lt;code class="language-r">kp_ego &amp;lt;- data.frame(
network_ego_df %&amp;gt;% arrange(-degree_c) %&amp;gt;% select(name),
network_ego_df %&amp;gt;% arrange(-betweenness_c) %&amp;gt;% select(name),
network_ego_df %&amp;gt;% arrange(-closeness_c) %&amp;gt;% select(name),
network_ego_df %&amp;gt;% arrange(-eigen) %&amp;gt;% select(name)) %&amp;gt;%
setNames(c(&amp;quot;degree&amp;quot;,&amp;quot;betweenness&amp;quot;,&amp;quot;closeness&amp;quot;,&amp;quot;eigen&amp;quot;))
kp_ego[-1,]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## degree betweenness closeness eigen
## 2 robbie_emmet robbie_emmet robbie_emmet robbie_emmet
## 3 g33k5p34k g33k5p34k g33k5p34k g33k5p34k
## 4 alexnicolharper alexnicolharper alexnicolharper alexnicolharper
## 5 EvpokPadding EvpokPadding LeafyEricScott mellenmartin
## 6 mellenmartin LeafyEricScott EvpokPadding LauraBlissEco
## 7 LauraBlissEco LauraBlissEco mellenmartin Souzam7139
## 8 LeafyEricScott mellenmartin Souzam7139 EvpokPadding
## 9 RoelandtN42 RoelandtN42 LauraBlissEco LeafyEricScott
## 10 gau gau RoelandtN42 gau
## 11 JosiahParry JosiahParry gau RoelandtN42
## 12 Souzam7139 genius_c137 JosiahParry JosiahParry
## 13 LegalizeBrain Zjbb LegalizeBrain LegalizeBrain
## 14 lifedispersing SJRAfloat lifedispersing lifedispersing
## 15 genius_c137 BMPARMA17622540 genius_c137 robanhk
## 16 Zjbb maryam_adeli Zjbb Sinalo_NM
## 17 SJRAfloat waywardaf SJRAfloat samcox
## 18 BMPARMA17622540 SusyVF6 BMPARMA17622540 genius_c137
## 19 maryam_adeli Sinalo_NM maryam_adeli Zjbb
## 20 waywardaf Leila_Lula waywardaf SJRAfloat
## 21 SusyVF6 CARThorpe SusyVF6 BMPARMA17622540
## 22 Sinalo_NM j_wilson_white Sinalo_NM waywardaf
## 23 Leila_Lula LegalizeBrain Leila_Lula SusyVF6
## 24 CARThorpe JaishriJuice CARThorpe Leila_Lula
## 25 j_wilson_white jaguaretepy j_wilson_white j_wilson_white
## 26 JaishriJuice Souzam7139 JaishriJuice JaishriJuice
## 27 jaguaretepy LaLince_ jaguaretepy jaguaretepy
## 28 LaLince_ Nina_Ella_ LaLince_ LaLince_
## 29 Nina_Ella_ samcox Nina_Ella_ Nina_Ella_
## 30 samcox TAdamsBio42 samcox brubakerl
## 31 TAdamsBio42 brubakerl TAdamsBio42 groundhog0202
## 32 brubakerl robanhk brubakerl zenmart
## 33 robanhk InesCCarv robanhk maryam_adeli
## 34 InesCCarv groundhog0202 InesCCarv CARThorpe
## 35 groundhog0202 zenmart groundhog0202 InesCCarv
## 36 zenmart lifedispersing zenmart TAdamsBio42
&lt;/code>&lt;/pre>
&lt;p>Robbie Emmet has the highest degree, betweenness, closeness centrality and eigenvector centrality. He has the most relations with the other nodes in the network. He also can spread information the further away and faster than anyone, and is also surrounded by important persons in the network. This is within the network we&amp;rsquo;ve just built.&lt;/p>
&lt;p>By the way, Robbie has just passed his dissertation defense, congrats! Follow him on Twitter at @robbie_emmet, he is awesome!&lt;/p>
&lt;h2 id="visualize-network">Visualize Network&lt;/h2>
&lt;p>We visualize the network using clusters or communities (first three only).&lt;/p>
&lt;pre>&lt;code class="language-r">options(ggrepel.max.overlaps = Inf)
network_viz &amp;lt;- network_ego1 %&amp;gt;%
filter(community %in% 1:3) %&amp;gt;%
mutate(node_size = ifelse(degree_c &amp;gt;= 50,degree_c,0)) %&amp;gt;%
mutate(node_label = ifelse(betweenness_c &amp;gt;= 0.01,name,NA))
plot_ego &amp;lt;- network_viz %&amp;gt;%
ggraph(layout = &amp;quot;stress&amp;quot;) +
geom_edge_fan(alpha = 0.05) +
geom_node_point(aes(color = as.factor(community),size = node_size)) +
geom_node_label(aes(label = node_label),nudge_y = 0.1,
show.legend = F, fontface = &amp;quot;bold&amp;quot;, fill = &amp;quot;#ffffff66&amp;quot;) +
theme_graph() +
theme(legend.position = &amp;quot;none&amp;quot;) +
labs(title = &amp;quot;Top 3 communities&amp;quot;)
plot_ego
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-31-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;p>Let&amp;rsquo;s use an alternative dataviz to identify the nodes.&lt;/p>
&lt;pre>&lt;code class="language-r">network_viz %&amp;gt;%
mutate(community = group_spinglass()) %&amp;gt;%
ggraph(layout = &amp;quot;nicely&amp;quot;) +
geom_edge_fan(alpha = 0.25) +
geom_node_point(aes(color = factor(community)),size = 5, show.legend = F) +
geom_node_text(aes(label = name),repel = T) +
theme_graph() + theme(legend.position = &amp;quot;none&amp;quot;) +
labs(title = &amp;quot;Network with named nodes&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="unnamed-chunk-32-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;h2 id="concluding-words">Concluding words&lt;/h2>
&lt;p>Not sure what the communities are about. Due to limitation with the Twitter API I did not attempt to retrieve the info on all the 5,000 followers, but I used only 35 of them, which explains why there is not much to say about this network I guess. I might take some time to try and retrieve all the info (will take days), or not. I might also use some information I already have, like how followers describe themselves and do some topic modelling to identify common themes of interest.&lt;/p>
&lt;p>Here I have simply followed the steps that Joe Cristian illustrated so brillantly in his post at &lt;a href="https://algotech.netlify.app/blog/social-network-analysis-in-r/">https://algotech.netlify.app/blog/social-network-analysis-in-r/&lt;/a>. Make sure you read his post for more details on the code and theory. In particular, he illustrates how information spreads through the network.&lt;/p>
&lt;p>For more about social network analyses and Twitter, see this thread &lt;a href="https://twitter.com/Mehdi_Moussaid/status/1389174715990876160?s=20">https://twitter.com/Mehdi_Moussaid/status/1389174715990876160?s=20&lt;/a> and this video &lt;a href="https://www.youtube.com/watch?v=UX7YQ6m2r_o">https://www.youtube.com/watch?v=UX7YQ6m2r_o&lt;/a> in French, this is what inspired me to do the analysis. See also this &lt;a href="https://github.com/eleurent/twitter-graph">https://github.com/eleurent/twitter-graph&lt;/a> for a much better analysis than what I could do (with Python for data retrieving and manipulation, and
&lt;a href="https://gephi.org/" target="_blank" rel="noopener">Gephi&lt;/a> for network visualisation).&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>This post was also published on &lt;a href="https://www.r-bloggers.com">https://www.r-bloggers.com&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Workshop on individual-based models with R</title><link>https://oliviergimenez.github.io/blog/ibmworkshop/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/ibmworkshop/</guid><description>&lt;p>Slides and codes available from &lt;a href="https://sarahbauduin.github.io/formation_IBM_NetLogoR/">https://sarahbauduin.github.io/formation_IBM_NetLogoR/&lt;/a>.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Our workshop on individual-based models &lt;a href="https://twitter.com/hashtag/IBMs?src=hash&amp;amp;ref_src=twsrc%5Etfw">#IBMs&lt;/a> with NetLogoR &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> starts today, led by &lt;a href="https://twitter.com/hashtag/twitterless?src=hash&amp;amp;ref_src=twsrc%5Etfw">#twitterless&lt;/a> IBM expert Sarah Bauduin &lt;a href="https://twitter.com/OFBiodiversite?ref_src=twsrc%5Etfw">@OFBiodiversite&lt;/a> &lt;a href="https://twitter.com/cefemontpellier?ref_src=twsrc%5Etfw">@cefemontpellier&lt;/a> w/ &lt;a href="https://twitter.com/oksanagrente?ref_src=twsrc%5Etfw">@oksanagrente&lt;/a> &amp;amp; &lt;a href="https://twitter.com/NSantostasi?ref_src=twsrc%5Etfw">@NSantostasi&lt;/a>&lt;br>â–¶ï¸ Slides &amp;amp; codes &lt;a href="https://t.co/LjCh4AH0UA">https://t.co/LjCh4AH0UA&lt;/a>&lt;br>â–¶ï¸ NetLogoR &lt;a href="https://t.co/KXQxIGwKMh">https://t.co/KXQxIGwKMh&lt;/a> &lt;a href="https://t.co/MjvVZxMi7H">pic.twitter.com/MjvVZxMi7H&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1406879847545290753?ref_src=twsrc%5Etfw">June 21, 2021&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Workshop on Bayesian capture-recapture inference</title><link>https://oliviergimenez.github.io/blog/hmmcrworkshop/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/hmmcrworkshop/</guid><description>&lt;p>Everything (including data, slides, codes and video recordings) about the workshop we run on Bayesian capture-recapture inference with hidden Markov models in R and Nimble.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">ðŸ¥³ðŸ» Had fun running a workshop on Bayesian capture-recapture inference with hidden Markov models in R &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> &amp;amp; Nimble w/ &lt;a href="https://twitter.com/SarahCubaynes?ref_src=twsrc%5Etfw">@SarahCubaynes&lt;/a> &lt;a href="https://twitter.com/chloe_nater?ref_src=twsrc%5Etfw">@chloe_nater&lt;/a> &lt;a href="https://twitter.com/MaudQueroue?ref_src=twsrc%5Etfw">@MaudQueroue&lt;/a> &amp;amp; Perry de Valpine &lt;a href="https://twitter.com/R_nimble?ref_src=twsrc%5Etfw">@R_nimble&lt;/a> &lt;br>âœ… Slides, worksheets &amp;amp; video recordings freely available from &lt;a href="https://t.co/CgVd84MPYN">https://t.co/CgVd84MPYN&lt;/a> &lt;a href="https://t.co/WlHzXytVUG">pic.twitter.com/WlHzXytVUG&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1394770140978466818?ref_src=twsrc%5Etfw">May 18, 2021&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Workshop on Bayesian statistics with R</title><link>https://oliviergimenez.github.io/blog/bayesstatsworkshop/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/bayesstatsworkshop/</guid><description>&lt;p>Everything (including data, slides and codes) about the workshop on Bayesian statistics I run.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Just finished up a week teaching Bayesian statistics with R &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> for non-stats PhD students &amp;amp; ðŸ’œ it &lt;a href="https://twitter.com/hashtag/bayes?src=hash&amp;amp;ref_src=twsrc%5Etfw">#bayes&lt;/a> &lt;a href="https://twitter.com/hashtag/mcmc?src=hash&amp;amp;ref_src=twsrc%5Etfw">#mcmc&lt;/a> &lt;a href="https://twitter.com/hashtag/jags?src=hash&amp;amp;ref_src=twsrc%5Etfw">#jags&lt;/a>&lt;br>ðŸŒ Dedicated website &lt;a href="https://t.co/X7JY3f9eGX">https://t.co/X7JY3f9eGX&lt;/a>&lt;br>ðŸ³ Material &lt;a href="https://t.co/agY9CvQLiV">https://t.co/agY9CvQLiV&lt;/a>&lt;br>ðŸ“½ï¸ Video recording (in English w/ ðŸ‘½ accent) &lt;a href="https://t.co/DHabXmVYsT">https://t.co/DHabXmVYsT&lt;/a> &lt;a href="https://t.co/g94nSO3faQ">pic.twitter.com/g94nSO3faQ&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1375498491145940992?ref_src=twsrc%5Etfw">March 26, 2021&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Workshop on reproducible science in R &amp; RStudio</title><link>https://oliviergimenez.github.io/blog/rrworkshop/</link><pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/rrworkshop/</guid><description>&lt;p>Everything (including data, slides and codes) about the workshop on reproducible science I run.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Had fun running a 1-day workshop on reproducible science in R &amp;amp; RStudio for our lab &lt;a href="https://twitter.com/cefemontpellier?ref_src=twsrc%5Etfw">@cefemontpellier&lt;/a>. We covered data manip &amp;amp; visualisation w/ &lt;a href="https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw">#tidyverse&lt;/a>, version control w/ Git/GitHub and writing dynamic docs w/ R Markdown&lt;br>ðŸ“ Slides &amp;amp; practicals &lt;a href="https://t.co/LJRknr8Kzb">https://t.co/LJRknr8Kzb&lt;/a> ðŸ“½ï¸=ðŸ‡«ðŸ‡· &lt;a href="https://t.co/uCsm9kH0Hf">pic.twitter.com/uCsm9kH0Hf&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1373600348879872000?ref_src=twsrc%5Etfw">March 21, 2021&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Bayesian implementation of the robust design</title><link>https://oliviergimenez.github.io/blog/rdcmr/</link><pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/rdcmr/</guid><description>&lt;p>Bayesian implementation of Pollock&amp;rsquo;s robust design capture-recapture models w/ Jags.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Bayesian implementation of Pollock&amp;#39;s &lt;a href="https://twitter.com/kpollock48?ref_src=twsrc%5Etfw">@kpollock48&lt;/a> robust design capture-recapture models w/ &lt;a href="https://twitter.com/hashtag/jags?src=hash&amp;amp;ref_src=twsrc%5Etfw">#jags&lt;/a> &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> from a &lt;a href="https://twitter.com/MethodsEcolEvol?ref_src=twsrc%5Etfw">@MethodsEcolEvol&lt;/a> paper by T. Riecke and colleagues âž¡ï¸ &lt;a href="https://t.co/nE7N8kHPiR">https://t.co/nE7N8kHPiR&lt;/a> &lt;a href="https://t.co/ZToHY3luZR">pic.twitter.com/ZToHY3luZR&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1247098841771409408?ref_src=twsrc%5Etfw">April 6, 2020&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>R Markdown</title><link>https://oliviergimenez.github.io/blog/rmarkdown/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/rmarkdown/</guid><description>&lt;p>In our weekly group meeting this morning, I introduced R Markdown a great #rstats tool to write reproducible documents (reports, articles, slides, websites) smoothly mixing text, code, figures and equations in html, word or pdf format.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">In our weekly &lt;a href="https://twitter.com/twitthair1?ref_src=twsrc%5Etfw">@twitthair1&lt;/a> meeting this morning, I introduced R Markdown &lt;a href="https://t.co/Hza76oAw4K">https://t.co/Hza76oAw4K&lt;/a> a great &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> tool to write reproducible documents (reports, articles, slides, websites) smoothly mixing text, code, figures and equations in html, word or pdf format. Ressources â¬‡ï¸ &lt;a href="https://t.co/r8vKbEaz04">pic.twitter.com/r8vKbEaz04&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1244971251334033409?ref_src=twsrc%5Etfw">March 31, 2020&lt;>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>SIR models in R</title><link>https://oliviergimenez.github.io/blog/sirstats/</link><pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/sirstats/</guid><description>&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">In a recent seminar, S. Gandon (w/ S. Lion and T. Day) used SIR models to illustrate the effect of diff strategies of &lt;a href="https://twitter.com/hashtag/SocialDistancing?src=hash&amp;amp;ref_src=twsrc%5Etfw">#SocialDistancing&lt;/a> on the &lt;a href="https://twitter.com/hashtag/COVID19?src=hash&amp;amp;ref_src=twsrc%5Etfw">#COVID19&lt;/a> epidemic. What better way to learn than to reproduce their results in R? My two cents &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> code âž¡ï¸ &lt;a href="https://t.co/oJVQHJ6HqO">https://t.co/oJVQHJ6HqO&lt;/a> ðŸ¤“ &lt;a href="https://t.co/dqSIcjbhqq">pic.twitter.com/dqSIcjbhqq&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1244394361141506059?ref_src=twsrc%5Etfw">March 29, 2/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>IPM workshop</title><link>https://oliviergimenez.github.io/blog/ipmworkshop/</link><pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/ipmworkshop/</guid><description>&lt;p>It is my pleasure to announce that Michael Schaub and Marc KÃ©ry will run their Integrated Population Modelling workshop in Montpellier this November.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">It&amp;#39;s my pleasure to announce that &lt;a href="https://twitter.com/hashtag/MichaelSchaub?src=hash&amp;amp;ref_src=twsrc%5Etfw">#MichaelSchaub&lt;/a> &amp;amp; &lt;a href="https://twitter.com/hashtag/MarcK%C3%A9ry?src=hash&amp;amp;ref_src=twsrc%5Etfw">#MarcKÃ©ry&lt;/a> will run their Integrated Population Modelling &lt;a href="https://twitter.com/hashtag/IPM?src=hash&amp;amp;ref_src=twsrc%5Etfw">#IPM&lt;/a> workshop in Montpellier ðŸ‡«ðŸ‡· this November, with help from &lt;a href="https://twitter.com/MaudQueroue?ref_src=twsrc%5Etfw">@MaudQueroue&lt;/a> and I - details âž¡ï¸ &lt;a href="https://t.co/pgCSxwwJW3">https://t.co/pgCSxwwJW3&lt;/a>. Come over, it&amp;#39;s going to be aaaaawesome ðŸ˜ &lt;a href="https://t.co/JvoXk07Jb1">pic.twitter.com/JvoXk07Jb1&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1235118285034668032?ref_src=twsrc%5Etfw">March 4020&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Bayesian SEM capture-recapture models</title><link>https://oliviergimenez.github.io/blog/semjags/</link><pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/semjags/</guid><description>&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">We had to dig out some &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> &lt;a href="https://twitter.com/hashtag/jags?src=hash&amp;amp;ref_src=twsrc%5Etfw">#jags&lt;/a> code to fit a structural equation capture-recapture model we developed w/ &lt;a href="https://twitter.com/SCubi25?ref_src=twsrc%5Etfw">@SCubi25&lt;/a> in a 8-year old &lt;a href="https://twitter.com/ESAEcology?ref_src=twsrc%5Etfw">@ESAEcology&lt;/a> paper (!), and it runs like a charm ðŸ¥³ðŸ¾&lt;a href="https://twitter.com/hashtag/reproducibility?src=hash&amp;amp;ref_src=twsrc%5Etfw">#reproducibility&lt;/a> âž¡ï¸ &lt;a href="https://t.co/Te6BKnxgMF">https://t.co/Te6BKnxgMF&lt;/a> &lt;a href="https://t.co/78tWbOhqZD">pic.twitter.com/78tWbOhqZD&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1225417314474090499?ref_src=twsrc%5Etfw">February 6, 2020&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Dolphin PVA</title><link>https://oliviergimenez.github.io/blog/ninapva/</link><pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/ninapva/</guid><description>&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">ðŸ‡¬ðŸ‡·ðŸ¬ðŸ“ˆ&lt;a href="https://twitter.com/NSantostasi?ref_src=twsrc%5Etfw">@NSantostasi&lt;/a> shows in her new paper &lt;a href="https://t.co/EZIvJFZbV1">https://t.co/EZIvJFZbV1&lt;/a> that Common dolphins in the Gulf of Corinth are Critically Endangered (w/ a stochastic PVA) &lt;a href="https://twitter.com/hashtag/WomenInSTEM?src=hash&amp;amp;ref_src=twsrc%5Etfw">#WomenInSTEM&lt;/a> &lt;a href="https://twitter.com/IUCNRedList?ref_src=twsrc%5Etfw">@IUCNRedList&lt;/a> &lt;a href="https://twitter.com/hashtag/FreeAccess?src=hash&amp;amp;ref_src=twsrc%5Etfw">#FreeAccess&lt;/a> ðŸ‡«ðŸ‡·ðŸ‡®ðŸ‡¹ &lt;a href="https://t.co/eC0plbVgeW">pic.twitter.com/eC0plbVgeW&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1223960554496307200?ref_src=twsrc%5Etfw">February 2020&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Wolf population dynamics and individual-based models</title><link>https://oliviergimenez.github.io/blog/ibmwolf/</link><pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/ibmwolf/</guid><description>&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">New &lt;a href="https://twitter.com/biorxivpreprint?ref_src=twsrc%5Etfw">@biorxivpreprint&lt;/a> &lt;a href="https://twitter.com/hashtag/preprint?src=hash&amp;amp;ref_src=twsrc%5Etfw">#preprint&lt;/a> led by Sarah Bauduin ðŸºðŸ’» From ind behavior and pack dynamics to pop responses: An &lt;a href="https://twitter.com/hashtag/IBM?src=hash&amp;amp;ref_src=twsrc%5Etfw">#IBM&lt;/a> to model &lt;a href="https://twitter.com/hashtag/wolf?src=hash&amp;amp;ref_src=twsrc%5Etfw">#wolf&lt;/a> social life cycle &lt;a href="https://t.co/ea31RvDMoE">https://t.co/ea31RvDMoE&lt;/a> w/ contributions by &lt;a href="https://twitter.com/NSantostasi?ref_src=twsrc%5Etfw">@NSantostasi&lt;/a> &amp;amp; &lt;a href="https://twitter.com/oksanagrente?ref_src=twsrc%5Etfw">@oksanagrente&lt;/a> &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> code âž¡ï¸ &lt;a href="https://t.co/r0hSZYyTpe">https://t.co/r0hSZYyTpe&lt;/a> &lt;a href="https://twitter.com/hashtag/WomenInSTEM?src=hash&amp;amp;ref_src=twsrc%5Etfw">#WomenInSTEM&lt;/a> &lt;a href="https://t.co/i8e0C9Ow8w">pic.twitter.com/i8e0C9Ow8w&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1221475422174687232?ref_src=twsrc%5Etfw">January 26, 2020&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Tidytuesday</title><link>https://oliviergimenez.github.io/blog/tidytuesday/</link><pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/tidytuesday/</guid><description>&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Happy 2020 &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a>! To celebrate, my modest first &lt;a href="https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw">#TidyTuesday&lt;/a> submission for wk 2019-52. Spatio-temporal trends in ðŸº presence in ðŸ‡«ðŸ‡· w/ data from &lt;a href="https://twitter.com/oncfs?ref_src=twsrc%5Etfw">@oncfs&lt;/a> &lt;a href="https://twitter.com/OFBiodiversite?ref_src=twsrc%5Etfw">@OFBiodiversite&lt;/a> (bugged dynamic map below)&lt;a href="https://twitter.com/hashtag/ggplot2?src=hash&amp;amp;ref_src=twsrc%5Etfw">#ggplot2&lt;/a> &lt;a href="https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw">#tidyverse&lt;/a> &lt;a href="https://twitter.com/hashtag/dataviz?src=hash&amp;amp;ref_src=twsrc%5Etfw">#dataviz&lt;/a> &lt;a href="https://twitter.com/hashtag/sf?src=hash&amp;amp;ref_src=twsrc%5Etfw">#sf&lt;/a> &lt;a href="https://twitter.com/thomas_mock?ref_src=twsrc%5Etfw">@thomas_mock&lt;/a> &lt;br>&lt;br>Code: &lt;a href="https://t.co/eKkehXlTJH">https://t.co/eKkehXlTJH&lt;/a> &lt;a href="https://t.co/hHwkOmSamR">pic.twitter.com/hHwkOmSamR&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ–– (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1212701671802970113?ref_src=twsrc%5Etfw">January 2, 2020&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Calculate the standard error of any function using the delta method</title><link>https://oliviergimenez.github.io/blog/delta-method/</link><pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/delta-method/</guid><description>&lt;p>In statistical ecology, we often need to calculate the sampling variance of a function of an estimate of which we do know the sampling variance. I keep forgetting how to implement the so-called delta method in &lt;code>R&lt;/code> that allows to get an approximation of this quantity. So in this post I go through two examples in population ecology that should help me remembering. I use the &lt;code>deltamethod&lt;/code> function from the
&lt;a href="https://cran.r-project.org/web/packages/msm/index.html" target="_blank" rel="noopener">&lt;code>msm&lt;/code> package&lt;/a>.&lt;/p>
&lt;p>Load the package &lt;code>msm&lt;/code> and get some help on the delta-method function:&lt;/p>
&lt;pre>&lt;code class="language-r">library(msm)
?deltamethod
&lt;/code>&lt;/pre>
&lt;p>Further examples can be obtained by typing in:&lt;/p>
&lt;pre>&lt;code class="language-r">example(deltamethod)
&lt;/code>&lt;/pre>
&lt;p>For a nice introduction to the delta method, check
&lt;a href="http://www.phidot.org/software/mark/docs/book/pdf/app_2.pdf" target="_blank" rel="noopener">that&lt;/a> out. Two papers are worth reading on the topic:
&lt;a href="https://www.tandfonline.com/doi/abs/10.1080/00031305.2012.687494" target="_blank" rel="noopener">&amp;lsquo;Who Invented the Delta Method?'&lt;/a> by Jay M. Ver Hoef and
&lt;a href="https://bioone.org/journals/The-Condor/volume-109/issue-4/0010-5422%282007%29109[949:AVODPU]2.0.CO;2/APPROXIMATING-VARIANCE-OF-DEMOGRAPHIC-PARAMETERS-USING-THE-DELTA-METHOD/10.1650/0010-5422%282007%29109[949:AVODPU]2.0.CO;2.full" target="_blank" rel="noopener">&amp;lsquo;Approximating variance of demographic parameters using the delta method: A reference for avian biologists&amp;rsquo;&lt;/a> by Larkin A. Powell.&lt;/p>
&lt;h3 id="simple-example">Simple example&lt;/h3>
&lt;p>A simple example is when, for example, you get $\phi$ (ignore the traditional hat) an estimate of a survival probability on the logit scale in some capture-recapture analyses, and you would like to get the standard error (SE) of survival on its natural scale.&lt;/p>
&lt;p>For example, say $\text{logit}(\phi) = \text{lphi} = -0.4473122$ with $\text{SE} = 0.3362757$.&lt;/p>
&lt;p>To obtain $\phi$, you back-transform $\text{lphi}$ using the reciprocal function of the logit function: $$\phi = \displaystyle{\frac{\exp(\text{lphi})}{1+\exp(\text{lphi})}} = \displaystyle{\frac{1}{1+\exp(\text{-lphi})}} = \displaystyle{\frac{1}{1+\exp(\text{-(-0.4473122)})}} = 0.39.$$&lt;/p>
&lt;p>What about the SE of $\phi$? Well, a direct application of the &lt;code>deltamethod&lt;/code> function from the &lt;code>msm&lt;/code> package gives the answer:&lt;/p>
&lt;pre>&lt;code class="language-r">deltamethod(~ 1/(1+exp(-x1)), -0.4473122, 0.3362757^2)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.07999999
&lt;/code>&lt;/pre>
&lt;p>Two things to take care of:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>First, the variables in the formula must be labelled $x_1, x_2, \text{text}$. You cannot use $x, y, z, &amp;hellip;$ for example. Just numbered $x$&amp;rsquo;s.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Second, the input parameters are the estimate and its squared SE (not the SE), and by default you will get as an output the SE (not the squared SE) of the function defined by the formula.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="complex-example">Complex example&lt;/h2>
&lt;p>This example deals with an occupancy model. It is a bit more complex than the previous example because we consider a function of several parameters for which we would like to calculate its SE. I assume that occupancy at first occasion was estimated along with its SE, and that one would like to obtain the SE of subsequent occupancy probabilities.&lt;/p>
&lt;p>I calculate time-dependent occupancy probabilities with the following formula $$\psi_{t+1} = \psi_t (1 - \varepsilon) + (1 - \psi_t) \gamma$$ where $\varepsilon$ is extinction, $\gamma$ is colonisation and $\psi_t$ is occupancy year $t$.&lt;/p>
&lt;p>We assume that we obtained the following parameter estimates:&lt;/p>
&lt;pre>&lt;code class="language-r">epsilon = 0.39
gamma = 0.07
psi_init = 0.1 # first-occasion occupancy
&lt;/code>&lt;/pre>
&lt;p>with corresponding SEs:&lt;/p>
&lt;pre>&lt;code class="language-r">se_epsilon = 0.08
se_psi_init = 0.01
se_gamma = 0.05
&lt;/code>&lt;/pre>
&lt;p>We will estimate occupancy and get SEs at 10 occasions, which we store in two matrices (column vectors):&lt;/p>
&lt;pre>&lt;code class="language-r">psi = matrix(0, nrow = 10, ncol = 1)
psi_se = matrix(0, nrow = 10, ncol = 1)
&lt;/code>&lt;/pre>
&lt;p>The first element is occupancy at first occasion:&lt;/p>
&lt;pre>&lt;code class="language-r">psi[1,] &amp;lt;- psi_init
psi_se[1,] &amp;lt;- se_psi_init
&lt;/code>&lt;/pre>
&lt;p>Then we iterate calculations using the formula above:&lt;/p>
&lt;pre>&lt;code class="language-r">for(i in 2:10){
psi_current &amp;lt;- psi[i-1,]
psi_se_current &amp;lt;- psi_se[i-1,]
estmean &amp;lt;- c(psi_current,epsilon,gamma)
estvar &amp;lt;- diag(c(psi_se_current,se_epsilon,se_gamma)^2)
psi[i,] = (psi_current*(1-epsilon)) + ((1-psi_current)*gamma) # recurrence formula
psi_se[i,] = deltamethod(~ x1*(1-x2) + (1-x1)*x3, estmean, estvar) # delta-method
}
&lt;/code>&lt;/pre>
&lt;p>Display results:&lt;/p>
&lt;pre>&lt;code class="language-r">data.frame(psi = psi,sterr_psi = psi_se)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## psi sterr_psi
## 1 0.1000000 0.01000000
## 2 0.1240000 0.04602347
## 3 0.1369600 0.05132740
## 4 0.1439584 0.05244394
## 5 0.1477375 0.05259904
## 6 0.1497783 0.05255782
## 7 0.1508803 0.05250010
## 8 0.1514753 0.05245886
## 9 0.1517967 0.05243372
## 10 0.1519702 0.05241934
&lt;/code>&lt;/pre>
&lt;p>Here, we assumed that sampling correlation was 0, in other words that the estimates of $\psi$, $\gamma$ and $\epsilon$ were independent, hence the use of a diagonal matrix for &lt;code>estvar&lt;/code>. It is possible to use a non-diagonal covariance matrix to account for non-null correlation.&lt;/p></description></item><item><title>Scientific research is all about networking</title><link>https://oliviergimenez.github.io/blog/network_ecology/</link><pubDate>Wed, 02 Jan 2019 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/network_ecology/</guid><description>&lt;p>I read
&lt;a href="http://coulmont.com/blog/2018/12/02/sociologue-reseau-theses-2018/" target="_blank" rel="noopener">this awesome post&lt;/a> (in French) by
&lt;a href="http://coulmont.com/" target="_blank" rel="noopener">Baptiste Coulmont&lt;/a>, professor in sociology, who explored the French academic network in sociology. Coulmont used the composition of PhD commitees to determine academic links between colleagues. The approach very appealing because it uses public data available from the website
&lt;a href="www.these.fr">these.fr&lt;/a>. Here, I used Coulmont&amp;rsquo;s &lt;code>R&lt;/code> code to produce the French academic network in ecology. This was a nice opportunity to illustrate how to work in the &lt;code>tidyverse&lt;/code> and to do some
&lt;a href="https://en.wikipedia.org/wiki/Web_scraping" target="_blank" rel="noopener">web scraping&lt;/a> using the &lt;code>rvest&lt;/code> package.&lt;/p>
&lt;h2 id="get-the-data">Get the data&lt;/h2>
&lt;p>Load the packages we need:&lt;/p>
&lt;pre>&lt;code>library(RCurl)
library(tidyverse)
library(lubridate)
library(scales)
library(hrbrthemes)
library(data.table)
# devtools::install_github(&amp;quot;privefl/bigreadr&amp;quot;)
library(bigreadr)
&lt;/code>&lt;/pre>
&lt;p>We now prepare the URL requests. The total number of PhDs is around
88000 on the period 2015-2018. Because the website uses slices of 1000 on each page, we proceed
in sequence:&lt;/p>
&lt;pre>&lt;code>i &amp;lt;- 1:88
i &amp;lt;- i*1000
URL &amp;lt;-paste0(&amp;quot;http://www.theses.fr/?q=&amp;amp;fq=dateSoutenance:[2015-01-01T23:59:59Z%2BTO%2B2018-12-31T23:59:59Z]&amp;amp;checkedfacets=&amp;amp;start=&amp;quot;,i,&amp;quot;&amp;amp;sort=none&amp;amp;status=&amp;amp;access=&amp;amp;prevision=&amp;amp;filtrepersonne=&amp;amp;zone1=titreRAs&amp;amp;val1=&amp;amp;op1=AND&amp;amp;zone2=auteurs&amp;amp;val2=&amp;amp;op2=AND&amp;amp;zone3=etabSoutenances&amp;amp;val3=&amp;amp;zone4=dateSoutenance&amp;amp;val4a=&amp;amp;val4b=&amp;amp;type=&amp;amp;lng=&amp;amp;checkedfacets=&amp;amp;format=csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Alternatively, the search can be done by hand directly from the
&lt;a href="www.theses.fr">theses.fr&lt;/a> website. [FranÃ§ois-Xavier Coudert]
(&lt;a href="https://www.coudert.name/">https://www.coudert.name/&lt;/a>) also provides
&lt;a href="https://twitter.com/fxcoudert/status/1069188451898138624" target="_blank" rel="noopener">the search results for the
2015-2018
period&lt;/a>.&lt;/p>
&lt;p>We proceed with the requests, and store everything in a csv file:&lt;/p>
&lt;pre>&lt;code>j &amp;lt;- 1
SERP &amp;lt;- 1
for(j in 1:length(URL)){ # loop over the slices
SERP[j] &amp;lt;- getURL(URL[j])
write.csv(SERP,&amp;quot;SERP_2.csv&amp;quot;,append=F)
}
rm(SERP,i,j,URL)
&lt;/code>&lt;/pre>
&lt;p>We keep only the PhDs in the field (Discipline) of ecology. This is basically the only change I have made to Coulmont&amp;rsquo;s neat code.&lt;/p>
&lt;pre>&lt;code>theses &amp;lt;- read.csv(&amp;quot;SERP_2.csv&amp;quot;,sep=&amp;quot;;&amp;quot;,quote=&amp;quot;&amp;quot;,skip=1,stringsAsFactors = F)
#theses %&amp;gt;%
# pull(X..Discipline..) %&amp;gt;%
# unique()
ecology &amp;lt;- theses %&amp;gt;% filter(grepl(&amp;quot;ecologie&amp;quot;,X..Discipline..,ignore.case=T)) %&amp;gt;% # keep PhDs with Displine == ecologie
filter(X..Date.de.soutenance..!=&amp;quot;&amp;quot;) %&amp;gt;% # remove PhDs with missing dates of defense
filter(X..Statut..==&amp;quot;soutenue&amp;quot;) # keep only PhDs that have been defended
&lt;/code>&lt;/pre>
&lt;p>We now have the id of all PhDs in ecology defended during the period 2015-2018. We
will use the id to get the composition of all PhD commitees. Getting this composition
requires scraping the web page of each PhD, and to get the
ID of each PhD. For doing so, we use the &lt;code>rvest&lt;/code> package (see the
&lt;a href="https://masalmon.eu/tags/rvest/" target="_blank" rel="noopener">excellent posts&lt;/a>
by MaÃ«lle Salmon for examples).&lt;/p>
&lt;pre>&lt;code>library(rvest)
identifiants &amp;lt;- ecology$X..Identifiant.de.la.these.. # get PhD ids
reseau_total &amp;lt;- data_frame(noms_jury=&amp;quot;&amp;quot;,
liens_jury=&amp;quot;&amp;quot;,
these=&amp;quot;&amp;quot;,
directeurs=&amp;quot;&amp;quot;,
liens_directeurs=&amp;quot;&amp;quot;)
for (i in 1:length(identifiants)) {
# get info on current PhD
data_theses_eco &amp;lt;- read_html( paste0(&amp;quot;http://www.theses.fr/&amp;quot;,identifiants[i]) )
# get name PhD supervisor for
directeurs &amp;lt;- bind_cols(
directeurs = data_theses_eco %&amp;gt;%
html_nodes(&amp;quot;div .donnees-ombre p&amp;quot;) %&amp;gt;%
.[[1]] %&amp;gt;%
html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;%
html_text()
,
liens_directeurs = data_theses_eco %&amp;gt;%
html_nodes(&amp;quot;div .donnees-ombre p&amp;quot;) %&amp;gt;%
.[[1]] %&amp;gt;%
html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;%
html_attr(name=&amp;quot;href&amp;quot;)
) %&amp;gt;% mutate( these = identifiants[i] )
# get names of people in commitees
jury &amp;lt;- bind_cols(
noms_jury = data_theses_eco %&amp;gt;%
html_nodes(&amp;quot;div .donnees p a&amp;quot;) %&amp;gt;%
html_text()
,
liens_jury = data_theses_eco %&amp;gt;%
html_nodes(&amp;quot;div .donnees p a&amp;quot;) %&amp;gt;%
html_attr(name=&amp;quot;href&amp;quot;)
) %&amp;gt;% mutate( these = identifiants[i] )
# put all together
reseau &amp;lt;- jury %&amp;gt;% left_join(directeurs,by=&amp;quot;these&amp;quot;)
reseau_total &amp;lt;- bind_rows(reseau_total,reseau)
}
&lt;/code>&lt;/pre>
&lt;h2 id="build-the-network">Build the network&lt;/h2>
&lt;p>Load the packages we need, and the data we got at the previous step:&lt;/p>
&lt;pre>&lt;code>library(igraph)
library(ggraph)
library(ggrepel)
load('reseau_total.RData')
&lt;/code>&lt;/pre>
&lt;p>Coulmont defined a weighted link between two colleagues &lt;em>i&lt;/em> and &lt;em>j&lt;/em> as
follows: 3 if &lt;em>i&lt;/em> and &lt;em>j&lt;/em> are both supervisors, 2 if &lt;em>i&lt;/em> is a supervisor
and &lt;em>j&lt;/em> a PhD commitee member and 1 if both &lt;em>i&lt;/em> and &lt;em>j&lt;/em> are PhD commitee
members. A colleague may accumulate several weights.&lt;/p>
&lt;pre>&lt;code>directions_theses &amp;lt;- reseau_total %&amp;gt;%
select(these,directeurs) %&amp;gt;%
unique() %&amp;gt;%
group_by(these) %&amp;gt;%
mutate(N=n()) %&amp;gt;%
filter(N==2) %&amp;gt;% # keep co-supervision w/ 2 supervisors
mutate(rang=rank(directeurs)) %&amp;gt;%
spread(key=rang,value=directeurs) %&amp;gt;%
ungroup() %&amp;gt;%
select(nom1=`1`,nom2=`2`) %&amp;gt;%
mutate(poids=3)
directions_jury &amp;lt;- reseau_total %&amp;gt;%
select(nom1=noms_jury,nom2=directeurs) %&amp;gt;%
filter( nom1 != &amp;quot;&amp;quot;) %&amp;gt;%
mutate(poids=2) %&amp;gt;%
group_by(nom1,nom2) %&amp;gt;%
summarize(poids=sum(poids))
jury_jury &amp;lt;- reseau_total %&amp;gt;%
select(noms_jury,these) %&amp;gt;%
unique() %&amp;gt;%
filter(noms_jury!=&amp;quot;&amp;quot;)
g_j &amp;lt;- graph_from_data_frame(jury_jury,directed=F)
V(g_j)$type &amp;lt;- V(g_j)$name %in% jury_jury$noms_jury
g_j_1 &amp;lt;- bipartite_projection(g_j,which=&amp;quot;true&amp;quot;)
jurys &amp;lt;- as_long_data_frame(g_j_1) %&amp;gt;%
select(nom1=`ver[el[, 1], ]`, nom2=`ver2[el[, 2], ]`, poids=weight)
reseau_petit &amp;lt;- bind_rows(directions_theses,directions_jury,jurys) %&amp;gt;%
group_by(nom1,nom2) %&amp;gt;%
summarize(poids=sum(poids)) # data.frame from which the network will be created
&lt;/code>&lt;/pre>
&lt;p>Each node in the network has a size proportional to its
&lt;a href="https://en.wikipedia.org/wiki/Betweenness_centrality" target="_blank" rel="noopener">betweenness&lt;/a>
score. We also determine communities using the
&lt;a href="http://arxiv.org/abs/physics/0512106" target="_blank" rel="noopener">walktrap
algorithm&lt;/a> that will be colored differently. The width of an edge is
proportional to the strength of the link between the two corresponding
nodes.&lt;/p>
&lt;pre>&lt;code>g &amp;lt;- graph_from_data_frame(reseau_petit, directed=F) # create network from data.frame
g &amp;lt;- simplify(g,edge.attr.comb = sum)
V(g)$degres &amp;lt;- degree(g)
V(g)$label &amp;lt;- gsub(&amp;quot;^\\S+\\s+(.+)$&amp;quot;,&amp;quot;\\1&amp;quot;,V(g)$name)
V(g)$communaute &amp;lt;- as.character(cluster_walktrap(g, steps=15)$membership) # determine communities
V(g)$closeness &amp;lt;- (5*closeness(g))^10
V(g)$btwns &amp;lt;- betweenness(g) # network metric betweeness
V(g)$eigen_centr &amp;lt;- eigen_centrality(g)$vector
g &amp;lt;- delete_edges(g, which(E(g)$poids&amp;lt;5) ) # delete edges with weight &amp;lt;= 4
V(g)$cluster_number &amp;lt;- clusters(g)$membership # to which community you belong
g &amp;lt;- induced_subgraph(g, V(g)$cluster_number== which( max(clusters(g)$csize) == clusters(g)$csize) )
E(g)$weight &amp;lt;- 1/E(g)$poids # width of edge proportional to weight
V(g)$label &amp;lt;- ifelse(V(g)$degres&amp;lt;20,&amp;quot;&amp;quot;,V(g)$label) # do not display all names
&lt;/code>&lt;/pre>
&lt;h2 id="plot-the-network">Plot the network&lt;/h2>
&lt;p>We now plot the network. For clarity, we only indicate the names of
colleagues who were part of several phD commitees.&lt;/p>
&lt;pre>&lt;code>ggraph(g,layout=&amp;quot;igraph&amp;quot;,algorithm=&amp;quot;fr&amp;quot;) +
geom_edge_link(aes(width=.1*poids), alpha=.1,
end_cap = circle(5, 'mm'),
start_cap = circle(5, 'mm')) +
geom_node_point(aes(size=eigen_centr), color=&amp;quot;white&amp;quot;,alpha=1) +
geom_node_point(aes(color=communaute,size=eigen_centr), alpha=.5) +
scale_size_area(max_size = 20) +
geom_node_text(aes(label=label),size=3,repel=T,box.padding = 0.15) +
labs(title=&amp;quot;RÃ©seaux des Ã©cologues&amp;quot;,
subtitle=&amp;quot;Soutenances de thÃ¨ses entre 2015 et 2018&amp;quot;,
caption=&amp;quot;Sources : theses.fr \n Code par B. Coulmont, modifiÃ© par O. Gimenez&amp;quot;) +
theme_graph(foreground = 'white', fg_text_colour = 'white',
base_family = &amp;quot;Helvetica&amp;quot;) +
theme(legend.position=&amp;quot;none&amp;quot;,
text=element_text(size=16,family=&amp;quot;Helvetica&amp;quot;),
plot.margin = unit(c(0.2, 0.2, 0.2, 0.2), units=&amp;quot;line&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://oliviergimenez.github.io/img/ecolnetwork.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code># save
ggsave(filename = &amp;quot;ecology_network.pdf&amp;quot;,width=30,height = 20)
&lt;/code>&lt;/pre>
&lt;p>I played around the defaults Coulmont used to build and plot the network. It helps in getting a better understanding of the network and the links between colleagues working in ecology. Overall, I indeed feel very much connected to my colleagues in Montpellier, Lyon and Grenoble. I should probably go out of my comfort zone and interact even more with my colleagues from La Rochelle, Marseille and Aix-en-Provence ðŸ˜ƒ&lt;/p>
&lt;p>As always, data and code are available from
&lt;a href="https://github.com/oliviergimenez/phd-in-ecology-network/" target="_blank" rel="noopener">GitHub&lt;/a>.&lt;/p></description></item><item><title>New paper!</title><link>https://oliviergimenez.github.io/blog/articlenina/</link><pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/articlenina/</guid><description>&lt;blockquote class="twitter-tweet" data-lang="fr">&lt;p lang="en" dir="ltr">ðŸ¬ðŸ‡¬ðŸ‡·ðŸ‡®ðŸ‡¹ðŸ‡«ðŸ‡· New paper by &lt;a href="https://twitter.com/NSantostasi?ref_src=twsrc%5Etfw">@NSantostasi&lt;/a> &lt;a href="https://twitter.com/INEE_CNRS?ref_src=twsrc%5Etfw">@INEE_CNRS&lt;/a> &lt;a href="https://twitter.com/CNRS_OccitaniE?ref_src=twsrc%5Etfw">@CNRS_OccitaniE&lt;/a> &lt;a href="https://twitter.com/IsiteMUSE?ref_src=twsrc%5Etfw">@IsiteMUSE&lt;/a> &lt;a href="https://twitter.com/umontpellier?ref_src=twsrc%5Etfw">@umontpellier&lt;/a> ðŸ¤©ðŸ‘ &lt;a href="https://t.co/6vQ6d9HevV">https://t.co/6vQ6d9HevV&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ’¤ (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1060966359348269058?ref_src=twsrc%5Etfw">9 novembre 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Workshop on spatio-temporal models with INLA</title><link>https://oliviergimenez.github.io/blog/inla_workshop/</link><pubDate>Tue, 06 Nov 2018 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/inla_workshop/</guid><description>&lt;blockquote class="twitter-tweet" data-lang="fr">&lt;p lang="en" dir="ltr">&lt;a href="https://twitter.com/hashtag/INLA?src=hash&amp;amp;ref_src=twsrc%5Etfw">#INLA&lt;/a> workshop on spatio-temporal models in the beautiful city of &lt;a href="https://twitter.com/hashtag/Avignon?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Avignon&lt;/a> ðŸ¤© &lt;a href="https://twitter.com/hashtag/RESSTE?src=hash&amp;amp;ref_src=twsrc%5Etfw">#RESSTE&lt;/a> &lt;a href="https://twitter.com/hashtag/GdREcoStat?src=hash&amp;amp;ref_src=twsrc%5Etfw">#GdREcoStat&lt;/a> &lt;a href="https://twitter.com/oksanagrente?ref_src=twsrc%5Etfw">@oksanagrente&lt;/a> &lt;a href="https://twitter.com/CREEM_cake?ref_src=twsrc%5Etfw">@CREEM_cake&lt;/a> &lt;a href="https://t.co/nuLhIkMiwO">pic.twitter.com/nuLhIkMiwO&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸ’¤ (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1060085610793394177?ref_src=twsrc%5Etfw">718&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Crash course on individual-based models using NetLogoR</title><link>https://oliviergimenez.github.io/blog/crashcourse_netlogor/</link><pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/crashcourse_netlogor/</guid><description>&lt;blockquote class="twitter-tweet" data-lang="fr">&lt;p lang="en" dir="ltr">The famous &lt;a href="https://twitter.com/hashtag/NetLogo?src=hash&amp;amp;ref_src=twsrc%5Etfw">#NetLogo&lt;/a> butterfly example coded in &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> w/ &lt;a href="https://twitter.com/hashtag/NetLogoR?src=hash&amp;amp;ref_src=twsrc%5Etfw">#NetLogoR&lt;/a> &lt;a href="https://t.co/VbUUa5vIep">pic.twitter.com/VbUUa5vIep&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸš¸ (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1059445943689510913?ref_src=twsrc%5Etfw">5 novembre 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;blockquote class="twitter-tweet" data-lang="fr">&lt;p lang="en" dir="ltr">Wolf model from Marucco &amp;amp; &lt;a href="https://twitter.com/eliotmcintire?ref_src=twsrc%5Etfw">@eliotmcintire&lt;/a> coded in &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw">#rstats&lt;/a> w/ &lt;a href="https://twitter.com/hashtag/NetLogoR?src=hash&amp;amp;ref_src=twsrc%5Etfw">#NetLogoR&lt;/a> &amp;amp; &lt;a href="https://twitter.com/hashtag/SpaDES?src=hash&amp;amp;ref_src=twsrc%5Etfw">#SpaDES&lt;/a> &lt;a href="https://t.co/ERx71CIgZv">https://t.co/ERx71CIgZv&lt;/a> &lt;a href="https://t.co/f0YTHuKGSS">pic.twitter.com/f0YTHuKGSS&lt;/a>&lt;/p>&amp;mdash; Olivier Gimenez ðŸš¸ (@oaggimenez) &lt;a href="https://twitter.com/oaggimenez/status/1059450575660687361?ref_src=twsrc%5Etfw">5 novembre 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Running OpenBUGS in parallel</title><link>https://oliviergimenez.github.io/blog/run_openbugs_parallel/</link><pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/run_openbugs_parallel/</guid><description>&lt;img style="float:left;margin-right:10px;margin-top:10px;width:200px;margin-bottom:5px" src="https://oliviergimenez.github.io/img/parcomp.jpg">
Recently, I have been using `OpenBUGS` for some analyses that `JAGS` cannot do. However, `JAGS` can be run in parallel through [the `jagsUI` package](https://github.com/kenkellner/jagsUI), which can save you some precious time. So the question is how to run several chains in parallel with `OpenBUGS`.
&lt;p>Well, first you&amp;rsquo;ll need to install &lt;code>OpenBUGS&lt;/code> (if you&amp;rsquo;re on a Mac, check out
&lt;a href="https://oliviergimenez.github.io/post/run_openbugs_on_mac/" target="_blank" rel="noopener">this short tutorial&lt;/a>). Then, you&amp;rsquo;ll need to run &lt;code>OpenBUGS&lt;/code> from &lt;code>R&lt;/code> through the pacage &lt;code>R2OpenBUGS&lt;/code>, which you can install via:&lt;/p>
&lt;pre>&lt;code class="language-r">if(!require(R2OpenBUGS)) install.packages(&amp;quot;R2OpenBUGS&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Loading required package: R2OpenBUGS
&lt;/code>&lt;/pre>
&lt;h2 id="standard-analysis">Standard analysis&lt;/h2>
&lt;p>Now let&amp;rsquo;s run the classical &lt;code>BUGS&lt;/code> &lt;code>school&lt;/code> example:&lt;/p>
&lt;p>Load the &lt;code>OpenBUGS&lt;/code> Package&lt;/p>
&lt;pre>&lt;code class="language-r">library(R2OpenBUGS)
&lt;/code>&lt;/pre>
&lt;p>Load the data&lt;/p>
&lt;pre>&lt;code class="language-r">data(schools)
&lt;/code>&lt;/pre>
&lt;p>Define the model, write it to a text file and have a look&lt;/p>
&lt;pre>&lt;code class="language-r">nummodel &amp;lt;- function(){
for (j in 1:J){
y[j] ~ dnorm (theta[j], tau.y[j])
theta[j] ~ dnorm (mu.theta, tau.theta)
tau.y[j] &amp;lt;- pow(sigma.y[j], -2)}
mu.theta ~ dnorm (0.0, 1.0E-6)
tau.theta &amp;lt;- pow(sigma.theta, -2)
sigma.theta ~ dunif (0, 1000)
}
write.model(nummodel, &amp;quot;nummodel.txt&amp;quot;)
model.file1 = paste(getwd(),&amp;quot;nummodel.txt&amp;quot;, sep=&amp;quot;/&amp;quot;)
file.show(&amp;quot;nummodel.txt&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Prepare the data for input into OpenBUGS&lt;/p>
&lt;pre>&lt;code class="language-r">J &amp;lt;- nrow(schools)
y &amp;lt;- schools$estimate
sigma.y &amp;lt;- schools$sd
data &amp;lt;- list (&amp;quot;J&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;sigma.y&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Initialization of variables&lt;/p>
&lt;pre>&lt;code class="language-r">inits &amp;lt;- function(){
list(theta = rnorm(J, 0, 100), mu.theta = rnorm(1, 0, 100), sigma.theta = runif(1, 0, 100))}
&lt;/code>&lt;/pre>
&lt;p>Set the &lt;code>Wine&lt;/code> working directory and the directory to &lt;code>OpenBUGS&lt;/code>, and change the OpenBUGS.exe location as necessary:&lt;/p>
&lt;pre>&lt;code class="language-r">WINE=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/wine&amp;quot;
WINEPATH=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/winepath&amp;quot;
OpenBUGS.pgm=&amp;quot;/Applications/OpenBUGS323/OpenBUGS.exe&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The are the parameters to save&lt;/p>
&lt;pre>&lt;code class="language-r">parameters = c(&amp;quot;theta&amp;quot;, &amp;quot;mu.theta&amp;quot;, &amp;quot;sigma.theta&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Run the model&lt;/p>
&lt;pre>&lt;code class="language-r">ptm &amp;lt;- proc.time()
schools.sim &amp;lt;- bugs(data, inits, model.file = model.file1,parameters=parameters,n.chains = 2, n.iter = 500000, n.burnin = 10000, OpenBUGS.pgm=OpenBUGS.pgm, WINE=WINE, WINEPATH=WINEPATH,useWINE=T)
elapsed_time &amp;lt;- proc.time() - ptm
elapsed_time
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## user system elapsed
## 50.835 2.053 55.010
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">print(schools.sim)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Inference for Bugs model at &amp;quot;/Users/oliviergimenez/Desktop/nummodel.txt&amp;quot;,
## Current: 2 chains, each with 5e+05 iterations (first 10000 discarded)
## Cumulative: n.sims = 980000 iterations saved
## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff
## theta[1] 11.6 8.4 -1.9 6.1 10.5 15.8 32.0 1 370000
## theta[2] 8.0 6.4 -4.8 4.0 8.0 12.0 20.9 1 67000
## theta[3] 6.4 7.8 -11.3 2.2 6.8 11.2 20.9 1 55000
## theta[4] 7.7 6.6 -5.7 3.7 7.8 11.8 20.9 1 70000
## theta[5] 5.5 6.5 -8.8 1.6 5.9 9.8 17.1 1 26000
## theta[6] 6.2 6.9 -8.9 2.3 6.6 10.7 18.9 1 23000
## theta[7] 10.7 6.9 -1.4 6.0 10.1 14.7 26.2 1 480000
## theta[8] 8.7 7.9 -6.8 4.0 8.4 13.0 25.7 1 76000
## mu.theta 8.1 5.3 -2.0 4.7 8.1 11.4 18.5 1 30000
## sigma.theta 6.6 5.7 0.2 2.5 5.2 9.1 20.9 1 12000
## deviance 60.5 2.2 57.0 59.1 60.1 61.4 66.0 1 980000
##
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
##
## DIC info (using the rule, pD = Dbar-Dhat)
## pD = 2.8 and DIC = 63.2
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code>&lt;/pre>
&lt;h2 id="parallel-computations">Parallel computations&lt;/h2>
&lt;p>To run several chains in parallel, we&amp;rsquo;ll follow the steps described in
&lt;a href="http://www.petrkeil.com/?p=63" target="_blank" rel="noopener">this nice post&lt;/a>.&lt;/p>
&lt;pre>&lt;code class="language-r"># loading packages
library(snow)
library(snowfall)
# setting the number of CPUs to be 2
sfInit(parallel=TRUE, cpus=2)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning in searchCommandline(parallel, cpus = cpus, type
## = type, socketHosts = socketHosts, : Unknown option on
## commandline: rmarkdown::render('/Users/oliviergimenez/Desktop/
## run_openbugs_in_parallel.Rmd',~+~~+~encoding~+~
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## R Version: R version 3.4.3 (2017-11-30)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## snowfall 1.84-6.1 initialized (using snow 0.4-2): parallel execution on 2 CPUs.
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r"># and assigning the R2OpenBUGS library to each CPU
sfLibrary(R2OpenBUGS)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Library R2OpenBUGS loaded.
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Library R2OpenBUGS loaded in cluster.
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r"># create list of data
J &amp;lt;- nrow(schools)
y &amp;lt;- schools$estimate
sigma.y &amp;lt;- schools$sd
x.data &amp;lt;- list (J=J, y=y, sigma.y=sigma.y)
# creating separate directory for each CPU process
folder1 &amp;lt;- paste(getwd(), &amp;quot;/chain1&amp;quot;, sep=&amp;quot;&amp;quot;)
folder2 &amp;lt;- paste(getwd(), &amp;quot;/chain2&amp;quot;, sep=&amp;quot;&amp;quot;)
dir.create(folder1); dir.create(folder2);
# sinking the model into a file in each directory
for (folder in c(folder1, folder2))
{
sink(paste(folder, &amp;quot;/nummodel.txt&amp;quot;, sep=&amp;quot;&amp;quot;))
cat(&amp;quot;
model{
for (j in 1:J){
y[j] ~ dnorm (theta[j], tau.y[j])
theta[j] ~ dnorm (mu.theta, tau.theta)
tau.y[j] &amp;lt;- pow(sigma.y[j], -2)}
mu.theta ~ dnorm (0.0, 1.0E-6)
tau.theta &amp;lt;- pow(sigma.theta, -2)
sigma.theta ~ dunif (0, 1000)
}
&amp;quot;)
sink()
}
# defining the function that will run MCMC on each CPU
# Arguments:
# chain - will be 1 or 2
# x.data - the data list
# params - parameters to be monitored
parallel.bugs &amp;lt;- function(chain, x.data, params)
{
# a. defining directory for each CPU
sub.folder &amp;lt;- paste(getwd(),&amp;quot;/chain&amp;quot;, chain, sep=&amp;quot;&amp;quot;)
# b. specifying the initial MCMC values
inits &amp;lt;- function()list(theta = rnorm(x.data$J, 0, 100), mu.theta = rnorm(1, 0, 100), sigma.theta = runif(1, 0, 100))
# c. calling OpenBugs
# (you may need to change the OpenBUGS.pgm directory)
# je suis sous Mac, je fais tourner OpenBUGS via Wine
bugs(data=x.data, inits=inits, parameters.to.save=params,
n.iter = 500000, n.burnin = 10000, n.chains=1,
model.file=&amp;quot;nummodel.txt&amp;quot;, debug=FALSE, codaPkg=TRUE,
useWINE=TRUE, OpenBUGS.pgm = &amp;quot;/Applications/OpenBUGS323/OpenBUGS.exe&amp;quot;,
working.directory = sub.folder,
WINE=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/wine&amp;quot;,
WINEPATH=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/winepath&amp;quot;)
}
# setting the parameters to be monitored
params &amp;lt;- c(&amp;quot;theta&amp;quot;, &amp;quot;mu.theta&amp;quot;, &amp;quot;sigma.theta&amp;quot;)
# calling the sfLapply function that will run
# parallel.bugs on each of the 2 CPUs
ptm &amp;lt;- proc.time()
sfLapply(1:2, fun=parallel.bugs, x.data=x.data, params=params)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [[1]]
## [1] &amp;quot;/Users/oliviergimenez/Desktop/chain1/CODAchain1.txt&amp;quot;
##
## [[2]]
## [1] &amp;quot;/Users/oliviergimenez/Desktop/chain2/CODAchain1.txt&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">elapsed_time = proc.time() - ptm
elapsed_time
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## user system elapsed
## 0.013 0.000 32.157
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r"># locating position of each CODA chain
chain1 &amp;lt;- paste(folder1, &amp;quot;/CODAchain1.txt&amp;quot;, sep=&amp;quot;&amp;quot;)
chain2 &amp;lt;- paste(folder2, &amp;quot;/CODAchain1.txt&amp;quot;, sep=&amp;quot;&amp;quot;)
# and, finally, getting the results
res &amp;lt;- read.bugs(c(chain1, chain2))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Abstracting deviance ... 490000 valid values
## Abstracting mu.theta ... 490000 valid values
## Abstracting sigma.theta ... 490000 valid values
## Abstracting theta[1] ... 490000 valid values
## Abstracting theta[2] ... 490000 valid values
## Abstracting theta[3] ... 490000 valid values
## Abstracting theta[4] ... 490000 valid values
## Abstracting theta[5] ... 490000 valid values
## Abstracting theta[6] ... 490000 valid values
## Abstracting theta[7] ... 490000 valid values
## Abstracting theta[8] ... 490000 valid values
## Abstracting deviance ... 490000 valid values
## Abstracting mu.theta ... 490000 valid values
## Abstracting sigma.theta ... 490000 valid values
## Abstracting theta[1] ... 490000 valid values
## Abstracting theta[2] ... 490000 valid values
## Abstracting theta[3] ... 490000 valid values
## Abstracting theta[4] ... 490000 valid values
## Abstracting theta[5] ... 490000 valid values
## Abstracting theta[6] ... 490000 valid values
## Abstracting theta[7] ... 490000 valid values
## Abstracting theta[8] ... 490000 valid values
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">summary(res)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>##
## Iterations = 10001:5e+05
## Thinning interval = 1
## Number of chains = 2
## Sample size per chain = 490000
##
## 1. Empirical mean and standard deviation for each variable,
## plus standard error of the mean:
##
## Mean SD Naive SE Time-series SE
## deviance 60.453 2.221 0.002243 0.005737
## mu.theta 8.109 5.261 0.005315 0.020596
## sigma.theta 6.610 5.682 0.005740 0.027336
## theta[1] 11.697 8.407 0.008493 0.027974
## theta[2] 8.023 6.395 0.006460 0.019264
## theta[3] 6.365 7.866 0.007946 0.022485
## theta[4] 7.735 6.601 0.006668 0.019766
## theta[5] 5.467 6.504 0.006570 0.022580
## theta[6] 6.234 6.885 0.006955 0.021332
## theta[7] 10.727 6.891 0.006961 0.023304
## theta[8] 8.648 7.892 0.007972 0.021541
##
## 2. Quantiles for each variable:
##
## 2.5% 25% 50% 75% 97.5%
## deviance 57.0200 59.120 60.040 61.430 65.99
## mu.theta -2.0600 4.784 8.066 11.410 18.50
## sigma.theta 0.2275 2.456 5.275 9.190 20.82
## theta[1] -1.8850 6.195 10.560 15.880 32.04
## theta[2] -4.8350 4.049 8.004 11.980 20.96
## theta[3] -11.4800 2.194 6.871 11.170 20.90
## theta[4] -5.7500 3.711 7.784 11.820 20.92
## theta[5] -8.8490 1.603 5.938 9.834 17.14
## theta[6] -8.8940 2.255 6.632 10.680 18.95
## theta[7] -1.3450 6.125 10.140 14.680 26.27
## theta[8] -6.8910 4.037 8.409 12.960 25.72
&lt;/code>&lt;/pre></description></item><item><title>Run OpenBUGS on a Mac</title><link>https://oliviergimenez.github.io/blog/run_openbugs_on_mac/</link><pubDate>Sat, 13 Jan 2018 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/run_openbugs_on_mac/</guid><description>&lt;img style="float:left;margin-right:10px;margin-top:10px;width:200px;margin-bottom:5px" src="https://oliviergimenez.github.io/img/bugs.jpg">
I had to use the good old `OpenBUGS` for some analyses that cannot be done in `JAGS`. Below are the steps to install `OpenBUGS` then to run it from your Mac either natively or from `R`. This tutorial is an adaptation of [this post](https://sites.google.com/site/mmeclimate/-bayesmet/openbugs-on-mac-os-x) and [that one](http://www.davideagle.org/r-2/bayesian-modeling-using-winbugs-and-openbugs/running-openbugs-on-mac-using-wine).
&lt;ol>
&lt;li>
&lt;p>If not done already, install
&lt;a href="https://brew.sh/" target="_blank" rel="noopener">Homebrew&lt;/a>. This program will make the installation of any other programs on your Mac so easy!&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Install
&lt;a href="https://www.winehq.org/" target="_blank" rel="noopener">Wine&lt;/a> which will allow you to run any Windows programs (.exe) on your Mac. To do so, start by
&lt;a href="http://blog.teamtreehouse.com/introduction-to-the-mac-os-x-command-line" target="_blank" rel="noopener">opening Terminal&lt;/a>, then type in the command: &lt;em>brew install wine&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Next, download the Windows version of &lt;code>OpenBUGS&lt;/code>
&lt;a href="https://www.mrc-bsu.cam.ac.uk/training/short-courses/bayescourse/download/" target="_blank" rel="noopener">here&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To install &lt;code>OpenBUGS&lt;/code>, still in Terminal, go to the directory where the file was downloaded and type (you might need to unzip the file you downloaded first): &lt;em>wine OpenBUGS323setup.exe&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>OpenBUGS&lt;/code> is now installed and ready to be used! You can run it by first going to the directory where &lt;code>OpenBUGS&lt;/code> was installed. On my laptop, it can be achieved via the command: &lt;em>cd /Applications/OpenBUGS323&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Then, you just need to tye in the following command in the Terminal, and you should see an OpenBUGS windows poping up: &lt;em>wine OpenBUGS&lt;/em>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Now we would like to run &lt;code>OpenBUGS&lt;/code> from &lt;code>R&lt;/code>.&lt;/p>
&lt;ol start="7">
&lt;li>Install the package &lt;code>R2OpenBUGS&lt;/code> by typing in the &lt;code>R&lt;/code> console:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-r">if(!require(R2OpenBUGS)) install.packages(&amp;quot;R2OpenBUGS&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Loading required package: R2OpenBUGS
&lt;/code>&lt;/pre>
&lt;ol start="8">
&lt;li>Now let&amp;rsquo;s see whether everything works well by running the classical &lt;code>BUGS&lt;/code> &lt;code>school&lt;/code> example:&lt;/li>
&lt;/ol>
&lt;p>Load the &lt;code>OpenBUGS&lt;/code> Package&lt;/p>
&lt;pre>&lt;code class="language-r">library(R2OpenBUGS)
&lt;/code>&lt;/pre>
&lt;p>Load the data&lt;/p>
&lt;pre>&lt;code class="language-r">data(schools)
&lt;/code>&lt;/pre>
&lt;p>Define the model, write it to a text file and have a look&lt;/p>
&lt;pre>&lt;code class="language-r">nummodel &amp;lt;- function(){
for (j in 1:J){
y[j] ~ dnorm (theta[j], tau.y[j])
theta[j] ~ dnorm (mu.theta, tau.theta)
tau.y[j] &amp;lt;- pow(sigma.y[j], -2)}
mu.theta ~ dnorm (0.0, 1.0E-6)
tau.theta &amp;lt;- pow(sigma.theta, -2)
sigma.theta ~ dunif (0, 1000)
}
write.model(nummodel, &amp;quot;nummodel.txt&amp;quot;)
model.file1 = paste(getwd(),&amp;quot;nummodel.txt&amp;quot;, sep=&amp;quot;/&amp;quot;)
file.show(&amp;quot;nummodel.txt&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Prepare the data for input into OpenBUGS&lt;/p>
&lt;pre>&lt;code class="language-r">J &amp;lt;- nrow(schools)
y &amp;lt;- schools$estimate
sigma.y &amp;lt;- schools$sd
data &amp;lt;- list (&amp;quot;J&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;sigma.y&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Initialization of variables&lt;/p>
&lt;pre>&lt;code class="language-r">inits &amp;lt;- function(){
list(theta = rnorm(J, 0, 100), mu.theta = rnorm(1, 0, 100), sigma.theta = runif(1, 0, 100))}
&lt;/code>&lt;/pre>
&lt;p>Set the &lt;code>Wine&lt;/code> working directory and the directory to &lt;code>OpenBUGS&lt;/code>, and change the OpenBUGS.exe location as necessary:&lt;/p>
&lt;pre>&lt;code class="language-r">WINE=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/wine&amp;quot;
WINEPATH=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/winepath&amp;quot;
OpenBUGS.pgm=&amp;quot;/Applications/OpenBUGS323/OpenBUGS.exe&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The are the parameters to save&lt;/p>
&lt;pre>&lt;code class="language-r">parameters = c(&amp;quot;theta&amp;quot;, &amp;quot;mu.theta&amp;quot;, &amp;quot;sigma.theta&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Run the model&lt;/p>
&lt;pre>&lt;code class="language-r">schools.sim &amp;lt;- bugs(data, inits, model.file = model.file1,parameters=parameters,n.chains = 3, n.iter = 1000, OpenBUGS.pgm=OpenBUGS.pgm, WINE=WINE, WINEPATH=WINEPATH,useWINE=T)
&lt;/code>&lt;/pre>
&lt;p>&lt;code>R&lt;/code> will pause. You might get a weird message starting by err:ole, just ignore it. When the run is complete, a prompt will reappear, then just type the following command to get the result:&lt;/p>
&lt;pre>&lt;code class="language-r">print(schools.sim)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Inference for Bugs model at &amp;quot;/Users/oliviergimenez/Desktop/nummodel.txt&amp;quot;,
## Current: 3 chains, each with 1000 iterations (first 500 discarded)
## Cumulative: n.sims = 1500 iterations saved
## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff
## theta[1] 12.2 7.9 -1.3 7.5 11.2 16.4 32.1 1.0 62
## theta[2] 9.1 6.5 -4.0 5.1 9.4 13.2 21.4 1.0 150
## theta[3] 7.8 7.7 -9.4 3.6 8.5 12.6 21.1 1.0 360
## theta[4] 8.8 6.6 -4.5 4.5 9.2 13.3 20.4 1.0 110
## theta[5] 6.8 6.9 -8.2 2.3 7.5 11.4 17.7 1.0 410
## theta[6] 7.3 7.2 -8.6 2.7 8.2 11.8 18.9 1.0 190
## theta[7] 11.5 6.4 -0.3 7.5 11.2 15.7 25.0 1.1 42
## theta[8] 9.7 7.6 -4.7 5.1 9.6 14.4 25.1 1.0 130
## mu.theta 9.2 5.2 -1.2 5.8 9.3 12.5 18.2 1.0 88
## sigma.theta 5.9 5.6 0.2 1.7 4.4 8.5 20.2 1.1 51
## deviance 60.7 2.2 57.2 59.2 60.1 61.9 65.6 1.0 120
##
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
##
## DIC info (using the rule, pD = Dbar-Dhat)
## pD = 2.8 and DIC = 63.4
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code>&lt;/pre>
&lt;p>When run natively, &lt;code>WinBUGS&lt;/code> and &lt;code>OpenBUGS&lt;/code> have nice debugging capabilities; also, you can see what is going on, I mean the program reading the data, generating inits, and so on. To get the &lt;code>OpenBUGS&lt;/code> window with a bunch of useful info, just add &lt;code>debug=T&lt;/code> to the call of the &lt;code>bugs&lt;/code> function, and re-run the model&lt;/p>
&lt;pre>&lt;code class="language-r">schools.sim &amp;lt;- bugs(data, inits, model.file = model.file1,parameters=parameters,n.chains = 3, n.iter = 1000, OpenBUGS.pgm=OpenBUGS.pgm, WINE=WINE, WINEPATH=WINEPATH,useWINE=T,debug=T)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## arguments 'show.output.on.console', 'minimized' and 'invisible' are for Windows only
&lt;/code>&lt;/pre>
&lt;p>You will have to close the &lt;code>OpenBUGS&lt;/code> window to get the prompt back.&lt;/p></description></item><item><title>Simulating data with JAGS</title><link>https://oliviergimenez.github.io/blog/sim_with_jags/</link><pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/sim_with_jags/</guid><description>&lt;img style="float:left;margin-right:10px;margin-top:10px;width:200px;margin-bottom:5px" src="https://oliviergimenez.github.io/img/posterior_plots_lr.png">
Here, I illustrate the possibility to use `JAGS` to simulate data with two examples that might be of interest to population ecologists: first a linear regression, second a Cormack-Jolly-Seber capture-recapture model to estimate animal survival (formulated as a state-space model). The code is available from [GitHub](https://github.com/oliviergimenez/simul_with_jags).
&lt;p>Recently, I have been struggling with simulating data from complex hierarchical models. After several unsuccessful attempts in &lt;code>R&lt;/code>, I remembered the good old times when I was using &lt;code>WinBUGS&lt;/code> (more than 10 years already!) and the possibility to simulate data with it. I&amp;rsquo;m using &lt;code>JAGS&lt;/code> now, and a quick search in Google with &amp;lsquo;simulating data with jags&amp;rsquo; led me to
&lt;a href="https://www.georg-hosoya.de/wordpress/?p=799" target="_blank" rel="noopener">a complex example&lt;/a> and
&lt;a href="https://stackoverflow.com/questions/38295839/simulate-data-in-jags-r2jags" target="_blank" rel="noopener">a simple example&lt;/a>.&lt;/p>
&lt;p>Simulating data with &lt;code>JAGS&lt;/code> is convenient because you can use (almost) the same code for simulation and inference, and you can carry out simulation studies (bias, precision, interval coverage) in the same environment (namely &lt;code>JAGS&lt;/code>).&lt;/p>
&lt;h2 id="linear-regression-example">Linear regression example&lt;/h2>
&lt;p>We first load the packages we need for this tutorial:&lt;/p>
&lt;pre>&lt;code class="language-r">library(R2jags)
library(runjags)
library(mcmcplots)
&lt;/code>&lt;/pre>
&lt;p>Then straight to the point, let&amp;rsquo;s generate data from a linear regression model. The trick is to use a &lt;code>data&lt;/code> block, have the simplest &lt;code>model&lt;/code> block you could think of and pass the parameters as if they were data. Note that it&amp;rsquo;d be possible to use only a model block, see comment
&lt;a href="https://stackoverflow.com/questions/38295839/simulate-data-in-jags-r2jags" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;pre>&lt;code class="language-r">txtstring &amp;lt;- '
data{
# Likelihood:
for (i in 1:N){
y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
mu[i] &amp;lt;- alpha + beta * x[i]
}
}
model{
fake &amp;lt;- 0
}
'
&lt;/code>&lt;/pre>
&lt;p>Here, &lt;code>alpha&lt;/code> and &lt;code>beta&lt;/code> are the intercept and slope, &lt;code>tau&lt;/code> the precision or the inverse of the variance, &lt;code>y&lt;/code> the response variable and &lt;code>x&lt;/code> the explanatory variable.&lt;/p>
&lt;p>We pick some values for the model parameters that we will use as data:&lt;/p>
&lt;pre>&lt;code class="language-r"># parameters for simulations
N = 30 # nb of observations
x &amp;lt;- 1:N # predictor
alpha = 0.5 # intercept
beta = 1 # slope
sigma &amp;lt;- .1 # residual sd
tau &amp;lt;- 1/(sigma*sigma) # precision
# parameters are treated as data for the simulation step
data&amp;lt;-list(N=N,x=x,alpha=alpha,beta=beta,tau=tau)
&lt;/code>&lt;/pre>
&lt;p>Now call &lt;code>JAGS&lt;/code>; note that we monitor the response variable instead of parameters as we would do when conducting standard inference:&lt;/p>
&lt;pre>&lt;code class="language-r"># run jags
out &amp;lt;- run.jags(txtstring, data = data,monitor=c(&amp;quot;y&amp;quot;),sample=1, n.chains=1, summarise=FALSE)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Compiling rjags model...
## Calling the simulation using the rjags method...
## Note: the model did not require adaptation
## Burning in the model for 4000 iterations...
## Running the model for 1 iterations...
## Simulation complete
## Finished running the simulation
&lt;/code>&lt;/pre>
&lt;p>The output is a bit messy and needs to be formatted appropriately:&lt;/p>
&lt;pre>&lt;code class="language-r"># reformat the outputs
Simulated &amp;lt;- coda::as.mcmc(out)
Simulated
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Markov Chain Monte Carlo (MCMC) output:
## Start = 5001
## End = 5001
## Thinning interval = 1
## y[1] y[2] y[3] y[4] y[5] y[6] y[7] y[8]
## 5001 1.288399 2.52408 3.61516 4.583587 5.600675 6.566052 7.593407 8.457497
## y[9] y[10] y[11] y[12] y[13] y[14] y[15] y[16]
## 5001 9.70847 10.38035 11.5105 12.55048 13.49143 14.46356 15.45641 16.56148
## y[17] y[18] y[19] y[20] y[21] y[22] y[23]
## 5001 17.50935 18.51501 19.66197 20.49477 21.57079 22.6199 23.48232
## y[24] y[25] y[26] y[27] y[28] y[29] y[30]
## 5001 24.57923 25.47368 26.33674 27.46525 28.35525 29.60279 30.42952
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">dim(Simulated)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 1 30
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">dat = as.vector(Simulated)
dat
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 1.288399 2.524080 3.615160 4.583587 5.600675 6.566052 7.593407
## [8] 8.457497 9.708470 10.380351 11.510500 12.550482 13.491435 14.463564
## [15] 15.456410 16.561483 17.509350 18.515005 19.661969 20.494767 21.570790
## [22] 22.619899 23.482317 24.579228 25.473676 26.336736 27.465251 28.355248
## [29] 29.602791 30.429517
&lt;/code>&lt;/pre>
&lt;p>Now let&amp;rsquo;s fit the model we used to simulate to the data we just generated. I won&amp;rsquo;t go into the details and assume that the reader is familiar with &lt;code>JAGS&lt;/code> and linear regression.&lt;/p>
&lt;pre>&lt;code class="language-r"># specify model in BUGS language
model &amp;lt;-
paste(&amp;quot;
model {
# Likelihood:
for (i in 1:N){
y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
mu[i] &amp;lt;- alpha + beta * x[i]
}
# Priors:
alpha ~ dnorm(0, 0.01) # intercept
beta ~ dnorm(0, 0.01) # slope
sigma ~ dunif(0, 100) # standard deviation
tau &amp;lt;- 1 / (sigma * sigma)
}
&amp;quot;)
writeLines(model,&amp;quot;lin_reg.jags&amp;quot;)
# data
jags.data &amp;lt;- list(y = dat, N = length(dat), x = x)
# initial values
inits &amp;lt;- function(){list(alpha = rnorm(1), beta = rnorm(1), sigma = runif(1,0,10))}
# parameters monitored
parameters &amp;lt;- c(&amp;quot;alpha&amp;quot;, &amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;)
# MCMC settings
ni &amp;lt;- 10000
nt &amp;lt;- 6
nb &amp;lt;- 5000
nc &amp;lt;- 2
# call JAGS from R
res &amp;lt;- jags(jags.data, inits, parameters, &amp;quot;lin_reg.jags&amp;quot;, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb, working.directory = getwd())
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## module glm loaded
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Compiling model graph
## Resolving undeclared variables
## Allocating nodes
## Graph information:
## Observed stochastic nodes: 30
## Unobserved stochastic nodes: 3
## Total graph size: 130
##
## Initializing model
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s have a look to the results and compare with the parameters we used to simulate the data (see above):&lt;/p>
&lt;pre>&lt;code class="language-r"># summarize posteriors
print(res, digits = 3)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Inference for Bugs model at &amp;quot;lin_reg.jags&amp;quot;, fit using jags,
## 2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 6
## n.sims = 1668 iterations saved
## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat
## alpha 0.544 0.038 0.469 0.518 0.545 0.570 0.617 1.000
## beta 0.998 0.002 0.994 0.997 0.998 1.000 1.003 1.001
## sigma 0.102 0.015 0.078 0.091 0.100 0.110 0.138 1.002
## deviance -53.810 2.724 -56.867 -55.808 -54.516 -52.641 -46.676 1.001
## n.eff
## alpha 1700
## beta 1700
## sigma 780
## deviance 1700
##
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
##
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 3.7 and DIC = -50.1
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code>&lt;/pre>
&lt;p>Pretty close!&lt;/p>
&lt;p>Check convergence:&lt;/p>
&lt;pre>&lt;code class="language-r"># trace plots
traplot(res,c(&amp;quot;alpha&amp;quot;, &amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://oliviergimenez.github.io/img/unnamed-chunk-8-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;p>Plot the posterior distribution of the regression parameters and residual standard deviation:&lt;/p>
&lt;pre>&lt;code class="language-r"># posterior distributions
denplot(res,c(&amp;quot;alpha&amp;quot;, &amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://oliviergimenez.github.io/img/unnamed-chunk-9-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;h2 id="capture-recapture-example">Capture-recapture example&lt;/h2>
&lt;p>I now illustrate the use of &lt;code>JAGS&lt;/code> to simulate data from a Cormack-Jolly-Seber model with constant survival and recapture probabilities. I assume that the reader is familiar with this model and its formulation as a state-space model.&lt;/p>
&lt;p>Let&amp;rsquo;s simulate!&lt;/p>
&lt;pre>&lt;code class="language-r">txtstring &amp;lt;- '
data{
# Constant survival and recapture probabilities
for (i in 1:nind){
for (t in f[i]:(n.occasions-1)){
phi[i,t] &amp;lt;- mean.phi
p[i,t] &amp;lt;- mean.p
} #t
} #i
# Likelihood
for (i in 1:nind){
# Define latent state and obs at first capture
z[i,f[i]] &amp;lt;- 1
mu2[i,1] &amp;lt;- 1 * z[i,f[i]] # detection is 1 at first capture (&amp;quot;conditional on first capture&amp;quot;)
y[i,1] ~ dbern(mu2[i,1])
# then deal w/ subsequent occasions
for (t in (f[i]+1):n.occasions){
# State process
z[i,t] ~ dbern(mu1[i,t])
mu1[i,t] &amp;lt;- phi[i,t-1] * z[i,t-1]
# Observation process
y[i,t] ~ dbern(mu2[i,t])
mu2[i,t] &amp;lt;- p[i,t-1] * z[i,t]
} #t
} #i
}
model{
fake &amp;lt;- 0
}
'
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s pick some values for parameters and store them in a data list:&lt;/p>
&lt;pre>&lt;code class="language-r"># parameter for simulations
n.occasions = 10 # nb of occasions
nind = 100 # nb of individuals
mean.phi &amp;lt;- 0.8 # survival
mean.p &amp;lt;- 0.6 # recapture
f = rep(1,nind) # date of first capture
data&amp;lt;-list(n.occasions = n.occasions, mean.phi = mean.phi, mean.p = mean.p, f = f, nind = nind)
&lt;/code>&lt;/pre>
&lt;p>Now run &lt;code>JAGS&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-r">out &amp;lt;- run.jags(txtstring, data = data,monitor=c(&amp;quot;y&amp;quot;),sample=1, n.chains=1, summarise=FALSE)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Compiling rjags model...
## Calling the simulation using the rjags method...
## Note: the model did not require adaptation
## Burning in the model for 4000 iterations...
## Running the model for 1 iterations...
## Simulation complete
## Finished running the simulation
&lt;/code>&lt;/pre>
&lt;p>Format the output:&lt;/p>
&lt;pre>&lt;code class="language-r">Simulated &amp;lt;- coda::as.mcmc(out)
dim(Simulated)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 1 1000
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">dat = matrix(Simulated,nrow=nind)
head(dat)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,] 1 1 0 0 0 0 0 0 0 0
## [2,] 1 1 1 1 0 0 0 0 0 0
## [3,] 1 0 0 0 0 0 0 0 0 0
## [4,] 1 0 0 0 0 0 0 0 0 0
## [5,] 1 0 0 0 0 0 0 0 0 0
## [6,] 1 1 1 1 0 0 1 0 1 1
&lt;/code>&lt;/pre>
&lt;p>Here I monitored only the detections and non-detections, but it is also possible to get the simulated values for the states, i.e. whether an individual is alive or dead at each occasion. You just need to amend the call to &lt;code>JAGS&lt;/code> with &lt;code>monitor=c(&amp;quot;y&amp;quot;,&amp;quot;x&amp;quot;)&lt;/code> and to amend the output accordingly.&lt;/p>
&lt;p>Now we fit a Cormack-Jolly-Seber model to the data we&amp;rsquo;ve just simulated, assuming constant parameters:&lt;/p>
&lt;pre>&lt;code class="language-r">model &amp;lt;-
paste(&amp;quot;
model {
# Priors and constraints
for (i in 1:nind){
for (t in f[i]:(n.occasions-1)){
phi[i,t] &amp;lt;- mean.phi
p[i,t] &amp;lt;- mean.p
} #t
} #i
mean.phi ~ dunif(0, 1) # Prior for mean survival
mean.p ~ dunif(0, 1) # Prior for mean recapture
# Likelihood
for (i in 1:nind){
# Define latent state at first capture
z[i,f[i]] &amp;lt;- 1
for (t in (f[i]+1):n.occasions){
# State process
z[i,t] ~ dbern(mu1[i,t])
mu1[i,t] &amp;lt;- phi[i,t-1] * z[i,t-1]
# Observation process
y[i,t] ~ dbern(mu2[i,t])
mu2[i,t] &amp;lt;- p[i,t-1] * z[i,t]
} #t
} #i
}
&amp;quot;)
writeLines(model,&amp;quot;cjs.jags&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Prepare the data:&lt;/p>
&lt;pre>&lt;code class="language-r"># vector with occasion of marking
get.first &amp;lt;- function(x) min(which(x!=0))
f &amp;lt;- apply(dat, 1, get.first)
# data
jags.data &amp;lt;- list(y = dat, f = f, nind = dim(dat)[1], n.occasions = dim(dat)[2])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r"># Initial values
known.state.cjs &amp;lt;- function(ch){
state &amp;lt;- ch
for (i in 1:dim(ch)[1]){
n1 &amp;lt;- min(which(ch[i,]==1))
n2 &amp;lt;- max(which(ch[i,]==1))
state[i,n1:n2] &amp;lt;- 1
state[i,n1] &amp;lt;- NA
}
state[state==0] &amp;lt;- NA
return(state)
}
inits &amp;lt;- function(){list(mean.phi = runif(1, 0, 1), mean.p = runif(1, 0, 1), z = known.state.cjs(dat))}
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;d like to carry out inference about survival and recapture probabilities:&lt;/p>
&lt;pre>&lt;code class="language-r">parameters &amp;lt;- c(&amp;quot;mean.phi&amp;quot;, &amp;quot;mean.p&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Standard MCMC settings:&lt;/p>
&lt;pre>&lt;code class="language-r">ni &amp;lt;- 10000
nt &amp;lt;- 6
nb &amp;lt;- 5000
nc &amp;lt;- 2
&lt;/code>&lt;/pre>
&lt;p>Ready to run &lt;code>JAGS&lt;/code>!&lt;/p>
&lt;pre>&lt;code class="language-r"># Call JAGS from R (BRT 1 min)
cjs &amp;lt;- jags(jags.data, inits, parameters, &amp;quot;cjs.jags&amp;quot;, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb, working.directory = getwd())
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Compiling model graph
## Resolving undeclared variables
## Allocating nodes
## Graph information:
## Observed stochastic nodes: 900
## Unobserved stochastic nodes: 902
## Total graph size: 3707
##
## Initializing model
&lt;/code>&lt;/pre>
&lt;p>Summarize posteriors and compare to the values we used to simulate the data:&lt;/p>
&lt;pre>&lt;code class="language-r">print(cjs, digits = 3)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Inference for Bugs model at &amp;quot;cjs.jags&amp;quot;, fit using jags,
## 2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 6
## n.sims = 1668 iterations saved
## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat
## mean.p 0.596 0.033 0.531 0.574 0.597 0.618 0.660 1.000
## mean.phi 0.784 0.021 0.742 0.770 0.785 0.799 0.824 1.001
## deviance 440.611 18.374 408.121 427.569 438.662 452.512 479.608 1.001
## n.eff
## mean.p 1700
## mean.phi 1700
## deviance 1700
##
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
##
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 168.9 and DIC = 609.5
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code>&lt;/pre>
&lt;p>Again pretty close!&lt;/p>
&lt;p>Trace plots&lt;/p>
&lt;pre>&lt;code class="language-r">traplot(cjs,c(&amp;quot;mean.phi&amp;quot;, &amp;quot;mean.p&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://oliviergimenez.github.io/img/unnamed-chunk-21-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;p>Posterior distribution plots:&lt;/p>
&lt;pre>&lt;code class="language-r">denplot(cjs,c(&amp;quot;mean.phi&amp;quot;, &amp;quot;mean.p&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://oliviergimenez.github.io/img/unnamed-chunk-22-1.png" alt="">&lt;!-- -->&lt;/p></description></item><item><title>Analysing the social Star Wars network in The Attack of the Clones with R</title><link>https://oliviergimenez.github.io/blog/starwars_network/</link><pubDate>Sun, 07 Aug 2016 12:00:00 +0000</pubDate><guid>https://oliviergimenez.github.io/blog/starwars_network/</guid><description>&lt;p>This is a free adaptation of two (very) clever analyses made by others:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>
&lt;a href="http://evelinag.com/blog/2015/12-15-star-wars-social-network" target="_blank" rel="noopener">The Star Wars Social Network by Evelina Gabasov&lt;/a> in which program F# was mostly used to analyse the Star wars social networks&lt;/p>
&lt;/li>
&lt;li>
&lt;p>
&lt;a href="http://varianceexplained.org/r/love-actually-network/" target="_blank" rel="noopener">Analyzing networks of characters in &amp;lsquo;Love Actually&amp;rsquo; by David Robinson&lt;/a> in which R was used to analyse the links between the characters of the movie Love Actually.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The aim here is to try and reproduce Evelina&amp;rsquo;s analysis using R only, using David&amp;rsquo;s contribution plus several tweaks I found here and there on the internet. The R code and data are available on my
&lt;a href="https://github.com/oliviergimenez/starwars_network" target="_blank" rel="noopener">GitHub page&lt;/a>.&lt;/p>
&lt;p>&lt;em>Disclaimer&lt;/em>: The original blog posts are awesome and full of relevant details, check them out! My objective here was to teach myself how to manipulate data using trendy R packages and do some network analyses. Some comments below have been copied and pasted from these blogs, the credits entirely go to the authors Evelina and David. Last but not least, my code comes with mistakes probably.&lt;/p>
&lt;h1 id="read-and-format-data">Read and format data&lt;/h1>
&lt;p>First, read in data. I found the movie script in doc format
&lt;a href="theforce.net/timetales/ep2se.doc">here&lt;/a>, which I converted in txt format for convenience. Then, apply various treatments to have the data ready for analysis. I use the old school way for modifying the original dataframe.
&lt;a href="https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf" target="_blank" rel="noopener">Piping&lt;/a> would have made the code more readable, but I do not feel confident with this approach yet.&lt;/p>
&lt;pre>&lt;code class="language-r"># load convenient packages
library(dplyr)
library(stringr)
library(tidyr)
# read file line by line
raw &amp;lt;- readLines(&amp;quot;attack-of-the-clones.txt&amp;quot;)
# create data frame
lines &amp;lt;- data_frame(raw = raw)
# get rid of leading and trailing white spaces
# http://stackoverflow.com/questions/2261079/how-to-trim-leading-and-trailing-whitespace-in-r
trim &amp;lt;- function (x) gsub(&amp;quot;^\\s+|\\s+$&amp;quot;, &amp;quot;&amp;quot;, x)
lines &amp;lt;- mutate(lines,raw=trim(raw))
# get rid of the empty lines
lines2 &amp;lt;- filter(lines, raw != &amp;quot;&amp;quot;)
# detect scenes: begin by EXT. or INT.
lines3 &amp;lt;- mutate(lines2, is_scene = str_detect(raw, &amp;quot;T.&amp;quot;),scene = cumsum(is_scene))
# drop lines that start with EXT. or INT.
lines4 &amp;lt;- filter(lines3,!is_scene)
# distinguish characters from what they say
lines5 &amp;lt;- separate(lines4, raw, c(&amp;quot;speaker&amp;quot;, &amp;quot;dialogue&amp;quot;), sep = &amp;quot;:&amp;quot;, fill = &amp;quot;left&amp;quot;,extra='drop')
# read in aliases (from Evelina's post)
aliases &amp;lt;- read.table('aliases.csv',sep=',',header=T,colClasses = &amp;quot;character&amp;quot;)
aliases$Alias
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;BEN&amp;quot; &amp;quot;SEE-THREEPIO&amp;quot; &amp;quot;THREEPIO&amp;quot; &amp;quot;ARTOO-DETOO&amp;quot;
## [5] &amp;quot;ARTOO&amp;quot; &amp;quot;PALPATINE&amp;quot; &amp;quot;DARTH SIDIOUS&amp;quot; &amp;quot;BAIL&amp;quot;
## [9] &amp;quot;MACE&amp;quot; &amp;quot;WINDU&amp;quot; &amp;quot;MACE-WINDU&amp;quot; &amp;quot;NUTE&amp;quot;
## [13] &amp;quot;AUNT BERU&amp;quot; &amp;quot;DOOKU&amp;quot; &amp;quot;BOBA&amp;quot; &amp;quot;JANGO&amp;quot;
## [17] &amp;quot;PANAKA&amp;quot; &amp;quot;NUTE&amp;quot; &amp;quot;KI-ADI&amp;quot; &amp;quot;BIBBLE&amp;quot;
## [21] &amp;quot;BIB&amp;quot; &amp;quot;CHEWIE&amp;quot; &amp;quot;VADER&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">aliases$Name
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;OBI-WAN&amp;quot; &amp;quot;C-3PO&amp;quot; &amp;quot;C-3PO&amp;quot; &amp;quot;R2-D2&amp;quot;
## [5] &amp;quot;R2-D2&amp;quot; &amp;quot;EMPEROR&amp;quot; &amp;quot;EMPEROR&amp;quot; &amp;quot;BAIL ORGANA&amp;quot;
## [9] &amp;quot;MACE WINDU&amp;quot; &amp;quot;MACE WINDU&amp;quot; &amp;quot;MACE WINDU&amp;quot; &amp;quot;NUTE GUNRAY&amp;quot;
## [13] &amp;quot;BERU&amp;quot; &amp;quot;COUNT DOOKU&amp;quot; &amp;quot;BOBA FETT&amp;quot; &amp;quot;JANGO FETT&amp;quot;
## [17] &amp;quot;CAPTAIN PANAKA&amp;quot; &amp;quot;NUTE GUNRAY&amp;quot; &amp;quot;KI-ADI-MUNDI&amp;quot; &amp;quot;SIO BIBBLE&amp;quot;
## [21] &amp;quot;BIB FORTUNA&amp;quot; &amp;quot;CHEWBACCA&amp;quot; &amp;quot;DARTH VADER&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r"># assign unique name to characters
# http://stackoverflow.com/questions/28593265/is-there-a-function-like-switch-which-works-inside-of-dplyrmutate
multipleReplace &amp;lt;- function(x, what, by) {
stopifnot(length(what)==length(by))
ind &amp;lt;- match(x, what)
ifelse(is.na(ind),x,by[ind])
}
lines6 &amp;lt;- mutate(lines5,speaker=multipleReplace(speaker,what=aliases$Alias,by=aliases$Name))
# read in actual names (from Evelina's post)
actual.names &amp;lt;- read.csv('characters.csv',header=F,colClasses = &amp;quot;character&amp;quot;)
actual.names &amp;lt;- c(as.matrix(actual.names))
# filter out non-characters
lines7 &amp;lt;- filter(lines6,speaker %in% actual.names)
# group by scene
lines8 &amp;lt;- group_by(lines7, scene, line = cumsum(!is.na(speaker)))
lines9 &amp;lt;- summarize(lines8, speaker = speaker[1], dialogue = str_c(dialogue, collapse = &amp;quot; &amp;quot;))
# Count the lines-per-scene-per-character
# Turn the result into a binary speaker-by-scene matrix
by_speaker_scene &amp;lt;- count(lines9, scene, speaker)
by_speaker_scene
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## # A tibble: 447 x 3
## # Groups: scene [321]
## scene speaker n
## &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;
## 1 26 PADME 1
## 2 27 PADME 1
## 3 29 PADME 1
## 4 48 PADME 1
## 5 50 PADME 2
## 6 66 MACE WINDU 1
## 7 67 MACE WINDU 1
## 8 69 YODA 1
## 9 70 MACE WINDU 1
## 10 74 YODA 1
## # ... with 437 more rows
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">library(reshape2)
speaker_scene_matrix &amp;lt;-acast(by_speaker_scene , speaker ~ scene, fun.aggregate = length)
dim(speaker_scene_matrix)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 19 321
&lt;/code>&lt;/pre>
&lt;h1 id="analyses">Analyses&lt;/h1>
&lt;h2 id="hierarchical-clustering">Hierarchical clustering&lt;/h2>
&lt;pre>&lt;code class="language-r">norm &amp;lt;- speaker_scene_matrix / rowSums(speaker_scene_matrix)
h &amp;lt;- hclust(dist(norm, method = &amp;quot;manhattan&amp;quot;))
plot(h)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://oliviergimenez.github.io/img/starwars_network_files/figure-html/unnamed-chunk-2-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;h2 id="timeline">Timeline&lt;/h2>
&lt;p>Use tree to give an ordering that puts similar characters close together&lt;/p>
&lt;pre>&lt;code class="language-r">ordering &amp;lt;- h$labels[h$order]
ordering
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;MACE WINDU&amp;quot; &amp;quot;YODA&amp;quot; &amp;quot;SHMI&amp;quot; &amp;quot;QUI-GON&amp;quot; &amp;quot;PLO KOON&amp;quot;
## [6] &amp;quot;LAMA SU&amp;quot; &amp;quot;OBI-WAN&amp;quot; &amp;quot;BAIL ORGANA&amp;quot; &amp;quot;JAR JAR&amp;quot; &amp;quot;POGGLE&amp;quot;
## [11] &amp;quot;ANAKIN&amp;quot; &amp;quot;PADME&amp;quot; &amp;quot;CLIEGG&amp;quot; &amp;quot;BERU&amp;quot; &amp;quot;OWEN&amp;quot;
## [16] &amp;quot;SIO BIBBLE&amp;quot; &amp;quot;RUWEE&amp;quot; &amp;quot;JOBAL&amp;quot; &amp;quot;SOLA&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>This ordering can be used to make other graphs more informative. For instance, we can visualize a timeline of all scenes:&lt;/p>
&lt;pre>&lt;code class="language-r">scenes &amp;lt;- filter(by_speaker_scene, n() &amp;gt; 1) # scenes with &amp;gt; 1 character
scenes2 &amp;lt;- ungroup(scenes)
scenes3 &amp;lt;- mutate(scenes2, scene = as.numeric(factor(scene)),
character = factor(speaker, levels = ordering))
library(ggplot2)
ggplot(scenes3, aes(scene, character)) +
geom_point() +
geom_path(aes(group = scene))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://oliviergimenez.github.io/img/starwars_network_files/figure-html/unnamed-chunk-4-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;p>Create a cooccurence matrix (see
&lt;a href="http://stackoverflow.com/questions/13281303/creating-co-occurrence-matrix" target="_blank" rel="noopener">here&lt;/a>) containing how many times two characters share scenes&lt;/p>
&lt;pre>&lt;code class="language-r">cooccur &amp;lt;- speaker_scene_matrix %*% t(speaker_scene_matrix)
heatmap(cooccur)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://oliviergimenez.github.io/img/starwars_network_files/figure-html/unnamed-chunk-5-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;h2 id="social-network-analyses">Social network analyses&lt;/h2>
&lt;h3 id="graphical-representation-of-the-network">Graphical representation of the network&lt;/h3>
&lt;p>Here the nodes represent characters in the movies. The characters are connected by a link if they both speak in the same scene. And the more the characters speak together, the thicker the link between them.&lt;/p>
&lt;pre>&lt;code class="language-r">library(igraph)
g &amp;lt;- graph.adjacency(cooccur, weighted = TRUE, mode = &amp;quot;undirected&amp;quot;, diag = FALSE)
plot(g, edge.width = E(g)$weight)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://oliviergimenez.github.io/img/starwars_network_files/figure-html/unnamed-chunk-6-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;p>Compute standard network features, degree and betweeness.&lt;/p>
&lt;pre>&lt;code class="language-r">degree(g)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## ANAKIN BAIL ORGANA BERU CLIEGG JAR JAR JOBAL
## 12 1 4 4 4 4
## LAMA SU MACE WINDU OBI-WAN OWEN PADME PLO KOON
## 1 5 6 4 12 0
## POGGLE QUI-GON RUWEE SHMI SIO BIBBLE SOLA
## 1 1 4 1 0 4
## YODA
## 4
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">betweenness(g)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## ANAKIN BAIL ORGANA BERU CLIEGG JAR JAR JOBAL
## 42.600000 0.000000 1.750000 0.500000 22.000000 0.000000
## LAMA SU MACE WINDU OBI-WAN OWEN PADME PLO KOON
## 0.000000 18.366667 15.000000 5.250000 55.133333 0.000000
## POGGLE QUI-GON RUWEE SHMI SIO BIBBLE SOLA
## 0.000000 0.000000 0.700000 0.000000 0.000000 5.000000
## YODA
## 3.366667
&lt;/code>&lt;/pre>
&lt;p>To get a nicer representation of the network, see
&lt;a href="http://tagteam.harvard.edu/hub_feeds/1981/feed_items/1388531" target="_blank" rel="noopener">here&lt;/a> and the formating from igraph to d3Network. Below is the code youâ€™d need:&lt;/p>
&lt;pre>&lt;code class="language-r">library(d3Network)
library(networkD3)
sg &amp;lt;- simplify(g)
df &amp;lt;- get.edgelist(g, names=TRUE)
df &amp;lt;- as.data.frame(df)
colnames(df) &amp;lt;- c('source', 'target')
df$value &amp;lt;- rep(1, nrow(df))
# get communities
fc &amp;lt;- fastgreedy.community(g)
com &amp;lt;- membership(fc)
node.info &amp;lt;- data.frame(name=names(com), group=as.vector(com))
links &amp;lt;- data.frame(source=match(df$source, node.info$name)-1,target=match(df$target, node.info$name)-1,value=df$value)
forceNetwork(Links = links, Nodes = node.info,Source = &amp;quot;source&amp;quot;, Target = &amp;quot;target&amp;quot;,Value = &amp;quot;value&amp;quot;, NodeID = &amp;quot;name&amp;quot;,Group = &amp;quot;group&amp;quot;, opacity = 1, opacityNoHover=1)
&lt;/code>&lt;/pre>
&lt;p>The nodes represent characters in the movies. The characters are connected by a link if they both speak in the same scene. The colors are for groups obtained by some algorithms.&lt;/p></description></item></channel></rss>