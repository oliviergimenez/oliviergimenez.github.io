<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Olivier Gimenez</title>
    <link>https://oliviergimenez.github.io/tags/r/</link>
      <atom:link href="https://oliviergimenez.github.io/tags/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>¬© Olivier Gimenez 2025</copyright><lastBuildDate>Mon, 27 Mar 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://oliviergimenez.github.io/img/flyfishing.jpg</url>
      <title>R</title>
      <link>https://oliviergimenez.github.io/tags/r/</link>
    </image>
    
    <item>
      <title>How to assess landscape connectivity with spatial capture-recapture models?</title>
      <link>https://oliviergimenez.github.io/blog/connectoscr/</link>
      <pubDate>Mon, 27 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/connectoscr/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;How to assess landscape connectivity with spatial capture-recapture models? &lt;a href=&#34;https://twitter.com/oSCR_package?ref_src=twsrc%5Etfw&#34;&gt;@oSCR_package&lt;/a&gt; to the rescue ! Some code to reproduce results from a great paper by Dana Morin &lt;a href=&#34;https://twitter.com/Fuller_Lab?ref_src=twsrc%5Etfw&#34;&gt;@Fuller_Lab&lt;/a&gt; &lt;a href=&#34;https://twitter.com/andyroyle_pwrc?ref_src=twsrc%5Etfw&#34;&gt;@andyroyle_pwrc&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/chrissuthy?ref_src=twsrc%5Etfw&#34;&gt;@chrissuthy&lt;/a&gt; &lt;br&gt;&lt;br&gt;Post: &lt;a href=&#34;https://t.co/FVwBzMnqA9&#34;&gt;https://t.co/FVwBzMnqA9&lt;/a&gt;&lt;br&gt;&lt;br&gt;Code: &lt;a href=&#34;https://t.co/FMMyF0s22x&#34;&gt;https://t.co/FMMyF0s22x&lt;/a&gt; &lt;a href=&#34;https://t.co/QUlOjlGnPf&#34;&gt;pic.twitter.com/QUlOjlGnPf&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññü¶¶ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1640272014505127936?ref_src=twsrc%5Etfw&#34;&gt;March 27, 2023&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt; 
</description>
    </item>
    
    <item>
      <title>Workshop on quantitative methods for population dynamics in R</title>
      <link>https://oliviergimenez.github.io/blog/annonceworkshoppopdyn/</link>
      <pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/annonceworkshoppopdyn/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üì¢üëã Together with &lt;a href=&#34;https://twitter.com/SarahCubaynes?ref_src=twsrc%5Etfw&#34;&gt;@SarahCubaynes&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/abesnardEPHE?ref_src=twsrc%5Etfw&#34;&gt;@abesnardEPHE&lt;/a&gt; we will give a 2-day introductory workshop June 1-2 on quantitative methods for population dynamics in R &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;br&gt;&lt;br&gt;Join us ü•≥ It&amp;#39;s online and free of charge üòâ You just need to register via &lt;a href=&#34;https://t.co/otguiSMTY0&#34;&gt;https://t.co/otguiSMTY0&lt;/a&gt;&lt;br&gt;&lt;br&gt;Please RT üòá &lt;a href=&#34;https://t.co/6ghDHE92SJ&#34;&gt;pic.twitter.com/6ghDHE92SJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññü¶¶ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1623420332454420481?ref_src=twsrc%5Etfw&#34;&gt;February 8, 2023&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt; 
</description>
    </item>
    
    <item>
      <title>VIBASS and Bayesian capture-recapture workshop</title>
      <link>https://oliviergimenez.github.io/blog/annoncevibass/</link>
      <pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/annoncevibass/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Ey! VIBASS 6 is announced online now &lt;a href=&#34;https://t.co/VS8cLR8ocK&#34;&gt;https://t.co/VS8cLR8ocK&lt;/a&gt;&lt;br&gt;üóìÔ∏è July 10-14 2023&lt;br&gt;üö©Faculty of Mathematics &lt;a href=&#34;https://twitter.com/FMatemaUV_EG?ref_src=twsrc%5Etfw&#34;&gt;@FMatemaUV_EG&lt;/a&gt;, Valencia, Spain &lt;br&gt;&lt;br&gt;Be aware that the registration would start really really soon and we just have few spots. Don&amp;#39;t miss it. &lt;a href=&#34;https://t.co/5M61dbuxuF&#34;&gt;pic.twitter.com/5M61dbuxuF&lt;/a&gt;&lt;/p&gt;&amp;mdash; VIBASS 6 (@vibass7) &lt;a href=&#34;https://twitter.com/vibass7/status/1613928870192742401?ref_src=twsrc%5Etfw&#34;&gt;January 13, 2023&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt; 
</description>
    </item>
    
    <item>
      <title>Introduction on data wrangling with R</title>
      <link>https://oliviergimenez.github.io/blog/2022-05-17-introduction-on-data-wrangling-with-r/</link>
      <pubDate>Tue, 17 May 2022 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/2022-05-17-introduction-on-data-wrangling-with-r/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üîßüöß Intro on data wrangling w/ R üì¶ dplyr - import data, pipe, pivot, join, filter, count, work w/ columns, dates, char, factors, etc &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt;&lt;br&gt;&lt;br&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://t.co/Vs2TbYhbBj&#34;&gt;https://t.co/Vs2TbYhbBj&lt;/a&gt; ‚óÄÔ∏è&lt;br&gt;&lt;br&gt;w/ practicals &lt;a href=&#34;https://t.co/9ktKPp9wfW&#34;&gt;https://t.co/9ktKPp9wfW&lt;/a&gt; &amp;amp; solutions &lt;a href=&#34;https://t.co/YPwibL2w31&#34;&gt;https://t.co/YPwibL2w31&lt;/a&gt; &lt;br&gt;&lt;br&gt;Cartoons &lt;a href=&#34;https://twitter.com/allison_horst?ref_src=twsrc%5Etfw&#34;&gt;@allison_horst&lt;/a&gt; üíú &lt;a href=&#34;https://t.co/ZMSfxYFKWq&#34;&gt;pic.twitter.com/ZMSfxYFKWq&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1526574510392188928?ref_src=twsrc%5Etfw&#34;&gt;May 17, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt; 
</description>
    </item>
    
    <item>
      <title>Introduction to GIS and mapping in R</title>
      <link>https://oliviergimenez.github.io/blog/2022-05-12-introduction-to-gis-and-mapping-in-r/</link>
      <pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/2022-05-12-introduction-to-gis-and-mapping-in-r/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üòá I updated the slides of my introduction to &lt;a href=&#34;https://twitter.com/hashtag/GIS?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#GIS&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/hashtag/mapping?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#mapping&lt;/a&gt; in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; using the &lt;a href=&#34;https://twitter.com/hashtag/sf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#sf&lt;/a&gt; package and üêª üó∫Ô∏è in the &lt;a href=&#34;https://twitter.com/hashtag/pyrenees?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#pyrenees&lt;/a&gt; üèîÔ∏è &lt;br&gt;&lt;br&gt;üßë‚Äçüè´ Slides: &lt;a href=&#34;https://t.co/mO4Dg8l1H5&#34;&gt;https://t.co/mO4Dg8l1H5&lt;/a&gt;&lt;br&gt;üßë‚Äçüíª Code: &lt;a href=&#34;https://t.co/X8fDKcrbFR&#34;&gt;https://t.co/X8fDKcrbFR&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/rspatial?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rspatial&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/spatial?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#spatial&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ggplot2?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ggplot2&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/dataviz?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dataviz&lt;/a&gt; &lt;a href=&#34;https://t.co/mSsL6Qt7IS&#34;&gt;pic.twitter.com/mSsL6Qt7IS&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1524840175037206559?ref_src=twsrc%5Etfw&#34;&gt;May 12, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt; 
</description>
    </item>
    
    <item>
      <title>Matrix population models: Lecture slides and R codes for a workshop</title>
      <link>https://oliviergimenez.github.io/blog/2022-05-04-matrix-population-models-lecture-slides-and-r-codes-for-a-workshop/</link>
      <pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/2022-05-04-matrix-population-models-lecture-slides-and-r-codes-for-a-workshop/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Happy to share lecture slides &amp;amp; R codes of an earlier version of the workshop üëá we ran back in 2014 ü§Ø w JD Lebreton &amp;amp; &lt;a href=&#34;https://twitter.com/KoonsLab?ref_src=twsrc%5Etfw&#34;&gt;@KoonsLab&lt;/a&gt; &lt;br&gt;&lt;br&gt;‚û°Ô∏è &lt;a href=&#34;https://t.co/XdcCQ560Pz&#34;&gt;https://t.co/XdcCQ560Pz&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/RStats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#RStats&lt;/a&gt; &lt;br&gt;&lt;br&gt;For video üìΩÔ∏èüì∫, check out our workshop on pop dynamics &lt;a href=&#34;https://t.co/jlTE4iF5zk&#34;&gt;https://t.co/jlTE4iF5zk&lt;/a&gt; w &lt;a href=&#34;https://twitter.com/SarahCubaynes?ref_src=twsrc%5Etfw&#34;&gt;@SarahCubaynes&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/abesnardEPHE?ref_src=twsrc%5Etfw&#34;&gt;@abesnardEPHE&lt;/a&gt; &lt;a href=&#34;https://t.co/JFgS1GOU0V&#34;&gt;https://t.co/JFgS1GOU0V&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1521829494981496833?ref_src=twsrc%5Etfw&#34;&gt;May 4, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt; 
</description>
    </item>
    
    <item>
      <title>Bayesian analyses made easy: GLMMs in R package brms</title>
      <link>https://oliviergimenez.github.io/blog/glmm-brms/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/glmm-brms/</guid>
      <description>&lt;p&gt;Here I illustrate how to fit GLMMs with the R package &lt;code&gt;brms&lt;/code&gt;, and compare to &lt;code&gt;Jags&lt;/code&gt; and &lt;code&gt;lme4&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;I regularly give 
&lt;a href=&#34;https://oliviergimenez.github.io/bayesian-stats-with-R/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a course on Bayesian statistics with &lt;code&gt;R&lt;/code&gt; for non-specialists&lt;/a&gt;. To illustrate the course, we analyse data with 
&lt;a href=&#34;https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;generalized linear, often mixed, models or GLMMs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So far, I&amp;rsquo;ve been using &lt;code&gt;Jags&lt;/code&gt; to fit these models. This requires some programming skills, like e.g. coding a loop, to be able to write down the model likelihood. Although students learn a lot from going through that process, it can be daunting.&lt;/p&gt;
&lt;p&gt;This year, I thought I&amp;rsquo;d show them the 
&lt;a href=&#34;https://paul-buerkner.github.io/brms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;R&lt;/code&gt; package &lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; developed by 
&lt;a href=&#34;https://paul-buerkner.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paul-Christian B√ºrkner&lt;/a&gt;. In brief, &lt;code&gt;brms&lt;/code&gt; allows fitting GLMMs (but not only) in a &lt;code&gt;lme4&lt;/code&gt;-like syntax within the Bayesian framework and MCMC methods with Stan.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m not a Stan user, but it doesn&amp;rsquo;t matter. The 
&lt;a href=&#34;https://paul-buerkner.github.io/brms/articles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vignettes&lt;/a&gt; were more than enough to get me started. I also recommend the 
&lt;a href=&#34;https://paul-buerkner.github.io/blog/brms-blogposts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;list of blog posts about &lt;code&gt;brms&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First things first, we load the packages we will need:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(brms)
library(posterior) # tools for working with posterior and prior distributions
library(R2jags) # run Jags from within R
library(lme4) # fit GLMM in frequentist framework
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;beta-binomial-model&#34;&gt;Beta-Binomial model&lt;/h1&gt;
&lt;p&gt;The first example is about a survival experiment. We captured, marked and released 57 individuals (&lt;em&gt;total&lt;/em&gt;) out of which 19 made it through the year (&lt;em&gt;alive&lt;/em&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat &amp;lt;- data.frame(alive = 19, total = 57)
dat
&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
  &lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;alive&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;total&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;19&#34;,&#34;2&#34;:&#34;57&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;An obvious estimate of the probability of a binomial is the proportion of cases, $19/57 = 0.3333333$ here, but let&amp;rsquo;s use &lt;code&gt;R&lt;/code&gt; built-in functions for the sake of illustration.&lt;/p&gt;
&lt;h2 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum-likelihood estimation&lt;/h2&gt;
&lt;p&gt;To get an estimate of survival, we fit a logistic regression using the &lt;code&gt;glm()&lt;/code&gt; function. The data are grouped (or aggregated), so we need to specify the number of alive and dead individuals as a two-column matrix on the left hand side of the model formula:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mle &amp;lt;- glm(cbind(alive, total - alive) ~ 1, 
           family = binomial(&amp;quot;logit&amp;quot;), # family = binomial(&amp;quot;identity&amp;quot;) would be more straightforward
           data = dat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the logit scale, survival is estimated at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coef(mle) # logit scale
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) 
##  -0.6931472
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After back-transformation using the reciprocal logit, we obtain:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plogis(coef(mle))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) 
##   0.3333333
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far, so good.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-analysis-with-jags&#34;&gt;Bayesian analysis with &lt;code&gt;Jags&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;Jags&lt;/code&gt;, you can fit this model with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# model
betabin &amp;lt;- function(){
  alive ~ dbin(survival, total) # binomial likelihood
  logit(survival) &amp;lt;- beta # logit link
  beta ~ dnorm(0, 1/1.5) # prior
}
# data
datax &amp;lt;- list(total = 57,
              alive = 19)
# initial values
inits &amp;lt;- function() list(beta = rnorm(1, 0, sd = 1.5))
# parameter to monitor
params &amp;lt;- c(&amp;quot;survival&amp;quot;)
# run jags
bayes.jags &amp;lt;- jags(data = datax,
                      inits = inits,
                      parameters.to.save = params,
                      model.file = betabin,
                      n.chains = 2,
                      n.iter = 5000,
                      n.burnin = 1000,
                      n.thin = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 1
##    Unobserved stochastic nodes: 1
##    Total graph size: 8
## 
## Initializing model
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# display results
bayes.jags
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Bugs model at &amp;quot;/var/folders/r7/j0wqj1k95vz8w44sdxzm986c0000gn/T//RtmpP7MZ1A/modela81d72dd23e2.txt&amp;quot;, fit using jags,
##  2 chains, each with 5000 iterations (first 1000 discarded)
##  n.sims = 8000 iterations saved
##          mu.vect sd.vect  2.5%   25%   50%   75% 97.5%  Rhat n.eff
## survival   0.342   0.061 0.228 0.300 0.341 0.382 0.466 1.001  2600
## deviance   5.354   1.390 4.388 4.486 4.819 5.662 9.333 1.001  8000
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 1.0 and DIC = 6.3
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;bayesian-analysis-with-brms&#34;&gt;Bayesian analysis with &lt;code&gt;brms&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;brms&lt;/code&gt;, you write:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bayes.brms &amp;lt;- brm(alive | trials(total) ~ 1, 
                  family = binomial(&amp;quot;logit&amp;quot;), # binomial(&amp;quot;identity&amp;quot;) would be more straightforward
                  data = dat,
                  chains = 2, # nb of chains
                  iter = 5000, # nb of iterations, including burnin
                  warmup = 1000, # burnin
                  thin = 1) # thinning
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&amp;quot;/Library/Frameworks/R.framework/Resources/include&amp;quot; -DNDEBUG   -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/Rcpp/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/unsupported&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/BH/include&amp;quot; -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/src/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppParallel/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/rstan/include&amp;quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:88:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &amp;lt;complex&amp;gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1
## 
## SAMPLING FOR MODEL &#39;782790eabbd10296f779d85fa03cf1e5&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 1: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 1: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 1: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.010358 seconds (Warm-up)
## Chain 1:                0.046464 seconds (Sampling)
## Chain 1:                0.056822 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;782790eabbd10296f779d85fa03cf1e5&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.3e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 2: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 2: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.009763 seconds (Warm-up)
## Chain 2:                0.035949 seconds (Sampling)
## Chain 2:                0.045712 seconds (Total)
## Chain 2:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can display the results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bayes.brms
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: alive | trials(total) ~ 1 
##    Data: dat (Number of observations: 1) 
##   Draws: 2 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup draws = 8000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.69      0.28    -1.27    -0.14 1.00     3213     3572
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And visualize the posterior density and trace of survival (on the logit scale):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(bayes.brms)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/glmm-brms/unnamed-chunk-9-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;To get survival on the $[0,1]$ scale, we extract the MCMC values, then apply the reciprocal logit function to each of these values, and summarize its posterior distribution:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;draws_fit &amp;lt;- as_draws_matrix(bayes.brms)
draws_fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A draws_matrix: 4000 iterations, 2 chains, and 2 variables
##     variable
## draw b_Intercept lp__
##   1        -0.70 -4.2
##   2        -0.67 -4.2
##   3        -0.86 -4.4
##   4        -0.80 -4.3
##   5        -1.32 -6.6
##   6        -0.57 -4.2
##   7        -0.51 -4.4
##   8        -0.56 -4.3
##   9        -0.69 -4.2
##   10       -0.41 -4.6
## # ... with 7990 more draws
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summarize_draws(plogis(draws_fit[,1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
  &lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;variable&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;mean&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;median&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;sd&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;mad&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;q5&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;q95&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;rhat&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;ess_bulk&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;ess_tail&#34;],&#34;name&#34;:[10],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;b_Intercept&#34;,&#34;2&#34;:&#34;0.3372479&#34;,&#34;3&#34;:&#34;0.3346894&#34;,&#34;4&#34;:&#34;0.06186131&#34;,&#34;5&#34;:&#34;0.06164302&#34;,&#34;6&#34;:&#34;0.2388592&#34;,&#34;7&#34;:&#34;0.4422435&#34;,&#34;8&#34;:&#34;1.001294&#34;,&#34;9&#34;:&#34;3212.932&#34;,&#34;10&#34;:&#34;3571.757&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;What is the prior used by default?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;prior_summary(bayes.brms)
&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
  &lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;prior&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;class&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;coef&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;group&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;resp&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;dpar&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;nlpar&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;bound&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;source&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;student_t(3, 0, 2.5)&#34;,&#34;2&#34;:&#34;Intercept&#34;,&#34;3&#34;:&#34;&#34;,&#34;4&#34;:&#34;&#34;,&#34;5&#34;:&#34;&#34;,&#34;6&#34;:&#34;&#34;,&#34;7&#34;:&#34;&#34;,&#34;8&#34;:&#34;&#34;,&#34;9&#34;:&#34;default&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;What if I want to use the same prior as in &lt;code&gt;Jags&lt;/code&gt; instead?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nlprior &amp;lt;- prior(normal(0, 1.5), class = &amp;quot;Intercept&amp;quot;) # new prior
bayes.brms &amp;lt;- brm(alive | trials(total) ~ 1, 
                  family = binomial(&amp;quot;logit&amp;quot;), 
                  data = dat, 
                  prior = nlprior, # set prior by hand
                  chains = 2, # nb of chains
                  iter = 5000, # nb of iterations, including burnin
                  warmup = 1000, # burnin
                  thin = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&amp;quot;/Library/Frameworks/R.framework/Resources/include&amp;quot; -DNDEBUG   -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/Rcpp/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/unsupported&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/BH/include&amp;quot; -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/src/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppParallel/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/rstan/include&amp;quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:88:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &amp;lt;complex&amp;gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1
## 
## SAMPLING FOR MODEL &#39;59dcc499a2b4c8a1b2c8f4e80e70fcce&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.8e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 1: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 1: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 1: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.00875 seconds (Warm-up)
## Chain 1:                0.03345 seconds (Sampling)
## Chain 1:                0.0422 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;59dcc499a2b4c8a1b2c8f4e80e70fcce&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 5e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 2: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 2: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.009788 seconds (Warm-up)
## Chain 2:                0.036797 seconds (Sampling)
## Chain 2:                0.046585 seconds (Total)
## Chain 2:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Double-check the prior that was used:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;prior_summary(bayes.brms)
&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
  &lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;prior&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;class&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;coef&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;group&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;resp&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;dpar&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;nlpar&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;bound&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;source&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;normal(0, 1.5)&#34;,&#34;2&#34;:&#34;Intercept&#34;,&#34;3&#34;:&#34;&#34;,&#34;4&#34;:&#34;&#34;,&#34;5&#34;:&#34;&#34;,&#34;6&#34;:&#34;&#34;,&#34;7&#34;:&#34;&#34;,&#34;8&#34;:&#34;&#34;,&#34;9&#34;:&#34;user&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;h1 id=&#34;logistic-regression-with-covariates&#34;&gt;Logistic regression with covariates&lt;/h1&gt;
&lt;p&gt;In this example, we ask whether annual variation in white stork breeding success can be explained by rainfall in the wintering area. Breeding success is measured by the ratio of the number of chicks over the number of pairs.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nbchicks &amp;lt;- c(151,105,73,107,113,87,77,108,118,122,112,120,122,89,69,71,53,41,53,31,35,14,18)
nbpairs &amp;lt;- c(173,164,103,113,122,112,98,121,132,136,133,137,145,117,90,80,67,54,58,39,42,23,23)
rain &amp;lt;- c(67,52,88,61,32,36,72,43,92,32,86,28,57,55,66,26,28,96,48,90,86,78,87)
dat &amp;lt;- data.frame(nbchicks = nbchicks, 
                  nbpairs = nbpairs,
                  rain = (rain - mean(rain))/sd(rain)) # standardized rainfall
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are grouped.&lt;/p&gt;
&lt;h2 id=&#34;maximum-likelihood-estimation-1&#34;&gt;Maximum-likelihood estimation&lt;/h2&gt;
&lt;p&gt;You can get maximum likelihood estimates with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mle &amp;lt;- glm(cbind(nbchicks, nbpairs - nbchicks) ~ rain, 
           family = binomial(&amp;quot;logit&amp;quot;), 
           data = dat)
summary(mle)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = cbind(nbchicks, nbpairs - nbchicks) ~ rain, family = binomial(&amp;quot;logit&amp;quot;), 
##     data = dat)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -5.9630  -1.3354   0.3929   1.6168   3.8800  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)  1.55436    0.05565  27.931   &amp;lt;2e-16 ***
## rain        -0.14857    0.05993  -2.479   0.0132 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 109.25  on 22  degrees of freedom
## Residual deviance: 103.11  on 21  degrees of freedom
## AIC: 205.84
## 
## Number of Fisher Scoring iterations: 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a negative effect of rainfall on breeding success:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;visreg::visreg(mle, scale = &amp;quot;response&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/glmm-brms/unnamed-chunk-16-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;h2 id=&#34;bayesian-analysis-with-jags-1&#34;&gt;Bayesian analysis with &lt;code&gt;Jags&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;In Jags, we can fit the same model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# data 
dat &amp;lt;- list(N = 23, # nb of years
            nbchicks = nbchicks, 
            nbpairs = nbpairs,
            rain = (rain - mean(rain))/sd(rain)) # standardized rainfall

# define model
logistic &amp;lt;- function() {
  # likelihood
  for(i in 1:N){ # loop over years
    nbchicks[i] ~ dbin(p[i], nbpairs[i]) # binomial likelihood
    logit(p[i]) &amp;lt;- a + b.rain * rain[i] # prob success is linear fn of rainfall on logit scale
  }
  # priors
  a ~ dnorm(0,0.01) # intercept
  b.rain ~ dnorm(0,0.01) # slope for rainfall
}

# function that generates initial values 
inits &amp;lt;- function() list(a = rnorm(n = 1, mean = 0, sd = 5), 
                         b.rain = rnorm(n = 1, mean = 0, sd = 5))

# specify parameters that need to be estimated
parameters &amp;lt;- c(&amp;quot;a&amp;quot;, &amp;quot;b.rain&amp;quot;)

# run Jags
bayes.jags &amp;lt;- jags(data = dat,
               inits = inits,
               parameters.to.save = parameters,
               model.file = logistic, 
               n.chains = 2,
               n.iter = 5000, # includes burn-in!
               n.burnin = 1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 23
##    Unobserved stochastic nodes: 2
##    Total graph size: 134
## 
## Initializing model
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# display results
bayes.jags
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Bugs model at &amp;quot;/var/folders/r7/j0wqj1k95vz8w44sdxzm986c0000gn/T//RtmpP7MZ1A/modela81d1b01bd3f.txt&amp;quot;, fit using jags,
##  2 chains, each with 5000 iterations (first 1000 discarded), n.thin = 4
##  n.sims = 2000 iterations saved
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## a          1.546   0.163   1.439   1.514   1.552   1.588   1.657 1.182  2000
## b.rain    -0.152   0.121  -0.275  -0.190  -0.148  -0.106  -0.033 1.129  2000
## deviance 213.612 330.910 201.899 202.434 203.197 204.689 209.169 1.121  2000
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 54753.9 and DIC = 54967.5
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;bayesian-analysis-with-brms-1&#34;&gt;Bayesian analysis with &lt;code&gt;brms&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;With &lt;code&gt;brms&lt;/code&gt;, we write:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bayes.brms &amp;lt;- brm(nbchicks | trials(nbpairs) ~ rain, 
                  family = binomial(&amp;quot;logit&amp;quot;), 
                  data = dat,
                  chains = 2, # nb of chains
                  iter = 5000, # nb of iterations, including burnin
                  warmup = 1000, # burnin
                  thin = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&amp;quot;/Library/Frameworks/R.framework/Resources/include&amp;quot; -DNDEBUG   -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/Rcpp/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/unsupported&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/BH/include&amp;quot; -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/src/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppParallel/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/rstan/include&amp;quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:88:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &amp;lt;complex&amp;gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1
## 
## SAMPLING FOR MODEL &#39;70a68f0c53365768853c2791038e6352&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 4.7e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.47 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 1: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 1: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 1: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.035213 seconds (Warm-up)
## Chain 1:                0.131663 seconds (Sampling)
## Chain 1:                0.166876 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;70a68f0c53365768853c2791038e6352&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.4e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 2: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 2: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.033715 seconds (Warm-up)
## Chain 2:                0.15264 seconds (Sampling)
## Chain 2:                0.186355 seconds (Total)
## Chain 2:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Display results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bayes.brms
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: nbchicks | trials(nbpairs) ~ rain 
##    Data: dat (Number of observations: 23) 
##   Draws: 2 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup draws = 8000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.56      0.06     1.45     1.67 1.00     6907     5043
## rain         -0.15      0.06    -0.27    -0.03 1.00     7193     5067
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(bayes.brms)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/glmm-brms/unnamed-chunk-20-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;We can also calculate the posterior probability of the rainfall effect being below zero with the &lt;code&gt;hypothesis()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hypothesis(bayes.brms, &#39;rain &amp;lt; 0&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hypothesis Tests for class b:
##   Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star
## 1 (rain) &amp;lt; 0    -0.15      0.06    -0.25    -0.05     116.65      0.99    *
## ---
## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%;
## for two-sided hypotheses, the value tested against lies outside the 95%-CI.
## Posterior probabilities of point hypotheses assume equal prior probabilities.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These results confirm the important negative effect of rainfall on breeding success.&lt;/p&gt;
&lt;h1 id=&#34;linear-mixed-model&#34;&gt;Linear mixed model&lt;/h1&gt;
&lt;p&gt;In this example, we have several measurements for 33 Mediterranean plant species, specifically number of seeds and biomass. We ask whether there is a linear relationship between these two variables that would hold for all species.&lt;/p&gt;
&lt;p&gt;Read in data, directly from the course website:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- read_csv2(&amp;quot;https://raw.githubusercontent.com/oliviergimenez/bayesian-stats-with-R/master/slides/dat/VMG.csv&amp;quot;)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
  &lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Sp&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;NGrTotest&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Vm&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;5.220000e+02&#34;,&#34;3&#34;:&#34;2.7402&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;9.280000e+02&#34;,&#34;3&#34;:&#34;5.7107&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;2.349000e+03&#34;,&#34;3&#34;:&#34;3.7874&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;1.261500e+04&#34;,&#34;3&#34;:&#34;4.208&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;9.135000e+03&#34;,&#34;3&#34;:&#34;4.6764&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;1.189000e+03&#34;,&#34;3&#34;:&#34;4.5658&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;9.280000e+02&#34;,&#34;3&#34;:&#34;5.4989&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;1.232500e+04&#34;,&#34;3&#34;:&#34;6.0688&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;2.044500e+04&#34;,&#34;3&#34;:&#34;5.2587&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;1.145500e+04&#34;,&#34;3&#34;:&#34;5.4009&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;7.395000e+03&#34;,&#34;3&#34;:&#34;2.4289&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;1.812500e+04&#34;,&#34;3&#34;:&#34;5.7908&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;1.116500e+04&#34;,&#34;3&#34;:&#34;4.3584&#34;},{&#34;1&#34;:&#34;AGREUP&#34;,&#34;2&#34;:&#34;8.410000e+02&#34;,&#34;3&#34;:&#34;3.0194&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;9.888138e+06&#34;,&#34;3&#34;:&#34;0.2589&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;7.378453e+14&#34;,&#34;3&#34;:&#34;0.2509&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;5.370114e+14&#34;,&#34;3&#34;:&#34;0.1953&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;8.516877e+14&#34;,&#34;3&#34;:&#34;0.072&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;1.983254e+14&#34;,&#34;3&#34;:&#34;0.0922&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;1.026845e+07&#34;,&#34;3&#34;:&#34;0.0848&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;3.905707e+14&#34;,&#34;3&#34;:&#34;0.0502&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;2.842153e+14&#34;,&#34;3&#34;:&#34;0.03&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;7.697224e+14&#34;,&#34;3&#34;:&#34;0.0929&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;1.269531e+14&#34;,&#34;3&#34;:&#34;0.0288&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;4.860176e+14&#34;,&#34;3&#34;:&#34;0.0539&#34;},{&#34;1&#34;:&#34;ARESER&#34;,&#34;2&#34;:&#34;3.122590e+13&#34;,&#34;3&#34;:&#34;0.0432&#34;},{&#34;1&#34;:&#34;ARIROT&#34;,&#34;2&#34;:&#34;7.740000e+02&#34;,&#34;3&#34;:&#34;2.5837&#34;},{&#34;1&#34;:&#34;ARIROT&#34;,&#34;2&#34;:&#34;7.740000e+02&#34;,&#34;3&#34;:&#34;2.9822&#34;},{&#34;1&#34;:&#34;ARIROT&#34;,&#34;2&#34;:&#34;3.870000e+02&#34;,&#34;3&#34;:&#34;4.8398&#34;},{&#34;1&#34;:&#34;ARIROT&#34;,&#34;2&#34;:&#34;7.740000e+02&#34;,&#34;3&#34;:&#34;3.8271&#34;},{&#34;1&#34;:&#34;ARIROT&#34;,&#34;2&#34;:&#34;1.548000e+03&#34;,&#34;3&#34;:&#34;3.2125&#34;},{&#34;1&#34;:&#34;ARIROT&#34;,&#34;2&#34;:&#34;1.548000e+03&#34;,&#34;3&#34;:&#34;4.0537&#34;},{&#34;1&#34;:&#34;ARIROT&#34;,&#34;2&#34;:&#34;7.740000e+02&#34;,&#34;3&#34;:&#34;3.7995&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;1.885159e+06&#34;,&#34;3&#34;:&#34;0.7485&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;4.497265e+06&#34;,&#34;3&#34;:&#34;0.3794&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;2.679216e+07&#34;,&#34;3&#34;:&#34;0.9408&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;1.534996e+06&#34;,&#34;3&#34;:&#34;1.0311&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;1.396726e+09&#34;,&#34;3&#34;:&#34;6.2405&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;1.540947e+07&#34;,&#34;3&#34;:&#34;6.3843&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;1.851852e+08&#34;,&#34;3&#34;:&#34;0.8972&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;1.545312e+08&#34;,&#34;3&#34;:&#34;1.3303&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;1.908618e+08&#34;,&#34;3&#34;:&#34;12.0858&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;5.312807e+08&#34;,&#34;3&#34;:&#34;1.8016&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;3.087444e+08&#34;,&#34;3&#34;:&#34;3.8879&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;7.085170e+05&#34;,&#34;3&#34;:&#34;0.4081&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;9.622968e+07&#34;,&#34;3&#34;:&#34;0.4865&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;1.956194e+08&#34;,&#34;3&#34;:&#34;1.0913&#34;},{&#34;1&#34;:&#34;AVEBAR&#34;,&#34;2&#34;:&#34;6.710838e+07&#34;,&#34;3&#34;:&#34;0.3691&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;2.500000e+01&#34;,&#34;3&#34;:&#34;1.4411&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;8.000000e+00&#34;,&#34;3&#34;:&#34;3.3895&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;1.200000e+01&#34;,&#34;3&#34;:&#34;8.7321&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;3.200000e+01&#34;,&#34;3&#34;:&#34;0.7551&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;2.700000e+01&#34;,&#34;3&#34;:&#34;2.84&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;1.800000e+01&#34;,&#34;3&#34;:&#34;1.7812&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;4.000000e+00&#34;,&#34;3&#34;:&#34;2.2782&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;7.000000e+00&#34;,&#34;3&#34;:&#34;3.0333&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;2.700000e+01&#34;,&#34;3&#34;:&#34;4.5069&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;3.000000e+00&#34;,&#34;3&#34;:&#34;5.2177&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;1.000000e+00&#34;,&#34;3&#34;:&#34;2.6946&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;1.500000e+01&#34;,&#34;3&#34;:&#34;2.9896&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;1.500000e+01&#34;,&#34;3&#34;:&#34;2.6708&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;5.000000e+00&#34;,&#34;3&#34;:&#34;7.183&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;4.000000e+00&#34;,&#34;3&#34;:&#34;2.3428&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;7.000000e+00&#34;,&#34;3&#34;:&#34;1.0896&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;5.000000e+00&#34;,&#34;3&#34;:&#34;2.6775&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;1.000000e+01&#34;,&#34;3&#34;:&#34;1.7357&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;5.000000e+00&#34;,&#34;3&#34;:&#34;1.3361&#34;},{&#34;1&#34;:&#34;BRAPHO&#34;,&#34;2&#34;:&#34;1.400000e+01&#34;,&#34;3&#34;:&#34;2.0915&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;3.000000e+00&#34;,&#34;3&#34;:&#34;1.4552&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;2.300000e+01&#34;,&#34;3&#34;:&#34;7.0985&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;1.000000e+00&#34;,&#34;3&#34;:&#34;1.8038&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;1.100000e+01&#34;,&#34;3&#34;:&#34;1.165&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;6.800000e+01&#34;,&#34;3&#34;:&#34;3.2737&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;1.800000e+01&#34;,&#34;3&#34;:&#34;1.5305&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;9.000000e+00&#34;,&#34;3&#34;:&#34;2.674&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;1.300000e+01&#34;,&#34;3&#34;:&#34;1.4999&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;1.000000e+00&#34;,&#34;3&#34;:&#34;1.3962&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;1.100000e+01&#34;,&#34;3&#34;:&#34;2.6229&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;7.000000e+00&#34;,&#34;3&#34;:&#34;2.2209&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;1.400000e+01&#34;,&#34;3&#34;:&#34;1.8808&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;1.700000e+01&#34;,&#34;3&#34;:&#34;1.972&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;3.000000e+00&#34;,&#34;3&#34;:&#34;4.6314&#34;},{&#34;1&#34;:&#34;BROERE&#34;,&#34;2&#34;:&#34;1.300000e+01&#34;,&#34;3&#34;:&#34;3.9903&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;8.000000e+00&#34;,&#34;3&#34;:&#34;0.0287&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;1.000000e+01&#34;,&#34;3&#34;:&#34;0.0408&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;2.100000e+01&#34;,&#34;3&#34;:&#34;0.0573&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;2.000000e+01&#34;,&#34;3&#34;:&#34;0.0653&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;1.600000e+01&#34;,&#34;3&#34;:&#34;0.0557&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;1.600000e+01&#34;,&#34;3&#34;:&#34;0.0448&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;2.000000e+01&#34;,&#34;3&#34;:&#34;0.0577&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;2.500000e+01&#34;,&#34;3&#34;:&#34;0.0811&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;3.700000e+01&#34;,&#34;3&#34;:&#34;0.2731&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;1.800000e+01&#34;,&#34;3&#34;:&#34;0.0804&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;3.800000e+01&#34;,&#34;3&#34;:&#34;0.1597&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;8.000000e+00&#34;,&#34;3&#34;:&#34;0.0508&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;5.000000e+00&#34;,&#34;3&#34;:&#34;0.0227&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;1.700000e+01&#34;,&#34;3&#34;:&#34;0.041&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;5.000000e+00&#34;,&#34;3&#34;:&#34;0.0585&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;7.185522e+06&#34;,&#34;3&#34;:&#34;0.3702&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;5.950135e+07&#34;,&#34;3&#34;:&#34;0.3718&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;1.400000e+01&#34;,&#34;3&#34;:&#34;0.094&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;5.000000e+01&#34;,&#34;3&#34;:&#34;0.2454&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;7.000000e+00&#34;,&#34;3&#34;:&#34;0.0322&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;1.100000e+01&#34;,&#34;3&#34;:&#34;0.035&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;2.700000e+01&#34;,&#34;3&#34;:&#34;0.1002&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;1.000000e+01&#34;,&#34;3&#34;:&#34;0.1121&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;4.800000e+01&#34;,&#34;3&#34;:&#34;0.1931&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;3.100000e+01&#34;,&#34;3&#34;:&#34;0.0797&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;5.900000e+01&#34;,&#34;3&#34;:&#34;0.1937&#34;},{&#34;1&#34;:&#34;BROMAD&#34;,&#34;2&#34;:&#34;2.700000e+01&#34;,&#34;3&#34;:&#34;0.0921&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;1.943000e+03&#34;,&#34;3&#34;:&#34;0.6494&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;1.453900e+04&#34;,&#34;3&#34;:&#34;3.5915&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;3.316500e+04&#34;,&#34;3&#34;:&#34;3.8162&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;2.010000e+02&#34;,&#34;3&#34;:&#34;1.3129&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;2.043500e+04&#34;,&#34;3&#34;:&#34;0.8645&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;2.546000e+03&#34;,&#34;3&#34;:&#34;5.0374&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;6.867500e+04&#34;,&#34;3&#34;:&#34;1.3146&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;2.110500e+04&#34;,&#34;3&#34;:&#34;1.4455&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;2.613000e+03&#34;,&#34;3&#34;:&#34;1.5333&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;2.211000e+03&#34;,&#34;3&#34;:&#34;1.879&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;1.641500e+04&#34;,&#34;3&#34;:&#34;1.2037&#34;},{&#34;1&#34;:&#34;CALNEP&#34;,&#34;2&#34;:&#34;1.340000e+02&#34;,&#34;3&#34;:&#34;3.0969&#34;},{&#34;1&#34;:&#34;CENASP&#34;,&#34;2&#34;:&#34;1.055600e+04&#34;,&#34;3&#34;:&#34;1.9068&#34;},{&#34;1&#34;:&#34;CENASP&#34;,&#34;2&#34;:&#34;1.206400e+04&#34;,&#34;3&#34;:&#34;4.2394&#34;},{&#34;1&#34;:&#34;CENASP&#34;,&#34;2&#34;:&#34;1.809600e+04&#34;,&#34;3&#34;:&#34;3.7426&#34;},{&#34;1&#34;:&#34;CENASP&#34;,&#34;2&#34;:&#34;1.658800e+04&#34;,&#34;3&#34;:&#34;2.0106&#34;},{&#34;1&#34;:&#34;CENASP&#34;,&#34;2&#34;:&#34;5.956600e+04&#34;,&#34;3&#34;:&#34;11.937&#34;},{&#34;1&#34;:&#34;CENASP&#34;,&#34;2&#34;:&#34;2.789800e+04&#34;,&#34;3&#34;:&#34;6.6217&#34;},{&#34;1&#34;:&#34;CENASP&#34;,&#34;2&#34;:&#34;1.809600e+04&#34;,&#34;3&#34;:&#34;3.1996&#34;},{&#34;1&#34;:&#34;CENASP&#34;,&#34;2&#34;:&#34;1.131000e+03&#34;,&#34;3&#34;:&#34;2.3526&#34;},{&#34;1&#34;:&#34;CENASP&#34;,&#34;2&#34;:&#34;1.055600e+04&#34;,&#34;3&#34;:&#34;1.3443&#34;},{&#34;1&#34;:&#34;CENASP&#34;,&#34;2&#34;:&#34;1.206400e+04&#34;,&#34;3&#34;:&#34;3.2106&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;6.560000e+02&#34;,&#34;3&#34;:&#34;0.8748&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;2.296000e+03&#34;,&#34;3&#34;:&#34;1.0244&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;6.560000e+02&#34;,&#34;3&#34;:&#34;0.4904&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;9.840000e+02&#34;,&#34;3&#34;:&#34;0.4121&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;3.280000e+02&#34;,&#34;3&#34;:&#34;0.4221&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;1.968000e+03&#34;,&#34;3&#34;:&#34;1.5497&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;2.624000e+03&#34;,&#34;3&#34;:&#34;0.4686&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;3.936000e+03&#34;,&#34;3&#34;:&#34;0.5614&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;4.920000e+02&#34;,&#34;3&#34;:&#34;0.7569&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;5.904000e+03&#34;,&#34;3&#34;:&#34;1.4327&#34;},{&#34;1&#34;:&#34;CONARV&#34;,&#34;2&#34;:&#34;1.312000e+03&#34;,&#34;3&#34;:&#34;0.1692&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.098020e+05&#34;,&#34;3&#34;:&#34;1.823&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.771000e+04&#34;,&#34;3&#34;:&#34;3.965&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.931160e+05&#34;,&#34;3&#34;:&#34;2.929&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.016400e+04&#34;,&#34;3&#34;:&#34;1.879&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;2.882880e+05&#34;,&#34;3&#34;:&#34;4.749&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;2.858240e+05&#34;,&#34;3&#34;:&#34;5.477&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.182720e+05&#34;,&#34;3&#34;:&#34;1.893&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;6.375600e+04&#34;,&#34;3&#34;:&#34;0.605&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;7.068600e+04&#34;,&#34;3&#34;:&#34;0.003&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;8.870400e+04&#34;,&#34;3&#34;:&#34;1.795&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.787940e+05&#34;,&#34;3&#34;:&#34;3.167&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.067220e+05&#34;,&#34;3&#34;:&#34;1.739&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.321320e+05&#34;,&#34;3&#34;:&#34;2.691&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.466080e+05&#34;,&#34;3&#34;:&#34;2.201&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.016400e+04&#34;,&#34;3&#34;:&#34;0.941&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.111110e+05&#34;,&#34;3&#34;:&#34;2.1331&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;3.172785e+06&#34;,&#34;3&#34;:&#34;3.5198&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;1.552320e+05&#34;,&#34;3&#34;:&#34;2.7188&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;3.094245e+06&#34;,&#34;3&#34;:&#34;4.9485&#34;},{&#34;1&#34;:&#34;CONCAN&#34;,&#34;2&#34;:&#34;9.378600e+04&#34;,&#34;3&#34;:&#34;0.9582&#34;},{&#34;1&#34;:&#34;CONSUM&#34;,&#34;2&#34;:&#34;2.793000e+04&#34;,&#34;3&#34;:&#34;7.884&#34;},{&#34;1&#34;:&#34;CONSUM&#34;,&#34;2&#34;:&#34;1.764000e+04&#34;,&#34;3&#34;:&#34;5.827&#34;},{&#34;1&#34;:&#34;CONSUM&#34;,&#34;2&#34;:&#34;1.849260e+05&#34;,&#34;3&#34;:&#34;5.64&#34;},{&#34;1&#34;:&#34;CONSUM&#34;,&#34;2&#34;:&#34;6.585600e+04&#34;,&#34;3&#34;:&#34;14.242&#34;},{&#34;1&#34;:&#34;CONSUM&#34;,&#34;2&#34;:&#34;3.372180e+05&#34;,&#34;3&#34;:&#34;11.624&#34;},{&#34;1&#34;:&#34;CONSUM&#34;,&#34;2&#34;:&#34;6.043050e+11&#34;,&#34;3&#34;:&#34;11.0929&#34;},{&#34;1&#34;:&#34;CONSUM&#34;,&#34;2&#34;:&#34;1.360787e+11&#34;,&#34;3&#34;:&#34;4.6419&#34;},{&#34;1&#34;:&#34;CONSUM&#34;,&#34;2&#34;:&#34;4.655741e+09&#34;,&#34;3&#34;:&#34;9.3029&#34;},{&#34;1&#34;:&#34;CONSUM&#34;,&#34;2&#34;:&#34;2.747341e+11&#34;,&#34;3&#34;:&#34;3.7676&#34;},{&#34;1&#34;:&#34;CONSUM&#34;,&#34;2&#34;:&#34;3.394929e+11&#34;,&#34;3&#34;:&#34;11.7619&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;5.450000e+02&#34;,&#34;3&#34;:&#34;0.8151&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;6.195000e+03&#34;,&#34;3&#34;:&#34;1.2244&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;7.615000e+03&#34;,&#34;3&#34;:&#34;0.9312&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;5.022500e+04&#34;,&#34;3&#34;:&#34;0.5408&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;1.051750e+05&#34;,&#34;3&#34;:&#34;1.5945&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;6.385000e+03&#34;,&#34;3&#34;:&#34;0.8848&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;6.295000e+03&#34;,&#34;3&#34;:&#34;1.0231&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;6.675000e+03&#34;,&#34;3&#34;:&#34;0.6402&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;2.257500e+04&#34;,&#34;3&#34;:&#34;0.4692&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;1.497000e+03&#34;,&#34;3&#34;:&#34;2.4955&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;5.145000e+03&#34;,&#34;3&#34;:&#34;0.8226&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;4.280000e+02&#34;,&#34;3&#34;:&#34;0.43&#34;},{&#34;1&#34;:&#34;CREFOE&#34;,&#34;2&#34;:&#34;6.872500e+04&#34;,&#34;3&#34;:&#34;0.4259&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;6.944858e+14&#34;,&#34;3&#34;:&#34;0.5576&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;2.137188e+14&#34;,&#34;3&#34;:&#34;0.9056&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;1.628894e+14&#34;,&#34;3&#34;:&#34;1.1958&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;9.478765e+14&#34;,&#34;3&#34;:&#34;0.3839&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;3.042291e+14&#34;,&#34;3&#34;:&#34;0.5038&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;1.214020e+14&#34;,&#34;3&#34;:&#34;0.4434&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;3.871472e+14&#34;,&#34;3&#34;:&#34;0.7369&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;1.455768e+14&#34;,&#34;3&#34;:&#34;0.6764&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;4.066404e+14&#34;,&#34;3&#34;:&#34;0.8645&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;1.658677e+14&#34;,&#34;3&#34;:&#34;0.6768&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;9.252856e+14&#34;,&#34;3&#34;:&#34;0.6439&#34;},{&#34;1&#34;:&#34;CYNDAC&#34;,&#34;2&#34;:&#34;7.542087e+13&#34;,&#34;3&#34;:&#34;0.465&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;1.245469e+14&#34;,&#34;3&#34;:&#34;2.822&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;1.917928e+14&#34;,&#34;3&#34;:&#34;5.925&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;8.404472e+14&#34;,&#34;3&#34;:&#34;4.442&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;1.962699e+14&#34;,&#34;3&#34;:&#34;1.322&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;6.064351e+14&#34;,&#34;3&#34;:&#34;1.607&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;4.313892e+14&#34;,&#34;3&#34;:&#34;5.113&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;4.129153e+14&#34;,&#34;3&#34;:&#34;3.258&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;4.774426e+14&#34;,&#34;3&#34;:&#34;2.265&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;1.312217e+13&#34;,&#34;3&#34;:&#34;4.858&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;9.394479e+14&#34;,&#34;3&#34;:&#34;3.5832&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;7.719279e+14&#34;,&#34;3&#34;:&#34;5.422&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;1.296418e+13&#34;,&#34;3&#34;:&#34;3.696&#34;},{&#34;1&#34;:&#34;DACGLO&#34;,&#34;2&#34;:&#34;3.604338e+14&#34;,&#34;3&#34;:&#34;3.893&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;6.974874e+14&#34;,&#34;3&#34;:&#34;2.7065&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;2.789950e+14&#34;,&#34;3&#34;:&#34;7.2954&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;6.277387e+14&#34;,&#34;3&#34;:&#34;6.7922&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;6.974874e+14&#34;,&#34;3&#34;:&#34;5.8217&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;5.579899e+14&#34;,&#34;3&#34;:&#34;5.2203&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;4.882412e+14&#34;,&#34;3&#34;:&#34;4.2592&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;7.672361e+14&#34;,&#34;3&#34;:&#34;3.485&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;3.487437e+14&#34;,&#34;3&#34;:&#34;3.4059&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;4.882412e+14&#34;,&#34;3&#34;:&#34;5.6538&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;2.789950e+14&#34;,&#34;3&#34;:&#34;1.6279&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;4.184924e+13&#34;,&#34;3&#34;:&#34;6.3016&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;2.789950e+14&#34;,&#34;3&#34;:&#34;2.2578&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;1.325226e+14&#34;,&#34;3&#34;:&#34;13.8315&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;2.789950e+14&#34;,&#34;3&#34;:&#34;2.5385&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;4.184924e+13&#34;,&#34;3&#34;:&#34;2.4443&#34;},{&#34;1&#34;:&#34;DAUCAR&#34;,&#34;2&#34;:&#34;2.092462e+14&#34;,&#34;3&#34;:&#34;0.6998&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;5.991644e+13&#34;,&#34;3&#34;:&#34;9.26&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;9.300481e+14&#34;,&#34;3&#34;:&#34;7.963&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;4.492366e+13&#34;,&#34;3&#34;:&#34;7.141&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;2.405171e+14&#34;,&#34;3&#34;:&#34;23.364&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;2.030262e+14&#34;,&#34;3&#34;:&#34;24.003&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;1.657848e+14&#34;,&#34;3&#34;:&#34;14.728&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;7.631866e+14&#34;,&#34;3&#34;:&#34;8.506&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;7.541823e+14&#34;,&#34;3&#34;:&#34;7.253&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;8.924796e+14&#34;,&#34;3&#34;:&#34;12.1349&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;4.966303e+14&#34;,&#34;3&#34;:&#34;38.73&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;3.788564e+14&#34;,&#34;3&#34;:&#34;29.417&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;5.835986e+14&#34;,&#34;3&#34;:&#34;31.51&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;3.260063e+14&#34;,&#34;3&#34;:&#34;4.94&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;3.170862e+14&#34;,&#34;3&#34;:&#34;24.603&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;1.079704e+14&#34;,&#34;3&#34;:&#34;9.922&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;1.039471e+14&#34;,&#34;3&#34;:&#34;9.677&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;3.254672e+14&#34;,&#34;3&#34;:&#34;30.433&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;7.365023e+14&#34;,&#34;3&#34;:&#34;12.038&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;3.825576e+14&#34;,&#34;3&#34;:&#34;29.728&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;2.083637e+14&#34;,&#34;3&#34;:&#34;23.141&#34;},{&#34;1&#34;:&#34;DIPFUL&#34;,&#34;2&#34;:&#34;2.314471e+14&#34;,&#34;3&#34;:&#34;25.903&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;3.600000e+02&#34;,&#34;3&#34;:&#34;2.2693&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;1.035000e+03&#34;,&#34;3&#34;:&#34;13.3441&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;2.400000e+02&#34;,&#34;3&#34;:&#34;2.4296&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;7.300000e+02&#34;,&#34;3&#34;:&#34;8.7793&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;3.150000e+02&#34;,&#34;3&#34;:&#34;3.4946&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;2.000000e+02&#34;,&#34;3&#34;:&#34;1.8299&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;3.350000e+02&#34;,&#34;3&#34;:&#34;4.4712&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;8.200000e+02&#34;,&#34;3&#34;:&#34;8.0342&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;2.800000e+02&#34;,&#34;3&#34;:&#34;2.8258&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;2.650000e+02&#34;,&#34;3&#34;:&#34;1.7212&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;7.050000e+02&#34;,&#34;3&#34;:&#34;5.7427&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;2.050000e+02&#34;,&#34;3&#34;:&#34;1.3254&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;1.750000e+02&#34;,&#34;3&#34;:&#34;0.8842&#34;},{&#34;1&#34;:&#34;EROCIC&#34;,&#34;2&#34;:&#34;1.350000e+02&#34;,&#34;3&#34;:&#34;0.8806&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;3.090000e+02&#34;,&#34;3&#34;:&#34;0.6574&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;1.140000e+02&#34;,&#34;3&#34;:&#34;0.1645&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;9.700000e+01&#34;,&#34;3&#34;:&#34;0.1489&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;1.910000e+02&#34;,&#34;3&#34;:&#34;0.3903&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;2.790000e+02&#34;,&#34;3&#34;:&#34;0.4476&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;3.160000e+02&#34;,&#34;3&#34;:&#34;0.5542&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;2.120000e+02&#34;,&#34;3&#34;:&#34;0.311&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;1.180000e+02&#34;,&#34;3&#34;:&#34;0.1774&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;2.530000e+02&#34;,&#34;3&#34;:&#34;0.3745&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;1.540000e+02&#34;,&#34;3&#34;:&#34;0.1993&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;1.360000e+02&#34;,&#34;3&#34;:&#34;0.268&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;2.490000e+02&#34;,&#34;3&#34;:&#34;0.5365&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;3.800000e+02&#34;,&#34;3&#34;:&#34;0.6972&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;4.750000e+02&#34;,&#34;3&#34;:&#34;0.838&#34;},{&#34;1&#34;:&#34;GERROT&#34;,&#34;2&#34;:&#34;1.760000e+02&#34;,&#34;3&#34;:&#34;0.3191&#34;},{&#34;1&#34;:&#34;INUCON&#34;,&#34;2&#34;:&#34;2.547462e+10&#34;,&#34;3&#34;:&#34;3.4002&#34;},{&#34;1&#34;:&#34;INUCON&#34;,&#34;2&#34;:&#34;8.855462e+10&#34;,&#34;3&#34;:&#34;6.9655&#34;},{&#34;1&#34;:&#34;INUCON&#34;,&#34;2&#34;:&#34;3.093346e+11&#34;,&#34;3&#34;:&#34;3.1757&#34;},{&#34;1&#34;:&#34;INUCON&#34;,&#34;2&#34;:&#34;2.972038e+11&#34;,&#34;3&#34;:&#34;1.5498&#34;},{&#34;1&#34;:&#34;INUCON&#34;,&#34;2&#34;:&#34;2.122885e+11&#34;,&#34;3&#34;:&#34;13.2155&#34;},{&#34;1&#34;:&#34;INUCON&#34;,&#34;2&#34;:&#34;4.912962e+11&#34;,&#34;3&#34;:&#34;5.215&#34;},{&#34;1&#34;:&#34;INUCON&#34;,&#34;2&#34;:&#34;6.429308e+10&#34;,&#34;3&#34;:&#34;6.7267&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;1.370700e+04&#34;,&#34;3&#34;:&#34;0.1732&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;1.370700e+04&#34;,&#34;3&#34;:&#34;0.2786&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;7.234250e+05&#34;,&#34;3&#34;:&#34;1.197&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;2.132200e+04&#34;,&#34;3&#34;:&#34;0.2314&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;1.751450e+05&#34;,&#34;3&#34;:&#34;0.2886&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;7.081950e+05&#34;,&#34;3&#34;:&#34;0.8304&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;6.548900e+04&#34;,&#34;3&#34;:&#34;0.7924&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;3.274450e+05&#34;,&#34;3&#34;:&#34;0.4729&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;3.046000e+03&#34;,&#34;3&#34;:&#34;0.6055&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;2.056050e+05&#34;,&#34;3&#34;:&#34;0.2231&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;6.092000e+03&#34;,&#34;3&#34;:&#34;0.8283&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;4.949750e+05&#34;,&#34;3&#34;:&#34;0.5897&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;7.691150e+05&#34;,&#34;3&#34;:&#34;1.3697&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;3.046000e+03&#34;,&#34;3&#34;:&#34;0.2796&#34;},{&#34;1&#34;:&#34;LOLITA&#34;,&#34;2&#34;:&#34;6.320450e+05&#34;,&#34;3&#34;:&#34;1.3046&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;5.534226e+14&#34;,&#34;3&#34;:&#34;0.2427&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;1.555585e+14&#34;,&#34;3&#34;:&#34;0.0834&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;2.729915e+14&#34;,&#34;3&#34;:&#34;0.1296&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;8.481451e+14&#34;,&#34;3&#34;:&#34;0.4678&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;1.676752e+14&#34;,&#34;3&#34;:&#34;0.8808&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;4.266047e+14&#34;,&#34;3&#34;:&#34;0.1615&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;1.016326e+14&#34;,&#34;3&#34;:&#34;0.4961&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;1.843326e+14&#34;,&#34;3&#34;:&#34;0.1084&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;7.252031e+14&#34;,&#34;3&#34;:&#34;0.0753&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;1.272668e+13&#34;,&#34;3&#34;:&#34;0.0558&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;1.843326e+14&#34;,&#34;3&#34;:&#34;0.0815&#34;},{&#34;1&#34;:&#34;MEDLUP&#34;,&#34;2&#34;:&#34;2.135262e+14&#34;,&#34;3&#34;:&#34;0.0751&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;4.004000e+03&#34;,&#34;3&#34;:&#34;0.6305&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;5.324000e+03&#34;,&#34;3&#34;:&#34;0.7074&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;1.188000e+03&#34;,&#34;3&#34;:&#34;0.1144&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;3.520000e+02&#34;,&#34;3&#34;:&#34;0.029&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;2.552000e+03&#34;,&#34;3&#34;:&#34;0.2778&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;2.464000e+03&#34;,&#34;3&#34;:&#34;0.2053&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;6.160000e+02&#34;,&#34;3&#34;:&#34;0.063&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;1.232000e+03&#34;,&#34;3&#34;:&#34;0.184&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;8.360000e+02&#34;,&#34;3&#34;:&#34;0.1834&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;2.200000e+01&#34;,&#34;3&#34;:&#34;0.0484&#34;},{&#34;1&#34;:&#34;MEDMIN&#34;,&#34;2&#34;:&#34;1.408000e+03&#34;,&#34;3&#34;:&#34;0.3134&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;2.097000e+03&#34;,&#34;3&#34;:&#34;0.3644&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;2.923000e+03&#34;,&#34;3&#34;:&#34;0.4078&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;4.555000e+03&#34;,&#34;3&#34;:&#34;0.945&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;6.168000e+03&#34;,&#34;3&#34;:&#34;1.2278&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;2.800000e+01&#34;,&#34;3&#34;:&#34;0.6491&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;1.001000e+03&#34;,&#34;3&#34;:&#34;1.3631&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;5.400000e+01&#34;,&#34;3&#34;:&#34;1.285&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;3.610000e+02&#34;,&#34;3&#34;:&#34;0.6316&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;1.700000e+01&#34;,&#34;3&#34;:&#34;0.2334&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;3.323000e+03&#34;,&#34;3&#34;:&#34;0.3953&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;1.900000e+01&#34;,&#34;3&#34;:&#34;0.3508&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;2.000000e+01&#34;,&#34;3&#34;:&#34;0.4501&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;8.000000e+00&#34;,&#34;3&#34;:&#34;0.2085&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;2.600000e+01&#34;,&#34;3&#34;:&#34;0.6552&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;2.155000e+03&#34;,&#34;3&#34;:&#34;0.2435&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;5.633000e+03&#34;,&#34;3&#34;:&#34;1.0596&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;1.881000e+03&#34;,&#34;3&#34;:&#34;0.3286&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;3.052000e+03&#34;,&#34;3&#34;:&#34;0.5003&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;1.323100e+04&#34;,&#34;3&#34;:&#34;2.2946&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;2.126000e+03&#34;,&#34;3&#34;:&#34;0.3741&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;3.181000e+03&#34;,&#34;3&#34;:&#34;0.5459&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;2.600000e+01&#34;,&#34;3&#34;:&#34;0.5727&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;2.900000e+01&#34;,&#34;3&#34;:&#34;0.7562&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;1.284000e+03&#34;,&#34;3&#34;:&#34;0.2016&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;1.104900e+04&#34;,&#34;3&#34;:&#34;2.2006&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;7.607000e+03&#34;,&#34;3&#34;:&#34;0.8084&#34;},{&#34;1&#34;:&#34;ORLGRA&#34;,&#34;2&#34;:&#34;5.481000e+03&#34;,&#34;3&#34;:&#34;1.0416&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;3.590440e+05&#34;,&#34;3&#34;:&#34;3.6551&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;8.719640e+05&#34;,&#34;3&#34;:&#34;4.5974&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;7.693800e+04&#34;,&#34;3&#34;:&#34;2.8561&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;1.025840e+05&#34;,&#34;3&#34;:&#34;5.3615&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;8.206720e+05&#34;,&#34;3&#34;:&#34;4.5136&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;1.538760e+05&#34;,&#34;3&#34;:&#34;3.5334&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;1.077132e+06&#34;,&#34;3&#34;:&#34;5.2522&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;3.590440e+05&#34;,&#34;3&#34;:&#34;3.051&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;1.333592e+06&#34;,&#34;3&#34;:&#34;6.1755&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;1.179716e+06&#34;,&#34;3&#34;:&#34;9.8372&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;8.206720e+05&#34;,&#34;3&#34;:&#34;2.902&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;3.077520e+05&#34;,&#34;3&#34;:&#34;4.0246&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;1.025840e+05&#34;,&#34;3&#34;:&#34;7.6592&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;1.538760e+05&#34;,&#34;3&#34;:&#34;1.9749&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;1.846512e+06&#34;,&#34;3&#34;:&#34;7.5188&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;1.025840e+05&#34;,&#34;3&#34;:&#34;4.7883&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;9.232560e+05&#34;,&#34;3&#34;:&#34;3.688&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;4.616280e+05&#34;,&#34;3&#34;:&#34;2.4499&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;4.616280e+05&#34;,&#34;3&#34;:&#34;4.4144&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;2.102972e+06&#34;,&#34;3&#34;:&#34;9.5132&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;4.616280e+05&#34;,&#34;3&#34;:&#34;3.6414&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;2.051680e+05&#34;,&#34;3&#34;:&#34;3.081&#34;},{&#34;1&#34;:&#34;PICHIE&#34;,&#34;2&#34;:&#34;1.692636e+06&#34;,&#34;3&#34;:&#34;10.6821&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;1.000000e+01&#34;,&#34;3&#34;:&#34;0.2655&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;2.000000e+01&#34;,&#34;3&#34;:&#34;0.8077&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;6.044100e+04&#34;,&#34;3&#34;:&#34;0.95&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;3.244100e+04&#34;,&#34;3&#34;:&#34;0.2544&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;2.100000e+01&#34;,&#34;3&#34;:&#34;0.8371&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;2.744100e+04&#34;,&#34;3&#34;:&#34;0.5838&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;6.600000e+01&#34;,&#34;3&#34;:&#34;1.2204&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;2.011124e+14&#34;,&#34;3&#34;:&#34;3.7847&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;3.959632e+14&#34;,&#34;3&#34;:&#34;1.3835&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;5.244100e+04&#34;,&#34;3&#34;:&#34;0.5912&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;3.444100e+04&#34;,&#34;3&#34;:&#34;0.3044&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;3.744100e+04&#34;,&#34;3&#34;:&#34;0.8692&#34;},{&#34;1&#34;:&#34;SANMIN&#34;,&#34;2&#34;:&#34;1.100000e+01&#34;,&#34;3&#34;:&#34;0.6444&#34;},{&#34;1&#34;:&#34;SEDNIC&#34;,&#34;2&#34;:&#34;3.764507e+14&#34;,&#34;3&#34;:&#34;2.3502&#34;},{&#34;1&#34;:&#34;SEDNIC&#34;,&#34;2&#34;:&#34;4.897700e+04&#34;,&#34;3&#34;:&#34;0.3377&#34;},{&#34;1&#34;:&#34;SEDNIC&#34;,&#34;2&#34;:&#34;8.354900e+04&#34;,&#34;3&#34;:&#34;0.5203&#34;},{&#34;1&#34;:&#34;SEDNIC&#34;,&#34;2&#34;:&#34;2.160750e+05&#34;,&#34;3&#34;:&#34;0.139&#34;},{&#34;1&#34;:&#34;SEDNIC&#34;,&#34;2&#34;:&#34;5.473900e+04&#34;,&#34;3&#34;:&#34;0.8127&#34;},{&#34;1&#34;:&#34;SEDNIC&#34;,&#34;2&#34;:&#34;8.354900e+04&#34;,&#34;3&#34;:&#34;0.5362&#34;},{&#34;1&#34;:&#34;SEDNIC&#34;,&#34;2&#34;:&#34;5.617950e+05&#34;,&#34;3&#34;:&#34;0.4823&#34;},{&#34;1&#34;:&#34;SEDNIC&#34;,&#34;2&#34;:&#34;3.457200e+04&#34;,&#34;3&#34;:&#34;0.2996&#34;},{&#34;1&#34;:&#34;SEDNIC&#34;,&#34;2&#34;:&#34;4.177450e+05&#34;,&#34;3&#34;:&#34;0.195&#34;},{&#34;1&#34;:&#34;SEDNIC&#34;,&#34;2&#34;:&#34;5.906050e+05&#34;,&#34;3&#34;:&#34;0.4522&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;9.459805e+14&#34;,&#34;3&#34;:&#34;0.8427&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;5.424562e+14&#34;,&#34;3&#34;:&#34;0.4478&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;1.338922e+14&#34;,&#34;3&#34;:&#34;0.5202&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;1.447315e+14&#34;,&#34;3&#34;:&#34;1.0882&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;3.501219e+14&#34;,&#34;3&#34;:&#34;1.6174&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;1.733331e+14&#34;,&#34;3&#34;:&#34;1.678&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;2.042334e+14&#34;,&#34;3&#34;:&#34;1.2695&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;1.447315e+14&#34;,&#34;3&#34;:&#34;1.2443&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;9.459805e+14&#34;,&#34;3&#34;:&#34;1.3019&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;1.338922e+14&#34;,&#34;3&#34;:&#34;0.1205&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;2.042334e+14&#34;,&#34;3&#34;:&#34;2.19&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;5.511294e+14&#34;,&#34;3&#34;:&#34;0.1373&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;5.511294e+14&#34;,&#34;3&#34;:&#34;0.1748&#34;},{&#34;1&#34;:&#34;TORJAP&#34;,&#34;2&#34;:&#34;3.103686e+14&#34;,&#34;3&#34;:&#34;1.3899&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;3.200000e+02&#34;,&#34;3&#34;:&#34;1.4483&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;5.600000e+02&#34;,&#34;3&#34;:&#34;1.8853&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;3.200000e+02&#34;,&#34;3&#34;:&#34;1.1343&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;1.600000e+02&#34;,&#34;3&#34;:&#34;1.1512&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;8.000000e+02&#34;,&#34;3&#34;:&#34;5.5253&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;4.800000e+02&#34;,&#34;3&#34;:&#34;3.1542&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;8.000000e+01&#34;,&#34;3&#34;:&#34;0.2084&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;3.200000e+02&#34;,&#34;3&#34;:&#34;2.2014&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;9.600000e+02&#34;,&#34;3&#34;:&#34;4.3042&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;8.800000e+02&#34;,&#34;3&#34;:&#34;8.7397&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;1.280000e+03&#34;,&#34;3&#34;:&#34;7.2383&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;2.400000e+02&#34;,&#34;3&#34;:&#34;1.3229&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;4.800000e+02&#34;,&#34;3&#34;:&#34;1.8528&#34;},{&#34;1&#34;:&#34;TORMAX&#34;,&#34;2&#34;:&#34;4.000000e+02&#34;,&#34;3&#34;:&#34;2.9635&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;3.849552e+06&#34;,&#34;3&#34;:&#34;0.6519&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.347343e+07&#34;,&#34;3&#34;:&#34;0.8889&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.058627e+07&#34;,&#34;3&#34;:&#34;0.7121&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.828537e+07&#34;,&#34;3&#34;:&#34;1.3168&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;2.694686e+07&#34;,&#34;3&#34;:&#34;2.1945&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;5.774328e+06&#34;,&#34;3&#34;:&#34;0.3389&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.828537e+07&#34;,&#34;3&#34;:&#34;1.866&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.443582e+06&#34;,&#34;3&#34;:&#34;1.2153&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;4.811940e+05&#34;,&#34;3&#34;:&#34;2.8673&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;2.309731e+07&#34;,&#34;3&#34;:&#34;1.5864&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.347343e+07&#34;,&#34;3&#34;:&#34;1.1809&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;2.887164e+06&#34;,&#34;3&#34;:&#34;0.1957&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.154866e+07&#34;,&#34;3&#34;:&#34;0.8642&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.636060e+07&#34;,&#34;3&#34;:&#34;1.0314&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.443582e+06&#34;,&#34;3&#34;:&#34;1.2785&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;2.790925e+07&#34;,&#34;3&#34;:&#34;1.8897&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.251104e+07&#34;,&#34;3&#34;:&#34;0.5774&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;5.774328e+06&#34;,&#34;3&#34;:&#34;0.5708&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;1.347343e+07&#34;,&#34;3&#34;:&#34;0.7933&#34;},{&#34;1&#34;:&#34;TRIANG&#34;,&#34;2&#34;:&#34;2.598448e+07&#34;,&#34;3&#34;:&#34;1.9167&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;1.822800e+04&#34;,&#34;3&#34;:&#34;0.0665&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;1.380120e+05&#34;,&#34;3&#34;:&#34;0.68153&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;5.294800e+04&#34;,&#34;3&#34;:&#34;0.1755&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;4.860800e+04&#34;,&#34;3&#34;:&#34;0.3186&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;1.649200e+04&#34;,&#34;3&#34;:&#34;0.0568&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;1.388800e+04&#34;,&#34;3&#34;:&#34;0.0475&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;4.340000e+02&#34;,&#34;3&#34;:&#34;0.1655&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;7.378000e+03&#34;,&#34;3&#34;:&#34;0.30694&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;2.343600e+04&#34;,&#34;3&#34;:&#34;0.0845&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;7.551600e+04&#34;,&#34;3&#34;:&#34;0.2706&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;1.475600e+04&#34;,&#34;3&#34;:&#34;0.0436&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;2.083200e+04&#34;,&#34;3&#34;:&#34;0.0795&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;1.484280e+05&#34;,&#34;3&#34;:&#34;0.6652&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;1.562400e+04&#34;,&#34;3&#34;:&#34;0.0662&#34;},{&#34;1&#34;:&#34;VERPER&#34;,&#34;2&#34;:&#34;1.562400e+04&#34;,&#34;3&#34;:&#34;0.0506&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;1.152490e+05&#34;,&#34;3&#34;:&#34;0.5658&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;8.000000e+00&#34;,&#34;3&#34;:&#34;0.5338&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;1.352490e+05&#34;,&#34;3&#34;:&#34;0.7828&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;9.000000e+00&#34;,&#34;3&#34;:&#34;0.7538&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;7.000000e+00&#34;,&#34;3&#34;:&#34;0.4347&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;3.100000e+01&#34;,&#34;3&#34;:&#34;2.2225&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;3.557470e+05&#34;,&#34;3&#34;:&#34;2.3745&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;8.524900e+04&#34;,&#34;3&#34;:&#34;0.2368&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;1.100000e+01&#34;,&#34;3&#34;:&#34;0.693&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;6.000000e+00&#34;,&#34;3&#34;:&#34;0.3571&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;1.100000e+01&#34;,&#34;3&#34;:&#34;0.6245&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;9.000000e+00&#34;,&#34;3&#34;:&#34;0.6065&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;6.524900e+04&#34;,&#34;3&#34;:&#34;0.2&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;2.900000e+01&#34;,&#34;3&#34;:&#34;1.5332&#34;},{&#34;1&#34;:&#34;VICHYB&#34;,&#34;2&#34;:&#34;2.652490e+05&#34;,&#34;3&#34;:&#34;1.2056&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;6.000000e+00&#34;,&#34;3&#34;:&#34;0.0721&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;2.700000e+01&#34;,&#34;3&#34;:&#34;0.3702&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;3.072000e+03&#34;,&#34;3&#34;:&#34;0.2286&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;3.272000e+03&#34;,&#34;3&#34;:&#34;0.3512&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;2.358000e+03&#34;,&#34;3&#34;:&#34;0.1392&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;5.516000e+03&#34;,&#34;3&#34;:&#34;0.3732&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;2.400000e+01&#34;,&#34;3&#34;:&#34;0.2208&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;2.086000e+03&#34;,&#34;3&#34;:&#34;0.1915&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;1.600600e+04&#34;,&#34;3&#34;:&#34;1.2108&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;6.072000e+03&#34;,&#34;3&#34;:&#34;0.7572&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;2.600000e+01&#34;,&#34;3&#34;:&#34;0.4069&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;5.986000e+03&#34;,&#34;3&#34;:&#34;0.396&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;2.944000e+03&#34;,&#34;3&#34;:&#34;0.4301&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;3.030000e+02&#34;,&#34;3&#34;:&#34;0.1504&#34;},{&#34;1&#34;:&#34;VICSAT&#34;,&#34;2&#34;:&#34;3.700000e+01&#34;,&#34;3&#34;:&#34;0.528&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Use relevant format for columns species Sp and biomass Vm:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df$Sp &amp;lt;- as_factor(df$Sp)
df$Vm &amp;lt;- as.numeric(df$Vm)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define numeric vector of species, for nested indexing in &lt;code&gt;Jags&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;species &amp;lt;- as.numeric(df$Sp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define response variable, number of seeds:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- log(df$NGrTotest)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Standardize explanatory variable, biomass&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- (df$Vm - mean(df$Vm))/sd(df$Vm)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now build dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat &amp;lt;- data.frame(y = y, x = x, species = species)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;maximum-likelihood-estimation-2&#34;&gt;Maximum-likelihood estimation&lt;/h2&gt;
&lt;p&gt;You can get maximum likelihood estimates for a linear regression of number of seeds on biomass with a species random effect on the intercept (partial pooling):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mle &amp;lt;- lmer(y ~ x + (1 | species), data = dat)
summary(mle)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: y ~ x + (1 | species)
##    Data: dat
## 
## REML criterion at convergence: 2642.5
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.6761 -0.2704 -0.0120  0.2900  7.3163 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  species  (Intercept) 113.146  10.637  
##  Residual               9.373   3.062  
## Number of obs: 488, groups:  species, 33
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  14.5258     1.8574   7.821
## x             0.4779     0.2421   1.974
## 
## Correlation of Fixed Effects:
##   (Intr)
## x 0.002
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;bayesian-analysis-with-jags-2&#34;&gt;Bayesian analysis with &lt;code&gt;Jags&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;Jags&lt;/code&gt;, you would use:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# partial pooling model
partial_pooling &amp;lt;- function(){
  # likelihood
  for(i in 1:n){
    y[i] ~ dnorm(mu[i], tau.y)
    mu[i] &amp;lt;- a[species[i]] + b * x[i]
  }
  for (j in 1:nbspecies){
    a[j] ~ dnorm(mu.a, tau.a)
  }
  # priors
  tau.y &amp;lt;- 1 / (sigma.y * sigma.y)
  sigma.y ~ dunif(0,100)
  mu.a ~ dnorm(0,0.01)
  tau.a &amp;lt;- 1 / (sigma.a * sigma.a)
  sigma.a ~ dunif(0,100)
  b ~ dnorm(0,0.01)
}

# data
mydata &amp;lt;- list(y = y, 
               x = x, 
               n = length(y),
               species = species,
               nbspecies = length(levels(df$Sp)))

# initial values
inits &amp;lt;- function() list(mu.a = rnorm(1,0,5), 
                         sigma.a = runif(0,0,10), 
                         b = rnorm(1,0,5), 
                         sigma.y = runif(0,0,10))

# parameters to monitor
params &amp;lt;- c(&amp;quot;mu.a&amp;quot;, &amp;quot;sigma.a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;sigma.y&amp;quot;)

# call jags to fit model
bayes.jags &amp;lt;- jags(data = mydata,
                   inits = inits,
                   parameters.to.save = params,
                   model.file = partial_pooling,
                   n.chains = 2,
                   n.iter = 5000,
                   n.burnin = 1000,
                   n.thin = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 488
##    Unobserved stochastic nodes: 37
##    Total graph size: 2484
## 
## Initializing model
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bayes.jags
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Bugs model at &amp;quot;/var/folders/r7/j0wqj1k95vz8w44sdxzm986c0000gn/T//RtmpP7MZ1A/modela81d7639dc1c.txt&amp;quot;, fit using jags,
##  2 chains, each with 5000 iterations (first 1000 discarded)
##  n.sims = 8000 iterations saved
##           mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat
## b           0.476   0.244   -0.005    0.314    0.472    0.638    0.964 1.001
## mu.a       14.025   1.916   10.146   12.774   14.009   15.299   17.760 1.001
## sigma.a    11.094   1.465    8.696   10.039   10.934   11.952   14.330 1.001
## sigma.y     3.072   0.101    2.884    3.002    3.069    3.138    3.277 1.001
## deviance 2478.206   8.704 2463.096 2472.022 2477.521 2483.650 2497.107 1.001
##          n.eff
## b         8000
## mu.a      8000
## sigma.a   8000
## sigma.y   8000
## deviance  8000
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 37.9 and DIC = 2516.1
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;bayesian-analysis-with-brms-2&#34;&gt;Bayesian analysis with &lt;code&gt;brms&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;brms&lt;/code&gt;, you write:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bayes.brms &amp;lt;- brm(y ~ x + (1 | species), 
                  data = dat,
                  chains = 2, # nb of chains
                  iter = 5000, # nb of iterations, including burnin
                  warmup = 1000, # burnin
                  thin = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&amp;quot;/Library/Frameworks/R.framework/Resources/include&amp;quot; -DNDEBUG   -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/Rcpp/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/unsupported&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/BH/include&amp;quot; -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/src/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppParallel/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/rstan/include&amp;quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:88:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &amp;lt;complex&amp;gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1
## 
## SAMPLING FOR MODEL &#39;c7dcb37f6d55803a5216866e78a4a476&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000102 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.02 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 1: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 1: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 1: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.656886 seconds (Warm-up)
## Chain 1:                3.37389 seconds (Sampling)
## Chain 1:                4.03077 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;c7dcb37f6d55803a5216866e78a4a476&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 2.7e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 2: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 2: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.593596 seconds (Warm-up)
## Chain 2:                2.34578 seconds (Sampling)
## Chain 2:                2.93937 seconds (Total)
## Chain 2:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Display the results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bayes.brms
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ x + (1 | species) 
##    Data: dat (Number of observations: 488) 
##   Draws: 2 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup draws = 8000
## 
## Group-Level Effects: 
## ~species (Number of levels: 33) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)    10.74      1.39     8.45    13.78 1.00      427      943
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    14.13      1.90    10.31    17.86 1.01      331      580
## x             0.47      0.25    -0.00     0.97 1.00     2317     3382
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     3.07      0.10     2.88     3.28 1.00     2667     3408
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can extract a block of fixed effects:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(bayes.brms)$fixed
&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
  &lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;&#34;],&#34;name&#34;:[&#34;_rn_&#34;],&#34;type&#34;:[&#34;&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;Estimate&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Est.Error&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;l-95% CI&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;u-95% CI&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Rhat&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Bulk_ESS&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Tail_ESS&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;14.1340226&#34;,&#34;2&#34;:&#34;1.9039258&#34;,&#34;3&#34;:&#34;10.31129978&#34;,&#34;4&#34;:&#34;17.864402&#34;,&#34;5&#34;:&#34;1.005515&#34;,&#34;6&#34;:&#34;330.6591&#34;,&#34;7&#34;:&#34;580.1616&#34;,&#34;_rn_&#34;:&#34;Intercept&#34;},{&#34;1&#34;:&#34;0.4747192&#34;,&#34;2&#34;:&#34;0.2477705&#34;,&#34;3&#34;:&#34;-0.00471696&#34;,&#34;4&#34;:&#34;0.968302&#34;,&#34;5&#34;:&#34;1.000108&#34;,&#34;6&#34;:&#34;2317.4168&#34;,&#34;7&#34;:&#34;3382.0628&#34;,&#34;_rn_&#34;:&#34;x&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;And a block of random effects:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(bayes.brms)$random
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $species
##               Estimate Est.Error l-95% CI u-95% CI     Rhat Bulk_ESS Tail_ESS
## sd(Intercept) 10.74273  1.388061 8.454543   13.776 1.001916 426.9568 943.1178
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And visualize:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(bayes.brms)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/glmm-brms/unnamed-chunk-34-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;h1 id=&#34;glmm-with-poisson-distribution&#34;&gt;GLMM with Poisson distribution&lt;/h1&gt;
&lt;p&gt;This example is from Jason Matthiopoulos&amp;rsquo; excellent book 
&lt;a href=&#34;http://greenmaths.st-andrews.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;How to be a quantitative ecologist&lt;/em&gt;&lt;/a&gt;.
A survey of a coral reef uses 10 predefined linear transects covered by divers once every week. The response variable of interest is the abundance of a particular species of anemone as a function of water temperature. Counts of anemones are recorded at 20 regular line segments along the transect.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate some data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(666)
transects &amp;lt;- 10
data &amp;lt;- NULL
for (tr in 1:transects){
  # random effect (intercept)
  ref &amp;lt;- rnorm(1,0,.5) 
  # water temperature gradient
  t &amp;lt;- runif(1, 18,22) + runif(1,-.2,0.2)*1:20 
  # Anemone gradient (expected response)
  ans &amp;lt;- exp(ref -14 + 1.8 * t - 0.045 * t^2) 
  # actual counts on 20 segments of the current transect
  an &amp;lt;- rpois(20, ans) 
  data &amp;lt;- rbind(data, cbind(rep(tr, 20), t, an))
}

data &amp;lt;- data.frame(Transect = data[,1],
                   Temperature = data[,2],
                   Anemones = data[,3])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Standardize temperature:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boo &amp;lt;- data$Temperature
data$Temp &amp;lt;- (boo - mean(boo)) / sd(boo)
head(data)
&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
  &lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;&#34;],&#34;name&#34;:[&#34;_rn_&#34;],&#34;type&#34;:[&#34;&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;Transect&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Temperature&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Anemones&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Temp&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;21.79259&#34;,&#34;3&#34;:&#34;65&#34;,&#34;4&#34;:&#34;0.8852197&#34;,&#34;_rn_&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;21.67312&#34;,&#34;3&#34;:&#34;70&#34;,&#34;4&#34;:&#34;0.8201601&#34;,&#34;_rn_&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;21.55365&#34;,&#34;3&#34;:&#34;65&#34;,&#34;4&#34;:&#34;0.7551005&#34;,&#34;_rn_&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;21.43418&#34;,&#34;3&#34;:&#34;61&#34;,&#34;4&#34;:&#34;0.6900410&#34;,&#34;_rn_&#34;:&#34;4&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;21.31471&#34;,&#34;3&#34;:&#34;81&#34;,&#34;4&#34;:&#34;0.6249814&#34;,&#34;_rn_&#34;:&#34;5&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;21.19524&#34;,&#34;3&#34;:&#34;85&#34;,&#34;4&#34;:&#34;0.5599218&#34;,&#34;_rn_&#34;:&#34;6&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;h2 id=&#34;maximum-likelihood-estimation-3&#34;&gt;Maximum-likelihood estimation&lt;/h2&gt;
&lt;p&gt;With &lt;code&gt;lme4&lt;/code&gt;, you write:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fit_lme4 &amp;lt;- glmer(Anemones ~ Temp + I(Temp^2) + (1 | Transect), 
                  data = data, 
                  family = poisson)
summary(fit_lme4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: poisson  ( log )
## Formula: Anemones ~ Temp + I(Temp^2) + (1 | Transect)
##    Data: data
## 
##      AIC      BIC   logLik deviance df.resid 
##   1380.2   1393.4   -686.1   1372.2      196 
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.32356 -0.66154 -0.03391  0.63964  2.67283 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Transect (Intercept) 0.1497   0.3869  
## Number of obs: 200, groups:  Transect, 10
## 
## Fixed effects:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)  3.95436    0.12368  31.971   &amp;lt;2e-16 ***
## Temp        -0.04608    0.02435  -1.892   0.0584 .  
## I(Temp^2)   -0.11122    0.01560  -7.130    1e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##           (Intr) Temp  
## Temp       0.037       
## I(Temp^2) -0.118 -0.294
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;bayesian-analysis-with-jags-3&#34;&gt;Bayesian analysis with &lt;code&gt;Jags&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;Jags&lt;/code&gt;, we fit the corresponding GLMM with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- function() {
  for (i in 1:n){
    count[i] ~ dpois(lambda[i])
    log(lambda[i]) &amp;lt;- a[transect[i]] + b[1] * x[i] + b[2] * pow(x[i],2)
  }
  for (j in 1:nbtransects){
    a[j] ~ dnorm (mu.a, tau.a)
  }
  mu.a ~ dnorm (0, 0.01)
  tau.a &amp;lt;- pow(sigma.a, -2)
  sigma.a ~ dunif (0, 100)
  b[1] ~ dnorm (0, 0.01)
  b[2] ~ dnorm (0, 0.01)
}

dat &amp;lt;- list(n = nrow(data), 
            nbtransects = transects, 
            x = data$Temp, 
            count = data$Anemones, 
            transect = data$Transect)

inits &amp;lt;- function() list(a = rnorm(transects), 
                         b = rnorm(2), 
                         mu.a = rnorm(1), 
                         sigma.a = runif(1))

par &amp;lt;- c (&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;mu.a&amp;quot;, &amp;quot;sigma.a&amp;quot;)

fit &amp;lt;- jags(data = dat, 
            inits = inits, 
            parameters.to.save = par, 
            model.file = model,
            n.chains = 2, 
            n.iter = 5000, 
            n.burn = 1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 200
##    Unobserved stochastic nodes: 14
##    Total graph size: 1622
## 
## Initializing model
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;round(fit$BUGSoutput$summary[, -c(4,6)], 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean    sd     2.5%      50%    97.5%  Rhat n.eff
## a[1]        4.383 0.026    4.332    4.384    4.435 1.001  2000
## a[2]        3.223 0.050    3.120    3.224    3.322 1.001  2000
## a[3]        3.584 0.058    3.471    3.584    3.696 1.001  2000
## a[4]        3.620 0.054    3.513    3.622    3.721 1.001  2000
## a[5]        4.092 0.050    3.990    4.092    4.190 1.001  2000
## a[6]        3.848 0.038    3.774    3.849    3.921 1.000  2000
## a[7]        4.476 0.029    4.419    4.475    4.531 1.002  1300
## a[8]        4.398 0.055    4.288    4.397    4.499 1.003   550
## a[9]        3.835 0.033    3.771    3.835    3.900 1.001  1700
## a[10]       4.076 0.029    4.018    4.076    4.132 1.001  2000
## b[1]       -0.047 0.024   -0.093   -0.047    0.000 1.001  2000
## b[2]       -0.111 0.016   -0.142   -0.111   -0.079 1.004   450
## deviance 1324.697 4.651 1317.432 1324.185 1335.440 1.001  2000
## mu.a        3.950 0.160    3.617    3.957    4.272 1.005   310
## sigma.a     0.482 0.140    0.296    0.457    0.826 1.003  1000
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;bayesian-analysis-with-brms-3&#34;&gt;Bayesian analysis with &lt;code&gt;brms&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;What about with &lt;code&gt;brms&lt;/code&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bayes.brms &amp;lt;- brm(Anemones ~ Temp + I(Temp^2) + (1 | Transect), 
                  data = data,
                  family = poisson(&amp;quot;log&amp;quot;),
                  chains = 2, # nb of chains
                  iter = 5000, # nb of iterations, including burnin
                  warmup = 1000, # burnin
                  thin = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&amp;quot;/Library/Frameworks/R.framework/Resources/include&amp;quot; -DNDEBUG   -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/Rcpp/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/unsupported&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/BH/include&amp;quot; -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/src/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppParallel/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/rstan/include&amp;quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:88:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &amp;lt;complex&amp;gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1
## 
## SAMPLING FOR MODEL &#39;9326e0aa5a2dca4008c9e5b2ee4e86bf&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 4.5e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 1: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 1: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 1: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.893226 seconds (Warm-up)
## Chain 1:                4.91683 seconds (Sampling)
## Chain 1:                5.81006 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;9326e0aa5a2dca4008c9e5b2ee4e86bf&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 2.4e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 2: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 2: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 1.21769 seconds (Warm-up)
## Chain 2:                4.45281 seconds (Sampling)
## Chain 2:                5.6705 seconds (Total)
## Chain 2:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Display results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bayes.brms
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: Anemones ~ Temp + I(Temp^2) + (1 | Transect) 
##    Data: data (Number of observations: 200) 
##   Draws: 2 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup draws = 8000
## 
## Group-Level Effects: 
## ~Transect (Number of levels: 10) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.47      0.14     0.29     0.81 1.00     1366     2089
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     3.95      0.16     3.63     4.26 1.00     1359     1751
## Temp         -0.05      0.02    -0.09     0.00 1.00     4126     4490
## ITempE2      -0.11      0.02    -0.14    -0.08 1.00     4018     4209
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(bayes.brms)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/glmm-brms/unnamed-chunk-41-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;We can assess the quality of fit of this model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pp_check(bayes.brms, ndraws = 100, type = &#39;ecdf_overlay&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/glmm-brms/unnamed-chunk-42-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;As expected, the fit is almost perfect because we simulated data from this very model.&lt;/p&gt;
&lt;p&gt;What if we&amp;rsquo;d like to test the effect of temperature using WAIC?&lt;/p&gt;
&lt;p&gt;We fit a model with no effect of temperature:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bayes.brms2 &amp;lt;- brm(Anemones ~ 1 + (1 | Transect), 
                  data = data,
                  family = poisson(&amp;quot;log&amp;quot;),
                  chains = 2, # nb of chains
                  iter = 5000, # nb of iterations, including burnin
                  warmup = 1000, # burnin
                  thin = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&amp;quot;/Library/Frameworks/R.framework/Resources/include&amp;quot; -DNDEBUG   -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/Rcpp/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/unsupported&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/BH/include&amp;quot; -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/src/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppParallel/include/&amp;quot;  -I&amp;quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/rstan/include&amp;quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:88:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &amp;lt;built-in&amp;gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Dense:1:
## /Library/Frameworks/R.framework/Versions/4.1/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &amp;lt;complex&amp;gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1
## 
## SAMPLING FOR MODEL &#39;fd9804091450c044661531e89123d1dc&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 4.4e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 1: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 1: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 1: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.704799 seconds (Warm-up)
## Chain 1:                2.58736 seconds (Sampling)
## Chain 1:                3.29216 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;fd9804091450c044661531e89123d1dc&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.8e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 2: Iteration: 1001 / 5000 [ 20%]  (Sampling)
## Chain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)
## Chain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)
## Chain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)
## Chain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 2: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.648008 seconds (Warm-up)
## Chain 2:                2.85026 seconds (Sampling)
## Chain 2:                3.49826 seconds (Total)
## Chain 2:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we compare both models, by ranking them with their WAIC or using ELPD:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(waic1 &amp;lt;- waic(bayes.brms)) # waic model w/ tempterature
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 8000 by 200 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -668.2  9.8
## p_waic        11.0  1.1
## waic        1336.3 19.7
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(waic2 &amp;lt;- waic(bayes.brms2)) # waic model wo/ tempterature
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 8000 by 200 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -702.4 13.3
## p_waic        12.3  1.2
## waic        1404.9 26.5
## 
## 2 (1.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;loo_compare(waic1, waic2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             elpd_diff se_diff
## bayes.brms    0.0       0.0  
## bayes.brms2 -34.3       8.9
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;In all examples, you can check that i) there is little or no difference between &lt;code&gt;Jags&lt;/code&gt; and &lt;code&gt;brms&lt;/code&gt; numeric summaries of posterior distributions, and ii) maximum likelihood parameter estimates (&lt;code&gt;glm()&lt;/code&gt;, &lt;code&gt;lmer()&lt;/code&gt; and &lt;code&gt;glmer()&lt;/code&gt; functions) and posterior means/medians are very close to each other.&lt;/p&gt;
&lt;p&gt;In contrast to Jags, Nimble or Stan, you do not need to code the likelihood yourself in &lt;code&gt;brms&lt;/code&gt;, as long as it is available in the package. You can have a look to a list of distributions with &lt;code&gt;?brms::brmsfamily&lt;/code&gt;, and there is also a possibility to define custom distributions with &lt;code&gt;custom_family()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note I haven&amp;rsquo;t covered diagnostics of convergence, more 
&lt;a href=&#34;https://www.rensvandeschoot.com/brms-wambs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. You can also integrate &lt;code&gt;brms&lt;/code&gt; outputs in a 
&lt;a href=&#34;http://mjskay.github.io/tidybayes/articles/tidy-brms.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tidy workflow with &lt;code&gt;tidybayes&lt;/code&gt;&lt;/a&gt;, and use the &lt;code&gt;ggplot()&lt;/code&gt; magic.&lt;/p&gt;
&lt;p&gt;As usual, the code is on GitHub, visit 
&lt;a href=&#34;https://github.com/oliviergimenez/fit-glmm-with-brms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/oliviergimenez/fit-glmm-with-brms&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now I guess it&amp;rsquo;s up to the students to pick their favorite Bayesian tool.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Introductory workshop on Nimble</title>
      <link>https://oliviergimenez.github.io/blog/2022-04-08-introductory-workshop-on-nimble/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/2022-04-08-introductory-workshop-on-nimble/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üì¢ü•≥ With &lt;a href=&#34;https://twitter.com/MaudQueroue?ref_src=twsrc%5Etfw&#34;&gt;@MaudQueroue&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/ValentinLauret?ref_src=twsrc%5Etfw&#34;&gt;@ValentinLauret&lt;/a&gt; we gave a short introduction to nimble &lt;a href=&#34;https://twitter.com/R_nimble?ref_src=twsrc%5Etfw&#34;&gt;@R_nimble&lt;/a&gt; &lt;br&gt;&lt;br&gt;nimble is a &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; üì¶ to fit models in the Bayesian framework w/ MCMC, it&amp;#39;s also a programming environment for using/coding fns/distns/samplers&lt;br&gt;&lt;br&gt;‚û°Ô∏è Check out &lt;a href=&#34;https://t.co/MuUjGc4U8i&#34;&gt;https://t.co/MuUjGc4U8i&lt;/a&gt; &lt;a href=&#34;https://t.co/RBtu07v874&#34;&gt;pic.twitter.com/RBtu07v874&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1512431317593387008?ref_src=twsrc%5Etfw&#34;&gt;April 8, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt; 
</description>
    </item>
    
    <item>
      <title>New paper on trade-offs between Deep Learning for species id inference on predator-prey co-occurrence</title>
      <link>https://oliviergimenez.github.io/blog/2022-04-06-new-paper-on-trade-offs-between-deep-learning-for-species-id-inference-on-predator-prey-co-occurrence/</link>
      <pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/2022-04-06-new-paper-on-trade-offs-between-deep-learning-for-species-id-inference-on-predator-prey-co-occurrence/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;A pleasure to work w/ &lt;a href=&#34;https://twitter.com/hashtag/ComputoJournal?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ComputoJournal&lt;/a&gt; &lt;a href=&#34;https://twitter.com/Computo85445972?ref_src=twsrc%5Etfw&#34;&gt;@Computo85445972&lt;/a&gt; for our paper on trade-offs bw &lt;a href=&#34;https://twitter.com/hashtag/DeepLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DeepLearning&lt;/a&gt; for species id &amp;amp; inference on predator-prey co-occurrence, which comes w/ a reproducible R workflow üòá&lt;a href=&#34;https://t.co/Tgo6OJs7r0&#34;&gt;https://t.co/Tgo6OJs7r0&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/OpenAccess?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#OpenAccess&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ReproducibleResearch?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ReproducibleResearch&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/RStats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#RStats&lt;/a&gt; &lt;br&gt;&lt;br&gt; üßµ‚¨áÔ∏è &lt;a href=&#34;https://t.co/x2w73ezAcs&#34;&gt;https://t.co/x2w73ezAcs&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1511699911238111236?ref_src=twsrc%5Etfw&#34;&gt;April 6, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt; 
</description>
    </item>
    
    <item>
      <title>Binary image classification using Keras in R: Using CT scans to predict patients with Covid</title>
      <link>https://oliviergimenez.github.io/blog/image-classif/</link>
      <pubDate>Sun, 02 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/image-classif/</guid>
      <description>&lt;p&gt;Here I illustrate how to train a CNN with Keras in R to predict from patients&amp;rsquo; CT scans those who will develop severe illness from Covid.&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Michael Blum 
&lt;a href=&#34;https://twitter.com/mblum_g/status/1475940763716444161?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tweeted&lt;/a&gt; about the 
&lt;a href=&#34;https://stoic2021.grand-challenge.org/stoic2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;STOIC2021 - COVID-19 AI challenge&lt;/a&gt;. The main goal of this challenge is to predict from the patients&amp;rsquo; 
&lt;a href=&#34;https://en.wikipedia.org/wiki/CT_scan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CT scans&lt;/a&gt; who will develop severe illness from Covid.&lt;/p&gt;
&lt;p&gt;Given my 
&lt;a href=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent interest in machine learning&lt;/a&gt;, this challenge peaked my interest. Although &lt;code&gt;Python&lt;/code&gt; is the machine learning &lt;em&gt;lingua franca&lt;/em&gt;, it is possible to 
&lt;a href=&#34;https://github.com/oliviergimenez/computo-deeplearning-occupany-lynx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;train a convolutional neural network (CNN) in &lt;code&gt;R&lt;/code&gt;&lt;/a&gt; and perform (binary) image classification.&lt;/p&gt;
&lt;p&gt;Here, I will use an 
&lt;a href=&#34;https://keras.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;R&lt;/code&gt; interface to &lt;code&gt;Keras&lt;/code&gt;&lt;/a&gt; that allows training neural networks. Note that the 
&lt;a href=&#34;https://stoic2021.grand-challenge.org/stoic-db/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dataset shared for the challenge&lt;/a&gt; is big, like 280Go big, and it took me a day to download it. For the sake of illustration, I will use a similar but much lighter dataset from a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Kaggle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle&lt;/a&gt; repository 
&lt;a href=&#34;https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code is available on GitHub as usual 
&lt;a href=&#34;https://github.com/oliviergimenez/bin-image-classif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/oliviergimenez/bin-image-classif&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First things first, load the packages we will need.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
theme_set(theme_light())
library(keras)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;read-in-and-process-data&#34;&gt;Read in and process data&lt;/h1&gt;
&lt;p&gt;We will need a function to process images, I&amp;rsquo;m stealing 
&lt;a href=&#34;https://rpubs.com/spalladino14/653239&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;that one&lt;/a&gt; written by 
&lt;a href=&#34;https://www.linkedin.com/in/spencer-palladino/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spencer Palladino&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;process_pix &amp;lt;- function(lsf) {
  img &amp;lt;- lapply(lsf, image_load, grayscale = TRUE) # grayscale the image
  arr &amp;lt;- lapply(img, image_to_array) # turns it into an array
  arr_resized &amp;lt;- lapply(arr, image_array_resize, 
                        height = 100, 
                        width = 100) # resize
  arr_normalized &amp;lt;- normalize(arr_resized, axis = 1) #normalize to make small numbers 
  return(arr_normalized)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s process images for patients with Covid, and do some reshaping. Idem with images for patients without Covid.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# with covid
lsf &amp;lt;- list.files(&amp;quot;dat/COVID/&amp;quot;, full.names = TRUE) 
covid &amp;lt;- process_pix(lsf)
covid &amp;lt;- covid[,,,1] # get rid of last dim
covid_reshaped &amp;lt;- array_reshape(covid, c(nrow(covid), 100*100))
# without covid
lsf &amp;lt;- list.files(&amp;quot;dat/non-COVID/&amp;quot;, full.names = TRUE) 
ncovid &amp;lt;- process_pix(lsf)
ncovid &amp;lt;- ncovid[,,,1] # get rid of last dim
ncovid_reshaped &amp;lt;- array_reshape(ncovid, c(nrow(ncovid), 100*100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have 1252 CT scans of patients with Covid, and 1229 without.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s visualise these scans. Let&amp;rsquo;s pick a patient with Covid, and another one without.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;scancovid &amp;lt;- reshape2::melt(covid[10,,])
plotcovid &amp;lt;- scancovid %&amp;gt;%
  ggplot() +
  aes(x = Var1, y = Var2, fill = value) + 
  geom_raster() +
  labs(x = NULL, y = NULL, title = &amp;quot;CT scan of a patient with covid&amp;quot;) + 
  scale_fill_viridis_c() + 
  theme(legend.position = &amp;quot;none&amp;quot;)

scanncovid &amp;lt;- reshape2::melt(ncovid[10,,])
plotncovid &amp;lt;- scanncovid %&amp;gt;%
  ggplot() +
  aes(x = Var1, y = Var2, fill = value) + 
  geom_raster() +
  labs(x = NULL, y = NULL, title = &amp;quot;CT scan of a patient without covid&amp;quot;) + 
  scale_fill_viridis_c() + 
  theme(legend.position = &amp;quot;none&amp;quot;)

library(patchwork)
plotcovid + plotncovid
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/image-classif/unnamed-chunk-4-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Put altogether and shuffle.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- rbind(cbind(covid_reshaped, 1), # 1 = covid
            cbind(ncovid_reshaped, 0)) # 0 = no covid
set.seed(1234)
shuffle &amp;lt;- sample(nrow(df), replace = F)
df &amp;lt;- df[shuffle, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sounds great. We have everything we need to start training a convolutional neural network model.&lt;/p&gt;
&lt;h1 id=&#34;convolutional-neural-network-cnn&#34;&gt;Convolutional neural network (CNN)&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s build our training and testing datasets using a 80/20 split.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(2022)
split &amp;lt;- sample(2, nrow(df), replace = T, prob = c(0.8, 0.2))
train &amp;lt;- df[split == 1,]
test &amp;lt;- df[split == 2,]
train_target &amp;lt;- df[split == 1, 10001] # label in training dataset
test_target &amp;lt;- df[split == 2, 10001] # label in testing dataset
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now build our model. I use three layers (&lt;code&gt;layer_dense()&lt;/code&gt; function) that I put one after the other with piping. I also use regularization (&lt;code&gt;layer_dropout()&lt;/code&gt; function) to avoid overfitting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 512, activation = &amp;quot;relu&amp;quot;) %&amp;gt;% 
  layer_dropout(0.4) %&amp;gt;%
  layer_dense(units = 256, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(0.3) %&amp;gt;%
  layer_dense(units = 128, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(0.2) %&amp;gt;%
  layer_dense(units = 2, activation = &#39;softmax&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model with defaults specific to binary classification.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model %&amp;gt;%
  compile(optimizer = &#39;adam&#39;,
          loss = &#39;binary_crossentropy&#39;, 
          metrics = c(&#39;accuracy&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use one-hot encoding (&lt;code&gt;to_categorical()&lt;/code&gt; function) aka dummy coding in statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train_label &amp;lt;- to_categorical(train_target)
test_label &amp;lt;- to_categorical(test_target)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s fit our model to the training dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fit_covid &amp;lt;- model %&amp;gt;%
  fit(x = train,
      y = train_label, 
      epochs = 25,
      batch_size = 512, # try also 128 and 256
      verbose = 2,
      validation_split = 0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quick visualization of the performances shows that the algorithm is doing not too bad. No over/under-fitting. Accuracy and loss are fine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(fit_covid)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/image-classif/unnamed-chunk-11-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;What about the performances on the testing dataset?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model %&amp;gt;%
  evaluate(test, test_label)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       loss   accuracy 
## 0.02048795 0.99795920
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s do some predictions on the testing dataset, and compare with ground truth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;predictedclasses &amp;lt;- model %&amp;gt;%
  predict_classes(test)
table(Prediction = predictedclasses, 
      Actual = test_target)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Actual
## Prediction   0   1
##          0 243   0
##          1   1 246
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty cool. Only one healthy patient is misclassified as being sick. Let&amp;rsquo;s save our model for further use.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;save_model_tf(model, &amp;quot;model/covidmodel&amp;quot;) # save the model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I&amp;rsquo;m happy with these results. In general however, we need to find ways to improve the performances. Check out some tips 
&lt;a href=&#34;https://machinelearningmastery.com/improve-deep-learning-performance/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; with examples implemented in &lt;code&gt;Keras&lt;/code&gt; with &lt;code&gt;R&lt;/code&gt; 
&lt;a href=&#34;https://keras.rstudio.com/articles/examples/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;there&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Draft chapter on Bayes stats and MCMC in R</title>
      <link>https://oliviergimenez.github.io/blog/draft-bayesmcmc/</link>
      <pubDate>Mon, 20 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/draft-bayesmcmc/</guid>
      <description>&lt;p&gt;I have a draft chapter on Bayes stats and MCMC at 
&lt;a href=&#34;https://oliviergimenez.github.io/banana-book/crashcourse.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://oliviergimenez.github.io/banana-book/crashcourse.html&lt;/a&gt; I&amp;rsquo;d love your feedback about what is confusing and what is missing üòá #rstats&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Bayesian analysis of capture-recapture data with hidden Markov models - Theory and case studies in R</title>
      <link>https://oliviergimenez.github.io/blog/banana-book/</link>
      <pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/banana-book/</guid>
      <description>&lt;p&gt;So, I&amp;rsquo;m writing a book ü§Ø It&amp;rsquo;&#39;&amp;rsquo;s called &amp;ldquo;Bayesian analysis of capture-recapture data with hidden Markov models - Theory and case studies in R&amp;rdquo;.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;So, I&amp;#39;m writing a book ü§Ø It&amp;#39;s called &amp;#39;Bayesian analysis of capture-recapture data w hidden Markov models - Theory and case studies in R&amp;#39;. Online version here &lt;a href=&#34;https://t.co/Crgx5NX70s&#34;&gt;https://t.co/Crgx5NX70s&lt;/a&gt; to be published w &lt;a href=&#34;https://twitter.com/CRCPress?ref_src=twsrc%5Etfw&#34;&gt;@CRCPress&lt;/a&gt; Will share chapter drafts üòá &amp;amp; random thoughts along the way &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://t.co/ms6dIjvY7m&#34;&gt;pic.twitter.com/ms6dIjvY7m&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1439664049776644101?ref_src=twsrc%5Etfw&#34;&gt;September 19, 2021&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;</description>
    </item>
    
    <item>
      <title>Experimenting with machine learning in R with tidymodels and the Kaggle titanic dataset</title>
      <link>https://oliviergimenez.github.io/blog/learning-machine-learning/</link>
      <pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/learning-machine-learning/</guid>
      <description>&lt;p&gt;I would like to familiarize myself with machine learning (ML) techniques in &lt;code&gt;R&lt;/code&gt;. So I have been reading and learning by doing. I thought I&amp;rsquo;d share my experience for others who&amp;rsquo;d like to give it a try&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h1 id=&#34;first-version-august-13-2021-updated-august-23-2021&#34;&gt;First version August 13, 2021, updated August 23, 2021&lt;/h1&gt;
&lt;p&gt;Since my first post, I‚Äôve been reading notebooks shared by folks who
ranked high in the challenge, and added two features that they used.
Eventually, these new predictors did not help (I must be doing something
wrong). I also explored some other ML algorithms. Last, I tuned the
parameters more efficiently with a clever grid-search algorithm. All in
all, I slightly improved my score, but most importantly, I now have a
clean template for further use.&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;All material available from GitHub at

&lt;a href=&#34;https://github.com/oliviergimenez/learning-machine-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/oliviergimenez/learning-machine-learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The two great books I‚Äôm using are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.statlearning.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Statistical Learning with Applications in
R&lt;/a&gt; by Gareth James, Daniela Witten,
Trevor Hastie and Robert Tibshirani&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.tmwr.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tidy models in R&lt;/a&gt; by Max Kuhn and Julia
Silge&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also recommend checking out the material (codes, screencasts) shared
by 
&lt;a href=&#34;http://varianceexplained.org/r/sliced-ml/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Robinson&lt;/a&gt; and

&lt;a href=&#34;https://juliasilge.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia Silge&lt;/a&gt; from whom I picked some useful
tricks that I put to use below.&lt;/p&gt;
&lt;p&gt;To try things, I‚Äôve joined the

&lt;a href=&#34;https://en.wikipedia.org/wiki/Kaggle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle&lt;/a&gt; online community which
gathers folks with lots of experience in ML from whom you can learn.
Kaggle also hosts public datasets that can be used for playing around.&lt;/p&gt;
&lt;p&gt;I use the &lt;code&gt;tidymodels&lt;/code&gt; metapackage that contains a suite of packages for
modeling and machine learning using &lt;code&gt;tidyverse&lt;/code&gt; principles. Check out
all possibilities 
&lt;a href=&#34;https://www.tidymodels.org/find/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, and parsnip
models in particular 
&lt;a href=&#34;https://www.tidymodels.org/find/parsnip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;there&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let‚Äôs start with the famous 
&lt;a href=&#34;https://www.kaggle.com/c/titanic/overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Titanic
dataset&lt;/a&gt;. We need to predict
if a passenger survived the sinking of the Titanic (1) or not (0). A
dataset is provided for training our models (train.csv). Another dataset
is provided (test.csv) for which we do not know the answer. We will
predict survival for each passenger, submit our answer to Kaggle and see
how well we did compared to other folks. The metric for comparison is
the percentage of passengers we correctly predict ‚Äì aka as accuracy.&lt;/p&gt;
&lt;p&gt;First things first, let‚Äôs load some packages to get us started.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tidymodels) # metapackage for ML 
library(tidyverse) # metapackage for data manipulation and visulaisation
library(stacks) # stack ML models for better perfomance
theme_set(theme_light())
doParallel::registerDoParallel(cores = 4) # parallel computations
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;Read in training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rawdata &amp;lt;- read_csv(&amp;quot;dat/titanic/train.csv&amp;quot;)
glimpse(rawdata)

## Rows: 891
## Columns: 12
## $ PassengerId &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20‚Ä¶
## $ Survived    &amp;lt;dbl&amp;gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, ‚Ä¶
## $ Pclass      &amp;lt;dbl&amp;gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, ‚Ä¶
## $ Name        &amp;lt;chr&amp;gt; &amp;quot;Braund, Mr. Owen Harris&amp;quot;, &amp;quot;Cumings, Mrs. John Bradley (Florence Brig‚Ä¶
## $ Sex         &amp;lt;chr&amp;gt; &amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;male&amp;quot;,‚Ä¶
## $ Age         &amp;lt;dbl&amp;gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55, 2, NA, ‚Ä¶
## $ SibSp       &amp;lt;dbl&amp;gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, ‚Ä¶
## $ Parch       &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, ‚Ä¶
## $ Ticket      &amp;lt;chr&amp;gt; &amp;quot;A/5 21171&amp;quot;, &amp;quot;PC 17599&amp;quot;, &amp;quot;STON/O2. 3101282&amp;quot;, &amp;quot;113803&amp;quot;, &amp;quot;373450&amp;quot;, &amp;quot;330‚Ä¶
## $ Fare        &amp;lt;dbl&amp;gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 1‚Ä¶
## $ Cabin       &amp;lt;chr&amp;gt; NA, &amp;quot;C85&amp;quot;, NA, &amp;quot;C123&amp;quot;, NA, NA, &amp;quot;E46&amp;quot;, NA, NA, NA, &amp;quot;G6&amp;quot;, &amp;quot;C103&amp;quot;, NA, N‚Ä¶
## $ Embarked    &amp;lt;chr&amp;gt; &amp;quot;S&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;Q&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;,‚Ä¶

naniar::miss_var_summary(rawdata)

## # A tibble: 12 √ó 3
##    variable    n_miss pct_miss
##    &amp;lt;chr&amp;gt;        &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 Cabin          687   77.1  
##  2 Age            177   19.9  
##  3 Embarked         2    0.224
##  4 PassengerId      0    0    
##  5 Survived         0    0    
##  6 Pclass           0    0    
##  7 Name             0    0    
##  8 Sex              0    0    
##  9 SibSp            0    0    
## 10 Parch            0    0    
## 11 Ticket           0    0    
## 12 Fare             0    0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After some data exploration (not shown), I decided to take care of
missing values, gather the two family variables in a single variable,
and create a variable title.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Get most frequent port of embarkation
uniqx &amp;lt;- unique(na.omit(rawdata$Embarked))
mode_embarked &amp;lt;- as.character(fct_drop(uniqx[which.max(tabulate(match(rawdata$Embarked, uniqx)))]))


# Build function for data cleaning and handling NAs
process_data &amp;lt;- function(tbl){
  
  tbl %&amp;gt;%
    mutate(class = case_when(Pclass == 1 ~ &amp;quot;first&amp;quot;,
                             Pclass == 2 ~ &amp;quot;second&amp;quot;,
                             Pclass == 3 ~ &amp;quot;third&amp;quot;),
           class = as_factor(class),
           gender = factor(Sex),
           fare = Fare,
           age = Age,
           ticket = Ticket,
           alone = if_else(SibSp + Parch == 0, &amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;), # alone variable
           alone = as_factor(alone),
           port = factor(Embarked), # rename embarked as port
           title = str_extract(Name, &amp;quot;[A-Za-z]+\\.&amp;quot;), # title variable
           title = fct_lump(title, 4)) %&amp;gt;% # keep only most frequent levels of title
    mutate(port = ifelse(is.na(port), mode_embarked, port), # deal w/ NAs in port (replace by mode)
           port = as_factor(port)) %&amp;gt;%
    group_by(title) %&amp;gt;%
    mutate(median_age_title = median(age, na.rm = T)) %&amp;gt;%
    ungroup() %&amp;gt;%
    mutate(age = if_else(is.na(age), median_age_title, age)) %&amp;gt;% # deal w/ NAs in age (replace by median in title)
    mutate(ticketfreq = ave(1:nrow(.), FUN = length),
           fareadjusted = fare / ticketfreq) %&amp;gt;%
    mutate(familyage = SibSp + Parch + 1 + age/70)
    
}

# Process the data
dataset &amp;lt;- rawdata %&amp;gt;%
  process_data() %&amp;gt;%
  mutate(survived = as_factor(if_else(Survived == 1, &amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;))) %&amp;gt;%
  mutate(survived = relevel(survived, ref = &amp;quot;yes&amp;quot;)) %&amp;gt;% # first event is survived = yes
  select(survived, class, gender, age, alone, port, title, fareadjusted, familyage) 

# Have a look again
glimpse(dataset)

## Rows: 891
## Columns: 9
## $ survived     &amp;lt;fct&amp;gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, y‚Ä¶
## $ class        &amp;lt;fct&amp;gt; third, first, third, first, third, third, first, third, third, secon‚Ä¶
## $ gender       &amp;lt;fct&amp;gt; male, female, female, female, male, male, male, male, female, female‚Ä¶
## $ age          &amp;lt;dbl&amp;gt; 22, 38, 26, 35, 35, 30, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55, 2, 30,‚Ä¶
## $ alone        &amp;lt;fct&amp;gt; no, no, yes, no, yes, yes, yes, no, no, no, no, yes, yes, no, yes, y‚Ä¶
## $ port         &amp;lt;fct&amp;gt; 3, 1, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3, 3, 2,‚Ä¶
## $ title        &amp;lt;fct&amp;gt; Mr., Mrs., Miss., Mrs., Mr., Mr., Mr., Master., Mrs., Mrs., Miss., M‚Ä¶
## $ fareadjusted &amp;lt;dbl&amp;gt; 0.008136925, 0.080003704, 0.008894501, 0.059595960, 0.009034792, 0.0‚Ä¶
## $ familyage    &amp;lt;dbl&amp;gt; 2.314286, 2.542857, 1.371429, 2.500000, 1.500000, 1.428571, 1.771429‚Ä¶

naniar::miss_var_summary(dataset)

## # A tibble: 9 √ó 3
##   variable     n_miss pct_miss
##   &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 survived          0        0
## 2 class             0        0
## 3 gender            0        0
## 4 age               0        0
## 5 alone             0        0
## 6 port              0        0
## 7 title             0        0
## 8 fareadjusted      0        0
## 9 familyage         0        0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let‚Äôs apply the same treatment to the test dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rawdata &amp;lt;- read_csv(&amp;quot;dat/titanic/test.csv&amp;quot;) 
holdout &amp;lt;- rawdata %&amp;gt;%
  process_data() %&amp;gt;%
  select(PassengerId, class, gender, age, alone, port, title, fareadjusted, familyage) 

glimpse(holdout)

## Rows: 418
## Columns: 9
## $ PassengerId  &amp;lt;dbl&amp;gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905‚Ä¶
## $ class        &amp;lt;fct&amp;gt; third, third, second, third, third, third, third, second, third, thi‚Ä¶
## $ gender       &amp;lt;fct&amp;gt; male, female, male, male, female, male, female, male, female, male, ‚Ä¶
## $ age          &amp;lt;dbl&amp;gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0, 28.5, 46‚Ä¶
## $ alone        &amp;lt;fct&amp;gt; yes, no, yes, yes, no, yes, yes, no, yes, no, yes, yes, no, no, no, ‚Ä¶
## $ port         &amp;lt;fct&amp;gt; 2, 3, 2, 3, 3, 3, 2, 3, 1, 3, 3, 3, 3, 3, 3, 1, 2, 1, 3, 1, 1, 3, 3,‚Ä¶
## $ title        &amp;lt;fct&amp;gt; Mr., Mrs., Mr., Mr., Mrs., Mr., Miss., Mr., Mrs., Mr., Mr., Mr., Mrs‚Ä¶
## $ fareadjusted &amp;lt;dbl&amp;gt; 0.018730144, 0.016746411, 0.023175837, 0.020723684, 0.029395933, 0.0‚Ä¶
## $ familyage    &amp;lt;dbl&amp;gt; 1.492857, 2.671429, 1.885714, 1.385714, 3.314286, 1.200000, 1.428571‚Ä¶

naniar::miss_var_summary(holdout)

## # A tibble: 9 √ó 3
##   variable     n_miss pct_miss
##   &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 fareadjusted      1    0.239
## 2 PassengerId       0    0    
## 3 class             0    0    
## 4 gender            0    0    
## 5 age               0    0    
## 6 alone             0    0    
## 7 port              0    0    
## 8 title             0    0    
## 9 familyage         0    0
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;exploratory-data-analysis&#34;&gt;Exploratory data analysis&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;skimr::skim(dataset)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;Data summary&lt;/caption&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Name&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Number of rows&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;891&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Number of columns&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;_______________________&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Column type frequency:&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;factor&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;numeric&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;________________________&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Group variables&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;None&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Data summary&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Variable type: factor&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;skim_variable&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;n_missing&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;complete_rate&lt;/th&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;ordered&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;n_unique&lt;/th&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;top_counts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;survived&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;no: 549, yes: 342&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;class&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;thi: 491, fir: 216, sec: 184&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;gender&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;mal: 577, fem: 314&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;alone&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;yes: 537, no: 354&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;port&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;3: 644, 1: 168, 2: 77, S: 2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;title&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Mr.: 517, Mis: 182, Mrs: 125, Mas: 40&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: numeric&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;skim_variable&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;n_missing&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;complete_rate&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;mean&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;sd&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;p0&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;p25&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;p50&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;p75&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;p100&lt;/th&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;hist&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;age&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;29.39&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;13.26&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.42&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;21.00&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;30.00&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;35.00&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;80.00&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;‚ñÇ‚ñá‚ñÉ‚ñÅ‚ñÅ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;fareadjusted&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.04&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.06&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.00&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.01&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.02&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.03&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.58&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;familyage&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;2.32&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1.57&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1.07&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1.41&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;1.57&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;2.62&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;11.43&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let‚Äôs explore the data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset %&amp;gt;%
  count(survived)

## # A tibble: 2 √ó 2
##   survived     n
##   &amp;lt;fct&amp;gt;    &amp;lt;int&amp;gt;
## 1 yes        342
## 2 no         549

dataset %&amp;gt;%
  group_by(gender) %&amp;gt;%
  summarize(n = n(),
            n_surv = sum(survived == &amp;quot;yes&amp;quot;),
            pct_surv = n_surv / n)

## # A tibble: 2 √ó 4
##   gender     n n_surv pct_surv
##   &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 female   314    233    0.742
## 2 male     577    109    0.189

dataset %&amp;gt;%
  group_by(title) %&amp;gt;%
  summarize(n = n(),
            n_surv = sum(survived == &amp;quot;yes&amp;quot;),
            pct_surv = n_surv / n) %&amp;gt;%
  arrange(desc(pct_surv))

## # A tibble: 5 √ó 4
##   title       n n_surv pct_surv
##   &amp;lt;fct&amp;gt;   &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 Mrs.      125     99    0.792
## 2 Miss.     182    127    0.698
## 3 Master.    40     23    0.575
## 4 Other      27     12    0.444
## 5 Mr.       517     81    0.157

dataset %&amp;gt;%
  group_by(class, gender) %&amp;gt;%
  summarize(n = n(),
            n_surv = sum(survived == &amp;quot;yes&amp;quot;),
            pct_surv = n_surv / n) %&amp;gt;%
  arrange(desc(pct_surv))

## # A tibble: 6 √ó 5
## # Groups:   class [3]
##   class  gender     n n_surv pct_surv
##   &amp;lt;fct&amp;gt;  &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 first  female    94     91    0.968
## 2 second female    76     70    0.921
## 3 third  female   144     72    0.5  
## 4 first  male     122     45    0.369
## 5 second male     108     17    0.157
## 6 third  male     347     47    0.135
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some informative graphs.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset %&amp;gt;%
  group_by(class, gender) %&amp;gt;%
  summarize(n = n(),
            n_surv = sum(survived == &amp;quot;yes&amp;quot;),
            pct_surv = n_surv / n) %&amp;gt;%
    mutate(class = fct_reorder(class, pct_surv)) %&amp;gt;%
    ggplot(aes(pct_surv, class, fill = class, color = class)) +
    geom_col(position = position_dodge()) +
    scale_x_continuous(labels = percent) +
    labs(x = &amp;quot;% in category that survived&amp;quot;, fill = NULL, color = NULL, y = NULL) +
  facet_wrap(~gender)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-7-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset %&amp;gt;%
  mutate(age = cut(age, breaks = c(0, 20, 40, 60, 80))) %&amp;gt;%
  group_by(age, gender) %&amp;gt;%
  summarize(n = n(),
            n_surv = sum(survived == &amp;quot;yes&amp;quot;),
            pct_surv = n_surv / n) %&amp;gt;%
    mutate(age = fct_reorder(age, pct_surv)) %&amp;gt;%
    ggplot(aes(pct_surv, age, fill = age, color = age)) +
    geom_col(position = position_dodge()) +
    scale_x_continuous(labels = percent) +
    labs(x = &amp;quot;% in category that survived&amp;quot;, fill = NULL, color = NULL, y = NULL) +
  facet_wrap(~gender)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-7-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset %&amp;gt;%
    ggplot(aes(fareadjusted, group = survived, color = survived, fill = survived)) +
    geom_histogram(alpha = .4, position = position_dodge()) +
    labs(x = &amp;quot;fare&amp;quot;, y = NULL, color = &amp;quot;survived?&amp;quot;, fill = &amp;quot;survived?&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-7-3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset %&amp;gt;%
    ggplot(aes(familyage, group = survived, color = survived, fill = survived)) +
    geom_histogram(alpha = .4, position = position_dodge()) +
    labs(x = &amp;quot;family aged&amp;quot;, y = NULL, color = &amp;quot;survived?&amp;quot;, fill = &amp;quot;survived?&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-7-4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;trainingtesting-datasets&#34;&gt;Training/testing datasets&lt;/h1&gt;
&lt;p&gt;Split our dataset in two, one dataset for training and the other one for
testing. We will use an additionnal splitting step for cross-validation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;set.seed(2021)
spl &amp;lt;- initial_split(dataset, strata = &amp;quot;survived&amp;quot;)
train &amp;lt;- training(spl)
test &amp;lt;- testing(spl)

train_5fold &amp;lt;- train %&amp;gt;%
  vfold_cv(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;gradient-boosting-algorithms---xgboost&#34;&gt;Gradient boosting algorithms - xgboost&lt;/h1&gt;
&lt;p&gt;Let‚Äôs start with 
&lt;a href=&#34;https://en.wikipedia.org/wiki/XGBoost&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gradient boosting
methods&lt;/a&gt; which are very popular
in the ML community.&lt;/p&gt;
&lt;h2 id=&#34;tuning&#34;&gt;Tuning&lt;/h2&gt;
&lt;p&gt;Set up defaults.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mset &amp;lt;- metric_set(accuracy) # metric is accuracy
control &amp;lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First a recipe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xg_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then specify a gradient boosting model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xg_model &amp;lt;- boost_tree(mode = &amp;quot;classification&amp;quot;, # binary response
                       trees = tune(),
                       mtry = tune(),
                       tree_depth = tune(),
                       learn_rate = tune(),
                       loss_reduction = tune(),
                       min_n = tune()) # parameters to be tuned
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now set our workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xg_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(xg_model) %&amp;gt;% 
  add_recipe(xg_rec)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use cross-validation to evaluate our model with different param config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xg_tune &amp;lt;- xg_wf %&amp;gt;%
  tune_grid(train_5fold,
            metrics = mset,
            control = control,
            grid = crossing(trees = 1000,
                            mtry = c(3, 5, 8), # finalize(mtry(), train)
                            tree_depth = c(5, 10, 15),
                            learn_rate = c(0.01, 0.005),
                            loss_reduction = c(0.01, 0.1, 1),
                            min_n = c(2, 10, 25)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize the results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(xg_tune) + theme_light()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-14-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Collect metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xg_tune %&amp;gt;%
  collect_metrics() %&amp;gt;%
  arrange(desc(mean))

## # A tibble: 162 √ó 12
##     mtry trees min_n tree_depth learn_rate loss_reduction .metric  .estimator  mean     n
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1     3  1000     2          5       0.01           0.01 accuracy binary     0.849     5
##  2     8  1000     2          5       0.01           0.01 accuracy binary     0.847     5
##  3     8  1000     2          5       0.01           0.1  accuracy binary     0.846     5
##  4     3  1000     2         15       0.01           0.1  accuracy binary     0.844     5
##  5     5  1000     2         10       0.01           1    accuracy binary     0.844     5
##  6     3  1000     2          5       0.01           0.1  accuracy binary     0.844     5
##  7     5  1000     2         10       0.01           0.1  accuracy binary     0.843     5
##  8     3  1000     2         10       0.01           0.1  accuracy binary     0.843     5
##  9     5  1000     2          5       0.01           0.01 accuracy binary     0.843     5
## 10     5  1000     2          5       0.01           0.1  accuracy binary     0.843     5
## # ‚Ä¶ with 152 more rows, and 2 more variables: std_err &amp;lt;dbl&amp;gt;, .config &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tuning takes some time. There are other ways to explore the
parameter space more efficiently. For example, we will use the function

&lt;a href=&#34;https://dials.tidymodels.org/reference/grid_max_entropy.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;dials::grid_max_entropy()&lt;/code&gt;&lt;/a&gt;
in the last section about ensemble modelling. Here, I will use

&lt;a href=&#34;https://search.r-project.org/CRAN/refmans/finetune/html/tune_race_anova.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;finetune::tune_race_anova&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(finetune)
xg_tune &amp;lt;-
  xg_wf %&amp;gt;%
  tune_race_anova(
    train_5fold,
    grid = 50,
    param_info = xg_model %&amp;gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize the results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(xg_tune)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-17-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Collect metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xg_tune %&amp;gt;%
  collect_metrics() %&amp;gt;%
  arrange(desc(mean))

## # A tibble: 50 √ó 12
##     mtry trees min_n tree_depth learn_rate loss_reduction .metric  .estimator  mean     n
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1     6   856     4         13   1.12e- 2       2.49e- 8 accuracy binary     0.837     5
##  2    10  1952     6          7   3.36e- 2       2.07e+ 0 accuracy binary     0.829     5
##  3     3   896     2          5   1.73e- 5       6.97e- 8 accuracy binary     0.826     5
##  4    14  1122     4          6   1.16e- 6       3.44e+ 0 accuracy binary     0.815     4
##  5     7   939     8          4   2.88e- 5       1.50e- 4 accuracy binary     0.813     4
##  6     7    17    10          7   4.54e- 6       6.38e- 3 accuracy binary     0.813     4
##  7     8    92     9         11   3.60e- 3       3.01e-10 accuracy binary     0.811     4
##  8    13  1407    15          4   1.48e- 2       9.68e- 4 accuracy binary     0.807     3
##  9     4   658    11          9   9.97e-10       1.27e- 5 accuracy binary     0.805     3
## 10     2   628    14          9   1.84e- 6       9.44e- 5 accuracy binary     0.798     3
## # ‚Ä¶ with 40 more rows, and 2 more variables: std_err &amp;lt;dbl&amp;gt;, .config &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;fit-model&#34;&gt;Fit model&lt;/h2&gt;
&lt;p&gt;Use best config to fit model to training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xg_fit &amp;lt;- xg_wf %&amp;gt;%
  finalize_workflow(select_best(xg_tune)) %&amp;gt;%
  fit(train)

## [23:39:25] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out accuracy on testing dataset to see if we overfitted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xg_fit %&amp;gt;%
  augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 √ó 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.812
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out important features (aka predictors).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;importances &amp;lt;- xgboost::xgb.importance(model = extract_fit_engine(xg_fit))
importances %&amp;gt;%
  mutate(Feature = fct_reorder(Feature, Gain)) %&amp;gt;%
  ggplot(aes(Gain, Feature)) +
  geom_col()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-21-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;make-predictions&#34;&gt;Make predictions&lt;/h2&gt;
&lt;p&gt;Now we‚Äôre ready to predict survival for the holdout dataset and submit
to Kaggle. Note that I use the whole dataset, not just the training
dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xg_wf %&amp;gt;%
  finalize_workflow(select_best(xg_tune)) %&amp;gt;%
  fit(dataset) %&amp;gt;%
  augment(holdout) %&amp;gt;%
  select(PassengerId, Survived = .pred_class) %&amp;gt;%
  mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
  write_csv(&amp;quot;output/titanic/xgboost.csv&amp;quot;)

## [23:39:28] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I got and accuracy of 0.74162. Cool. Let‚Äôs train a random forest model
now.&lt;/p&gt;
&lt;h1 id=&#34;random-forests&#34;&gt;Random forests&lt;/h1&gt;
&lt;p&gt;Let‚Äôs continue with 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;random forest
methods&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;tuning-1&#34;&gt;Tuning&lt;/h2&gt;
&lt;p&gt;First a recipe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rf_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then specify a random forest model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rf_model &amp;lt;- rand_forest(mode = &amp;quot;classification&amp;quot;, # binary response
                        engine = &amp;quot;ranger&amp;quot;, # by default
                        mtry = tune(),
                        trees = tune(),
                        min_n = tune()) # parameters to be tuned
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now set our workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rf_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(rf_model) %&amp;gt;% 
  add_recipe(rf_rec)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use cross-validation to evaluate our model with different param config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rf_tune &amp;lt;-
  rf_wf %&amp;gt;%
  tune_race_anova(
    train_5fold,
    grid = 50,
    param_info = rf_model %&amp;gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize the results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(rf_tune)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-27-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Collect metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rf_tune %&amp;gt;%
  collect_metrics() %&amp;gt;%
  arrange(desc(mean))

## # A tibble: 50 √ó 9
##     mtry trees min_n .metric  .estimator  mean     n std_err .config              
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                
##  1    14  1554     5 accuracy binary     0.837     5 0.00656 Preprocessor1_Model49
##  2     4   774    18 accuracy binary     0.837     5 0.0133  Preprocessor1_Model50
##  3     5  1736     8 accuracy binary     0.834     5 0.0111  Preprocessor1_Model46
##  4     8  1322     5 accuracy binary     0.832     5 0.00713 Preprocessor1_Model41
##  5     2  1078    30 accuracy binary     0.831     5 0.00727 Preprocessor1_Model12
##  6     4  1892    14 accuracy binary     0.831     5 0.00886 Preprocessor1_Model39
##  7     8   962     7 accuracy binary     0.829     5 0.00742 Preprocessor1_Model43
##  8     7   946     4 accuracy binary     0.826     5 0.00452 Preprocessor1_Model23
##  9     7  1262     3 accuracy binary     0.826     5 0.00710 Preprocessor1_Model47
## 10     9   544     9 accuracy binary     0.825     5 0.00673 Preprocessor1_Model11
## # ‚Ä¶ with 40 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;fit-model-1&#34;&gt;Fit model&lt;/h2&gt;
&lt;p&gt;Use best config to fit model to training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rf_fit &amp;lt;- rf_wf %&amp;gt;%
  finalize_workflow(select_best(rf_tune)) %&amp;gt;%
  fit(train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out accuracy on testing dataset to see if we overfitted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rf_fit %&amp;gt;%
  augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 √ó 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.786
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out important features (aka predictors).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(vip)
finalize_model(
  x = rf_model,
  parameters = select_best(rf_tune)) %&amp;gt;%
  set_engine(&amp;quot;ranger&amp;quot;, importance = &amp;quot;permutation&amp;quot;) %&amp;gt;%
  fit(survived ~ ., data = juice(prep(rf_rec))) %&amp;gt;%
  vip(geom = &amp;quot;point&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-31-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;make-predictions-1&#34;&gt;Make predictions&lt;/h2&gt;
&lt;p&gt;Now we‚Äôre ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rf_wf %&amp;gt;%
  finalize_workflow(select_best(rf_tune)) %&amp;gt;%
  fit(dataset) %&amp;gt;%
  augment(holdout) %&amp;gt;%
  select(PassengerId, Survived = .pred_class) %&amp;gt;%
  mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
  write_csv(&amp;quot;output/titanic/randomforest.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I got and accuracy of 0.77990, a bit better than gradient boosting.&lt;/p&gt;
&lt;p&gt;Let‚Äôs continue with 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Catboost&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cat boosting
methods&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;gradient-boosting-algorithms---catboost&#34;&gt;Gradient boosting algorithms - catboost&lt;/h1&gt;
&lt;h2 id=&#34;tuning-2&#34;&gt;Tuning&lt;/h2&gt;
&lt;p&gt;Set up defaults.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mset &amp;lt;- metric_set(accuracy) # metric is accuracy
control &amp;lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First a recipe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cb_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then specify a cat boosting model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(treesnip)
cb_model &amp;lt;- boost_tree(mode = &amp;quot;classification&amp;quot;,
                       engine = &amp;quot;catboost&amp;quot;,
                       mtry = tune(),
                       trees = tune(),
                       min_n = tune(),
                       tree_depth = tune(),
                       learn_rate = tune()) # parameters to be tuned
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now set our workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cb_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(cb_model) %&amp;gt;% 
  add_recipe(cb_rec)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use cross-validation to evaluate our model with different param config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cb_tune &amp;lt;- cb_wf %&amp;gt;%
  tune_race_anova(
    train_5fold,
    grid = 30,
    param_info = cb_model %&amp;gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize the results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(cb_tune)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-38-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Collect metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cb_tune %&amp;gt;%
  collect_metrics() %&amp;gt;%
  arrange(desc(mean))

## # A tibble: 30 √ó 11
##     mtry trees min_n tree_depth learn_rate .metric  .estimator  mean     n std_err .config 
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   
##  1     1  1787    25          7   6.84e- 3 accuracy binary     0.835     5 0.0125  Preproc‚Ä¶
##  2    12  1885    18         15   2.06e- 3 accuracy binary     0.831     5 0.00602 Preproc‚Ä¶
##  3    13  1278     5          6   2.10e- 2 accuracy binary     0.826     5 0.00431 Preproc‚Ä¶
##  4     3  1681    22          5   5.42e- 3 accuracy binary     0.825     5 0.00507 Preproc‚Ä¶
##  5     9   303     2          8   9.94e- 2 accuracy binary     0.820     4 0.0120  Preproc‚Ä¶
##  6    11  1201    24         12   3.77e- 2 accuracy binary     0.812     3 0.00868 Preproc‚Ä¶
##  7    11   634    35         11   1.30e- 3 accuracy binary     0.805     3 0.0242  Preproc‚Ä¶
##  8     7  1648     4          7   2.38e- 4 accuracy binary     0.737     3 0.0115  Preproc‚Ä¶
##  9     1  1427    27         10   1.74e- 6 accuracy binary     0.378     3 0.0152  Preproc‚Ä¶
## 10     2   940    19          3   3.20e-10 accuracy binary     0.378     3 0.0152  Preproc‚Ä¶
## # ‚Ä¶ with 20 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;fit-model-2&#34;&gt;Fit model&lt;/h2&gt;
&lt;p&gt;Use best config to fit model to training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cb_fit &amp;lt;- cb_wf %&amp;gt;%
  finalize_workflow(select_best(cb_tune)) %&amp;gt;%
  fit(train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out accuracy on testing dataset to see if we overfitted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cb_fit %&amp;gt;%
  augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 √ó 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.808
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;make-predictions-2&#34;&gt;Make predictions&lt;/h2&gt;
&lt;p&gt;Now we‚Äôre ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cb_wf %&amp;gt;%
  finalize_workflow(select_best(cb_tune)) %&amp;gt;%
  fit(dataset) %&amp;gt;%
  augment(holdout) %&amp;gt;%
  select(PassengerId, Survived = .pred_class) %&amp;gt;%
  mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
  write_csv(&amp;quot;output/titanic/catboost.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I got and accuracy of 0.76076. Cool.&lt;/p&gt;
&lt;h1 id=&#34;regularization-methods&#34;&gt;Regularization methods&lt;/h1&gt;
&lt;p&gt;Let‚Äôs continue with 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Elastic_net_regularization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;elastic net
regularization&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;tuning-3&#34;&gt;Tuning&lt;/h2&gt;
&lt;p&gt;First a recipe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;en_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  step_normalize(all_numeric_predictors()) %&amp;gt;% # normalize
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then specify a regularization model. We tune parameter mixture, with
ridge regression for mixture = 0, and lasso for mixture = 1.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;en_model &amp;lt;- logistic_reg(penalty = tune(), 
                         mixture = tune()) %&amp;gt;% # param to be tuned
  set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;% # elastic net
  set_mode(&amp;quot;classification&amp;quot;) # binary response
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now set our workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;en_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(en_model) %&amp;gt;% 
  add_recipe(en_rec)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use cross-validation to evaluate our model with different param config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;en_tune &amp;lt;- en_wf %&amp;gt;%
  tune_grid(train_5fold,
            metrics = mset,
            control = control,
            grid = crossing(penalty = 10 ^ seq(-8, -.5, .5),
                            mixture = seq(0, 1, length.out = 10)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize the results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(en_tune)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-47-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Collect metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;en_tune %&amp;gt;%
  collect_metrics() %&amp;gt;%
  arrange(desc(mean))

## # A tibble: 160 √ó 8
##         penalty mixture .metric  .estimator  mean     n std_err .config               
##           &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                 
##  1 0.00000001     0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model017
##  2 0.0000000316   0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model018
##  3 0.0000001      0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model019
##  4 0.000000316    0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model020
##  5 0.000001       0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model021
##  6 0.00000316     0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model022
##  7 0.00001        0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model023
##  8 0.0000316      0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model024
##  9 0.0001         0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model025
## 10 0.000316       0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model026
## # ‚Ä¶ with 150 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;fit-model-3&#34;&gt;Fit model&lt;/h2&gt;
&lt;p&gt;Use best config to fit model to training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;en_fit &amp;lt;- en_wf %&amp;gt;%
  finalize_workflow(select_best(en_tune)) %&amp;gt;%
  fit(train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out accuracy on testing dataset to see if we overfitted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;en_fit %&amp;gt;%
  augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 √ó 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.826
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out important features (aka predictors).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(broom)
en_fit$fit$fit$fit %&amp;gt;%
  tidy() %&amp;gt;%
  filter(lambda &amp;gt;= select_best(en_tune)$penalty) %&amp;gt;%
  filter(lambda == min(lambda),
         term != &amp;quot;(Intercept)&amp;quot;) %&amp;gt;%
  mutate(term = fct_reorder(term, estimate)) %&amp;gt;%
  ggplot(aes(estimate, term, fill = estimate &amp;gt; 0)) +
  geom_col() +
  theme(legend.position = &amp;quot;none&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-51-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;make-predictions-3&#34;&gt;Make predictions&lt;/h2&gt;
&lt;p&gt;Now we‚Äôre ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;en_wf %&amp;gt;%
  finalize_workflow(select_best(en_tune)) %&amp;gt;%
  fit(dataset) %&amp;gt;%
  augment(holdout) %&amp;gt;%
  select(PassengerId, Survived = .pred_class) %&amp;gt;%
  mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
  write_csv(&amp;quot;output/titanic/regularization.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I got and accuracy of 0.76315.&lt;/p&gt;
&lt;h1 id=&#34;logistic-regression&#34;&gt;Logistic regression&lt;/h1&gt;
&lt;p&gt;And what about a good old-fashioned logistic regression (not a ML algo)?&lt;/p&gt;
&lt;p&gt;First a recipe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logistic_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  step_normalize(all_numeric_predictors()) %&amp;gt;% # normalize
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then specify a logistic regression.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logistic_model &amp;lt;- logistic_reg() %&amp;gt;% # no param to be tuned
  set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;% # elastic net
  set_mode(&amp;quot;classification&amp;quot;) # binary response
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now set our workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logistic_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(logistic_model) %&amp;gt;% 
  add_recipe(logistic_rec)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fit model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logistic_fit &amp;lt;- logistic_wf %&amp;gt;%
  fit(train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inspect significant features (aka predictors).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tidy(logistic_fit, exponentiate = TRUE) %&amp;gt;%
  filter(p.value &amp;lt; 0.05)

## # A tibble: 6 √ó 5
##   term         estimate std.error statistic       p.value
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
## 1 age             1.35      0.152      1.98 0.0473       
## 2 familyage       2.97      0.223      4.88 0.00000105   
## 3 class_first     0.136     0.367     -5.43 0.0000000559 
## 4 class_second    0.386     0.298     -3.19 0.00145      
## 5 title_Mr.      51.0       0.684      5.75 0.00000000912
## 6 title_Other    54.7       0.991      4.04 0.0000538
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Same thing, but graphically.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(broom)
logistic_fit %&amp;gt;%
  tidy() %&amp;gt;%
  mutate(term = fct_reorder(term, estimate)) %&amp;gt;%
  ggplot(aes(estimate, term, fill = estimate &amp;gt; 0)) +
  geom_col() +
  theme(legend.position = &amp;quot;none&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-58-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Check out accuracy on testing dataset to see if we overfitted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logistic_fit %&amp;gt;%
  augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 √ó 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.821
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Confusion matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logistic_fit %&amp;gt;%
  augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
  conf_mat(survived, .pred_class)

##           Truth
## Prediction yes  no
##        yes  59  13
##        no   27 125
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ROC curve.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logistic_fit %&amp;gt;%
  augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
  roc_curve(truth = survived, estimate = .pred_yes) %&amp;gt;%
  autoplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-61-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we‚Äôre ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logistic_wf %&amp;gt;%
  fit(dataset) %&amp;gt;%
  augment(holdout) %&amp;gt;%
  select(PassengerId, Survived = .pred_class) %&amp;gt;%
  mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
  write_csv(&amp;quot;output/titanic/logistic.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I got and accuracy of 0.76076. Oldies but goodies!&lt;/p&gt;
&lt;h1 id=&#34;neural-networks&#34;&gt;Neural networks&lt;/h1&gt;
&lt;p&gt;We go on with 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;neural
networks&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;tuning-4&#34;&gt;Tuning&lt;/h2&gt;
&lt;p&gt;Set up defaults.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mset &amp;lt;- metric_set(accuracy) # metric is accuracy
control &amp;lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First a recipe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nn_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  step_normalize(all_numeric_predictors()) %&amp;gt;%
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then specify a neural network.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nn_model &amp;lt;- mlp(epochs = tune(), 
                hidden_units = tune(), 
                dropout = tune()) %&amp;gt;% # param to be tuned
  set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;% # binary response var
  set_engine(&amp;quot;keras&amp;quot;, verbose = 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now set our workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nn_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(nn_model) %&amp;gt;% 
  add_recipe(nn_rec)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use cross-validation to evaluate our model with different param config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nn_tune &amp;lt;- nn_wf %&amp;gt;%
  tune_race_anova(
    train_5fold,
    grid = 30,
    param_info = nn_model %&amp;gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize the results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(nn_tune)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-68-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Collect metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nn_tune %&amp;gt;%
  collect_metrics() %&amp;gt;%
  arrange(desc(mean))

## # A tibble: 30 √ó 9
##    hidden_units dropout epochs .metric  .estimator  mean     n std_err .config             
##           &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
##  1           10 0.484      405 accuracy binary     0.838     5 0.0150  Preprocessor1_Model‚Ä¶
##  2            7 0.0597     220 accuracy binary     0.834     5 0.00881 Preprocessor1_Model‚Ä¶
##  3            4 0.536      629 accuracy binary     0.834     5 0.0154  Preprocessor1_Model‚Ä¶
##  4            9 0.198      768 accuracy binary     0.832     5 0.0146  Preprocessor1_Model‚Ä¶
##  5            6 0.752      822 accuracy binary     0.832     5 0.0112  Preprocessor1_Model‚Ä¶
##  6            9 0.406      293 accuracy binary     0.831     5 0.0145  Preprocessor1_Model‚Ä¶
##  7            4 0.00445    871 accuracy binary     0.831     5 0.0150  Preprocessor1_Model‚Ä¶
##  8            4 0.293      353 accuracy binary     0.831     5 0.0117  Preprocessor1_Model‚Ä¶
##  9           10 0.675      935 accuracy binary     0.831     5 0.0141  Preprocessor1_Model‚Ä¶
## 10            7 0.128      580 accuracy binary     0.831     5 0.0151  Preprocessor1_Model‚Ä¶
## # ‚Ä¶ with 20 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;fit-model-4&#34;&gt;Fit model&lt;/h2&gt;
&lt;p&gt;Use best config to fit model to training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nn_fit &amp;lt;- nn_wf %&amp;gt;%
  finalize_workflow(select_best(nn_tune)) %&amp;gt;%
  fit(train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out accuracy on testing dataset to see if we overfitted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nn_fit %&amp;gt;%
  augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 √ó 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.817
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;make-predictions-4&#34;&gt;Make predictions&lt;/h2&gt;
&lt;p&gt;Now we‚Äôre ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nn_wf %&amp;gt;%
  finalize_workflow(select_best(nn_tune)) %&amp;gt;%
  fit(train) %&amp;gt;%
  augment(holdout) %&amp;gt;%
  select(PassengerId, Survived = .pred_class) %&amp;gt;%
  mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
  write_csv(&amp;quot;output/titanic/nn.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I got and accuracy of 0.78708. My best score so far.&lt;/p&gt;
&lt;h1 id=&#34;support-vector-machines&#34;&gt;Support vector machines&lt;/h1&gt;
&lt;p&gt;We go on with 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Support-vector_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;support vector
machines&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;tuning-5&#34;&gt;Tuning&lt;/h2&gt;
&lt;p&gt;Set up defaults.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mset &amp;lt;- metric_set(accuracy) # metric is accuracy
control &amp;lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First a recipe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;svm_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  # remove any zero variance predictors
  step_zv(all_predictors()) %&amp;gt;% 
  # remove any linear combinations
  step_lincomb(all_numeric()) %&amp;gt;%
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then specify a svm.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;svm_model &amp;lt;- svm_rbf(cost = tune(), 
                     rbf_sigma = tune()) %&amp;gt;% # param to be tuned
  set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;% # binary response var
  set_engine(&amp;quot;kernlab&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now set our workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;svm_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(svm_model) %&amp;gt;% 
  add_recipe(svm_rec)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use cross-validation to evaluate our model with different param config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;svm_tune &amp;lt;- svm_wf %&amp;gt;%
  tune_race_anova(
    train_5fold,
    grid = 30,
    param_info = svm_model %&amp;gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize the results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(svm_tune)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-78-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Collect metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;svm_tune %&amp;gt;%
  collect_metrics() %&amp;gt;%
  arrange(desc(mean))

## # A tibble: 30 √ó 8
##        cost    rbf_sigma .metric  .estimator  mean     n std_err .config              
##       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                
##  1 17.7     0.00183      accuracy binary     0.829     5 0.0101  Preprocessor1_Model05
##  2  0.741   0.0506       accuracy binary     0.823     5 0.0129  Preprocessor1_Model14
##  3  1.77    0.0429       accuracy binary     0.820     5 0.0143  Preprocessor1_Model08
##  4  0.229   0.0197       accuracy binary     0.808     3 0.00626 Preprocessor1_Model23
##  5  1.15    0.00285      accuracy binary     0.795     3 0.00867 Preprocessor1_Model28
##  6  0.00182 0.00000203   accuracy binary     0.613     3 0.0171  Preprocessor1_Model01
##  7  0.0477  0.714        accuracy binary     0.613     3 0.0171  Preprocessor1_Model02
##  8  6.60    0.0000294    accuracy binary     0.613     3 0.0171  Preprocessor1_Model03
##  9  0.00254 0.000636     accuracy binary     0.613     3 0.0171  Preprocessor1_Model04
## 10  0.00544 0.0000000647 accuracy binary     0.613     3 0.0171  Preprocessor1_Model06
## # ‚Ä¶ with 20 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;fit-model-5&#34;&gt;Fit model&lt;/h2&gt;
&lt;p&gt;Use best config to fit model to training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;svm_fit &amp;lt;- svm_wf %&amp;gt;%
  finalize_workflow(select_best(svm_tune)) %&amp;gt;%
  fit(train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out accuracy on testing dataset to see if we overfitted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;svm_fit %&amp;gt;%
  augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 √ó 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.826
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;make-predictions-5&#34;&gt;Make predictions&lt;/h2&gt;
&lt;p&gt;Now we‚Äôre ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;svm_wf %&amp;gt;%
  finalize_workflow(select_best(svm_tune)) %&amp;gt;%
  fit(dataset) %&amp;gt;%
  augment(holdout) %&amp;gt;%
  select(PassengerId, Survived = .pred_class) %&amp;gt;%
  mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
  write_csv(&amp;quot;output/titanic/svm.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I got and accuracy of 0.77511.&lt;/p&gt;
&lt;h1 id=&#34;decision-trees&#34;&gt;Decision trees&lt;/h1&gt;
&lt;p&gt;We go on with 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;decision
trees&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;tuning-6&#34;&gt;Tuning&lt;/h2&gt;
&lt;p&gt;Set up defaults.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mset &amp;lt;- metric_set(accuracy) # metric is accuracy
control &amp;lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First a recipe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dt_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  step_zv(all_predictors()) %&amp;gt;% 
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then specify a decision tree model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(baguette)
dt_model &amp;lt;- bag_tree(cost_complexity = tune(),
                     tree_depth = tune(),
                     min_n = tune()) %&amp;gt;% # param to be tuned
  set_engine(&amp;quot;rpart&amp;quot;, times = 25) %&amp;gt;% # nb bootstraps
  set_mode(&amp;quot;classification&amp;quot;) # binary response var
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now set our workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dt_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(dt_model) %&amp;gt;% 
  add_recipe(dt_rec)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use cross-validation to evaluate our model with different param config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dt_tune &amp;lt;- dt_wf %&amp;gt;%
  tune_race_anova(
    train_5fold,
    grid = 30,
    param_info = dt_model %&amp;gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize the results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(dt_tune)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-88-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Collect metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dt_tune %&amp;gt;%
  collect_metrics() %&amp;gt;%
  arrange(desc(mean))

## # A tibble: 30 √ó 9
##    cost_complexity tree_depth min_n .metric  .estimator  mean     n std_err .config        
##              &amp;lt;dbl&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;          
##  1        3.02e- 7         13     7 accuracy binary     0.832     5 0.0130  Preprocessor1_‚Ä¶
##  2        3.43e- 3         14     4 accuracy binary     0.829     5 0.0145  Preprocessor1_‚Ä¶
##  3        5.75e- 5          5     7 accuracy binary     0.828     5 0.0126  Preprocessor1_‚Ä¶
##  4        2.13e- 6         14     5 accuracy binary     0.828     5 0.0108  Preprocessor1_‚Ä¶
##  5        1.34e- 5          5    35 accuracy binary     0.823     5 0.00654 Preprocessor1_‚Ä¶
##  6        4.47e- 5         13    10 accuracy binary     0.823     5 0.0120  Preprocessor1_‚Ä¶
##  7        1.42e- 2          4    25 accuracy binary     0.822     5 0.0121  Preprocessor1_‚Ä¶
##  8        4.54e-10         10    36 accuracy binary     0.822     5 0.0147  Preprocessor1_‚Ä¶
##  9        8.10e- 8         11    32 accuracy binary     0.822     5 0.0143  Preprocessor1_‚Ä¶
## 10        3.43e- 4         11    21 accuracy binary     0.820     5 0.0207  Preprocessor1_‚Ä¶
## # ‚Ä¶ with 20 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;fit-model-6&#34;&gt;Fit model&lt;/h2&gt;
&lt;p&gt;Use best config to fit model to training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dt_fit &amp;lt;- dt_wf %&amp;gt;%
  finalize_workflow(select_best(dt_tune)) %&amp;gt;%
  fit(train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out accuracy on testing dataset to see if we overfitted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dt_fit %&amp;gt;%
  augment(test, type.predict = &amp;quot;response&amp;quot;) %&amp;gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 √ó 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.808
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;make-predictions-6&#34;&gt;Make predictions&lt;/h2&gt;
&lt;p&gt;Now we‚Äôre ready to predict survival for the holdout dataset and submit
to Kaggle.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dt_wf %&amp;gt;%
  finalize_workflow(select_best(dt_tune)) %&amp;gt;%
  fit(dataset) %&amp;gt;%
  augment(holdout) %&amp;gt;%
  select(PassengerId, Survived = .pred_class) %&amp;gt;%
  mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
  write_csv(&amp;quot;output/titanic/dt.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I got and accuracy of 0.76794.&lt;/p&gt;
&lt;h1 id=&#34;stacked-ensemble-modelling&#34;&gt;Stacked ensemble modelling&lt;/h1&gt;
&lt;p&gt;Let‚Äôs do some ensemble modelling with all algo but logistic and
catboost. Tune again with a probability-based metric. Start with
xgboost.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(finetune)
library(stacks)
# xgboost
xg_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
xg_model &amp;lt;- boost_tree(mode = &amp;quot;classification&amp;quot;, # binary response
                       trees = tune(),
                       mtry = tune(),
                       tree_depth = tune(),
                       learn_rate = tune(),
                       loss_reduction = tune(),
                       min_n = tune()) # parameters to be tuned
xg_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(xg_model) %&amp;gt;% 
  add_recipe(xg_rec)
xg_grid &amp;lt;- grid_latin_hypercube(
  trees(),
  finalize(mtry(), train),
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  min_n(),
  size = 30)
xg_tune &amp;lt;- xg_wf %&amp;gt;%
  tune_grid(
    resamples = train_5fold,
    grid = xg_grid,
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then random forests.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# random forest
rf_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% 
  step_dummy(all_nominal_predictors()) 
rf_model &amp;lt;- rand_forest(mode = &amp;quot;classification&amp;quot;, # binary response
                        engine = &amp;quot;ranger&amp;quot;, # by default
                        mtry = tune(),
                        trees = tune(),
                        min_n = tune()) # parameters to be tuned
rf_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(rf_model) %&amp;gt;% 
  add_recipe(rf_rec)
rf_grid &amp;lt;- grid_latin_hypercube(
  finalize(mtry(), train),
  trees(),
  min_n(),
  size = 30)
rf_tune &amp;lt;- rf_wf %&amp;gt;%
  tune_grid(
    resamples = train_5fold,
    grid = rf_grid,
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Regularisation methods (between ridge and lasso).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# regularization methods
en_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  step_normalize(all_numeric_predictors()) %&amp;gt;% # normalize
  step_dummy(all_nominal_predictors()) 
en_model &amp;lt;- logistic_reg(penalty = tune(), 
                         mixture = tune()) %&amp;gt;% # param to be tuned
  set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;% # elastic net
  set_mode(&amp;quot;classification&amp;quot;) # binary response
en_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(en_model) %&amp;gt;% 
  add_recipe(en_rec)
en_grid &amp;lt;- grid_latin_hypercube(
  penalty(),
  mixture(),
  size = 30)
en_tune &amp;lt;- en_wf %&amp;gt;%
  tune_grid(
    resamples = train_5fold,
    grid = en_grid,
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Neural networks (takes time, so pick only a few values for illustration
purpose).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# neural networks
nn_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% # replace missing value by median
  step_normalize(all_numeric_predictors()) %&amp;gt;%
  step_dummy(all_nominal_predictors()) 
nn_model &amp;lt;- mlp(epochs = tune(), 
                hidden_units = 2, 
                dropout = tune()) %&amp;gt;% # param to be tuned
  set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;% # binary response var
  set_engine(&amp;quot;keras&amp;quot;, verbose = 0)
nn_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(nn_model) %&amp;gt;% 
  add_recipe(nn_rec)
# nn_grid &amp;lt;- grid_latin_hypercube(
#   epochs(),
#   hidden_units(),
#   dropout(),
#   size = 10)
nn_tune &amp;lt;- nn_wf %&amp;gt;%
  tune_grid(
    resamples = train_5fold,
    grid = crossing(dropout = c(0.1, 0.2), epochs = c(250, 500, 1000)), # nn_grid
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
#autoplot(nn_tune)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Support vector machines.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# support vector machines
svm_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% 
  step_normalize(all_numeric_predictors()) %&amp;gt;%
  step_dummy(all_nominal_predictors()) 
svm_model &amp;lt;- svm_rbf(cost = tune(), 
                     rbf_sigma = tune()) %&amp;gt;% # param to be tuned
  set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;% # binary response var
  set_engine(&amp;quot;kernlab&amp;quot;)
svm_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(svm_model) %&amp;gt;% 
  add_recipe(svm_rec)
svm_grid &amp;lt;- grid_latin_hypercube(
  cost(),
  rbf_sigma(),
  size = 30)
svm_tune &amp;lt;- svm_wf %&amp;gt;%
  tune_grid(
    resamples = train_5fold,
    grid = svm_grid,
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Last, decision trees.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# decision trees
dt_rec &amp;lt;- recipe(survived ~ ., data = train) %&amp;gt;%
  step_impute_median(all_numeric()) %&amp;gt;% 
  step_zv(all_predictors()) %&amp;gt;% 
  step_dummy(all_nominal_predictors()) 
library(baguette)
dt_model &amp;lt;- bag_tree(cost_complexity = tune(),
                     tree_depth = tune(),
                     min_n = tune()) %&amp;gt;% # param to be tuned
  set_engine(&amp;quot;rpart&amp;quot;, times = 25) %&amp;gt;% # nb bootstraps
  set_mode(&amp;quot;classification&amp;quot;) # binary response var
dt_wf &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(dt_model) %&amp;gt;% 
  add_recipe(dt_rec)
dt_grid &amp;lt;- grid_latin_hypercube(
  cost_complexity(),
  tree_depth(),
  min_n(),
  size = 30)
dt_tune &amp;lt;- dt_wf %&amp;gt;%
  tune_grid(
    resamples = train_5fold,
    grid = dt_grid,
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Get best config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xg_best &amp;lt;- xg_tune %&amp;gt;% filter_parameters(parameters = select_best(xg_tune))
rf_best &amp;lt;- rf_tune %&amp;gt;% filter_parameters(parameters = select_best(rf_tune))
en_best &amp;lt;- en_tune %&amp;gt;% filter_parameters(parameters = select_best(en_tune))
nn_best &amp;lt;- nn_tune %&amp;gt;% filter_parameters(parameters = select_best(nn_tune))
svm_best &amp;lt;- svm_tune %&amp;gt;% filter_parameters(parameters = select_best(svm_tune))
dt_best &amp;lt;- dt_tune %&amp;gt;% filter_parameters(parameters = select_best(dt_tune))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do the stacked ensemble modelling.&lt;/p&gt;
&lt;p&gt;Pile all models together.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;blended &amp;lt;- stacks() %&amp;gt;% # initialize
  add_candidates(en_best) %&amp;gt;% # add regularization model
  add_candidates(xg_best) %&amp;gt;% # add gradient boosting
  add_candidates(rf_best) %&amp;gt;% # add random forest
  add_candidates(nn_best) %&amp;gt;% # add neural network
  add_candidates(svm_best) %&amp;gt;% # add svm
  add_candidates(dt_best) # add decision trees
blended

## # A data stack with 6 model definitions and 6 candidate members:
## #   en_best: 1 model configuration
## #   xg_best: 1 model configuration
## #   rf_best: 1 model configuration
## #   nn_best: 1 model configuration
## #   svm_best: 1 model configuration
## #   dt_best: 1 model configuration
## # Outcome: survived (factor)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fit regularized model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;blended_fit &amp;lt;- blended %&amp;gt;%
  blend_predictions() # fit regularized model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualise penalized model. Note that neural networks are dropped,
despite achieving best score when used in isolation. I‚Äôll have to dig
into that.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(blended_fit)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-102-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(blended_fit, type = &amp;quot;members&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-102-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoplot(blended_fit, type = &amp;quot;weights&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/unnamed-chunk-102-3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Fit candidate members with non-zero stacking coef with full training
dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;blended_regularized &amp;lt;- blended_fit %&amp;gt;%
  fit_members() 
blended_regularized

## # A tibble: 3 √ó 3
##   member                type         weight
##   &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;
## 1 .pred_no_en_best_1_12 logistic_reg  2.67 
## 2 .pred_no_dt_best_1_11 bag_tree      1.95 
## 3 .pred_no_rf_best_1_05 rand_forest   0.922
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perf on testing dataset?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;test %&amp;gt;%
  bind_cols(predict(blended_regularized, .)) %&amp;gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 √ó 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.826
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now predict.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;holdout %&amp;gt;%
  bind_cols(predict(blended_regularized, .)) %&amp;gt;%
  select(PassengerId, Survived = .pred_class) %&amp;gt;%
  mutate(Survived = if_else(Survived == &amp;quot;yes&amp;quot;, 1, 0)) %&amp;gt;%
  write_csv(&amp;quot;output/titanic/stacked.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I got an 0.76076 accuracy.&lt;/p&gt;
&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;I covered several ML algorithms and logistic regression with the awesome
&lt;code&gt;tidymodels&lt;/code&gt; metapackage in &lt;code&gt;R&lt;/code&gt;. My scores at predicting Titanic
survivors were ok I guess. Some folks on Kaggle got a perfect accuracy,
so there is always room for improvement. Maybe better tuning, better
features (or predictors) or other algorithms would increase accuracy. Of
course, I forgot to use &lt;code&gt;set.seed()&lt;/code&gt; so results are not exactly
reproducible.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;This post was also published on 
&lt;a href=&#34;https://www.r-bloggers.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.r-bloggers.com&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Quick and dirty analysis of a Twitter social network</title>
      <link>https://oliviergimenez.github.io/blog/twitter-social-network/</link>
      <pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/twitter-social-network/</guid>
      <description>&lt;p&gt;I use &lt;code&gt;R&lt;/code&gt; to retrieve some data from Twitter, do some exploratory data analysis and visualisation and examine a network of followers.&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;I use Twitter to get live updates of what my follow scientists are up to, to communicate about my students&amp;rsquo; awesome work and to share material that I hope is useful to some people&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Recently, I reached 5,000 followers and I thought I&amp;rsquo;d spend some time trying to know better who they/you are. To do so, I use &lt;code&gt;R&lt;/code&gt; to retrieve some data from Twitter using 
&lt;a href=&#34;https://docs.ropensci.org/rtweet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;rtweet&lt;/code&gt;&lt;/a&gt;, do some data exploration and visualisation using the 
&lt;a href=&#34;https://www.tidyverse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt; and examine my network of followers with 
&lt;a href=&#34;https://tidygraph.data-imaginist.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;tidygraph&lt;/code&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://ggraph.data-imaginist.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ggraph&lt;/code&gt;&lt;/a&gt; and 
&lt;a href=&#34;https://igraph.org/r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;igraph&lt;/code&gt;&lt;/a&gt;. Data and codes are available from 
&lt;a href=&#34;https://github.com/oliviergimenez/sna-twitter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/oliviergimenez/sna-twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To reproduce the analyses below, you will need to access a Twitter API (application programming interface) to retrieve the information about your followers. In brief, an API is an intermediary application that allows applications to talk to each other. To access the Twitter APIs, you need a developer account for which you may apply at 
&lt;a href=&#34;https://developer.twitter.com/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://developer.twitter.com/en&lt;/a&gt;. There is a short form to fill in, and it takes less than a day to get an answer.&lt;/p&gt;
&lt;p&gt;Below I rely heavily on the code shared by Joe Cristian through the Algoritma Technical Blog at 
&lt;a href=&#34;https://algotech.netlify.app/blog/social-network-analysis-in-r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://algotech.netlify.app/blog/social-network-analysis-in-r/&lt;/a&gt;. Kuddos and credits to him.&lt;/p&gt;
&lt;h2 id=&#34;data-retrieving&#34;&gt;Data retrieving&lt;/h2&gt;
&lt;p&gt;We load the &lt;code&gt;rtweet&lt;/code&gt; package to work with Twitter from R.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#devtools::install_github(&amp;quot;ropensci/rtweet&amp;quot;)
library(rtweet)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also load the &lt;code&gt;tidyverse&lt;/code&gt; for data manipulation and visualisation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
theme_set(theme_light(base_size = 14))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we need to get credentials. The usual &lt;code&gt;rtweet&lt;/code&gt; sequence with &lt;code&gt;rtweet_app&lt;/code&gt;, &lt;code&gt;auth_save&lt;/code&gt; and &lt;code&gt;auth_as&lt;/code&gt; is supposed to work (see 
&lt;a href=&#34;https://docs.ropensci.org/rtweet/articles/auth.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;), but the Twitter API kept failing (error 401) for me. Tried a few things, in vain. I will use the deprecated &lt;code&gt;create_token&lt;/code&gt; function instead. You might need to change the defaults of your Twitter app from &amp;ldquo;Read only&amp;rdquo; to &amp;ldquo;Read, write and access direct messages&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Enter my API keys and access tokens (not shown), then authenticate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;token &amp;lt;- create_token(
  app = &amp;quot;sna-twitter-network-5k&amp;quot;,
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Get my Twitter info with my description, number of followers, number of likes, etc.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;og &amp;lt;- lookup_users(&amp;quot;oaggimenez&amp;quot;)
str(og, max = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	1 obs. of  21 variables:
##  $ id                     : num 7.51e+17
##  $ id_str                 : chr &amp;quot;750892662224453632&amp;quot;
##  $ name                   : chr &amp;quot;Olivier Gimenez \U0001f596&amp;quot;
##  $ screen_name            : chr &amp;quot;oaggimenez&amp;quot;
##  $ location               : chr &amp;quot;Montpellier, France&amp;quot;
##  $ description            : chr &amp;quot;Scientist (he/him) @CNRS @cefemontpellier @twitthair1 ‚Ä¢ Grown statistician ‚Ä¢ Improvised ecologist ‚Ä¢ Sociologist&amp;quot;| __truncated__
##  $ url                    : chr &amp;quot;https://t.co/l7NImYeGdY&amp;quot;
##  $ protected              : logi FALSE
##  $ followers_count        : int 5042
##  $ friends_count          : int 2191
##  $ listed_count           : int 68
##  $ created_at             : chr &amp;quot;Thu Jul 07 03:22:16 +0000 2016&amp;quot;
##  $ favourites_count       : int 18066
##  $ verified               : logi FALSE
##  $ statuses_count         : int 5545
##  $ profile_image_url_https: chr &amp;quot;https://pbs.twimg.com/profile_images/1330619806396067845/mIPmR-x4_normal.jpg&amp;quot;
##  $ profile_banner_url     : chr &amp;quot;https://pbs.twimg.com/profile_banners/750892662224453632/1602417664&amp;quot;
##  $ default_profile        : logi FALSE
##  $ default_profile_image  : logi FALSE
##  $ withheld_in_countries  :List of 1
##   ..$ : list()
##  $ entities               :List of 1
##   ..$ :List of 2
##  - attr(*, &amp;quot;tweets&amp;quot;)=&#39;data.frame&#39;:	1 obs. of  36 variables:
##   ..$ created_at                   : chr &amp;quot;Thu Jul 29 16:15:52 +0000 2021&amp;quot;
##   ..$ id                           : num 1.42e+18
##   ..$ id_str                       : chr &amp;quot;1420780122949529601&amp;quot;
##   ..$ text                         : chr &amp;quot;Looking forward to digging into this paper \U0001f929\U0001f9ee\U0001f60d https://t.co/ddCMraM3kj&amp;quot;
##   ..$ truncated                    : logi FALSE
##   ..$ entities                     :List of 1
##   ..$ source                       : chr &amp;quot;&amp;lt;a href=\&amp;quot;http://twitter.com/download/iphone\&amp;quot; rel=\&amp;quot;nofollow\&amp;quot;&amp;gt;Twitter for iPhone&amp;lt;/a&amp;gt;&amp;quot;
##   ..$ in_reply_to_status_id        : logi NA
##   ..$ in_reply_to_status_id_str    : logi NA
##   ..$ in_reply_to_user_id          : logi NA
##   ..$ in_reply_to_user_id_str      : logi NA
##   ..$ in_reply_to_screen_name      : logi NA
##   ..$ geo                          : logi NA
##   ..$ coordinates                  :List of 1
##   ..$ place                        :List of 1
##   ..$ contributors                 : logi NA
##   ..$ is_quote_status              : logi TRUE
##   ..$ quoted_status_id             : num 1.42e+18
##   ..$ quoted_status_id_str         : chr &amp;quot;1420768369620312065&amp;quot;
##   ..$ retweet_count                : int 0
##   ..$ favorite_count               : int 8
##   ..$ favorited                    : logi FALSE
##   ..$ retweeted                    : logi FALSE
##   ..$ possibly_sensitive           : logi FALSE
##   ..$ lang                         : chr &amp;quot;en&amp;quot;
##   ..$ quoted_status                :List of 1
##   ..$ display_text_width           : int 70
##   ..$ user                         :List of 1
##   ..$ full_text                    : logi NA
##   ..$ favorited_by                 : logi NA
##   ..$ display_text_range           : logi NA
##   ..$ retweeted_status             : logi NA
##   ..$ quoted_status_permalink      : logi NA
##   ..$ metadata                     : logi NA
##   ..$ query                        : logi NA
##   ..$ possibly_sensitive_appealable: logi NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I obtain the id of my followers using &lt;code&gt;get_followers&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;followers &amp;lt;- get_followers(user = &amp;quot;oaggimenez&amp;quot;,
                           n = og$followers_count,
                           retryonratelimit = T)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From their id, I can get the same details I got on my account using &lt;code&gt;lookup_users&lt;/code&gt;. This function is not vectorized, therefore I use a loop. Takes some time so I saved the results and load them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;details_followers &amp;lt;- NULL
for (i in 1:length(followers$user_id)){
  tmp &amp;lt;- try(lookup_users(followers$user_id[i], retryonratelimit = TRUE), silent = TRUE)
  if (length(tmp) == 1){
    next
  } else {
    tmp$listed_count &amp;lt;- NULL # get rid of this column which raised some format issues, we do not it anyway
    details_followers &amp;lt;- bind_rows(details_followers, tmp)
  }
}
save(details_followers, file = &amp;quot;details_followers.RData&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What info do we have?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(&amp;quot;dat/details_followers.RData&amp;quot;)
names(details_followers)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;id&amp;quot;                      &amp;quot;id_str&amp;quot;                 
##  [3] &amp;quot;name&amp;quot;                    &amp;quot;screen_name&amp;quot;            
##  [5] &amp;quot;location&amp;quot;                &amp;quot;description&amp;quot;            
##  [7] &amp;quot;url&amp;quot;                     &amp;quot;protected&amp;quot;              
##  [9] &amp;quot;followers_count&amp;quot;         &amp;quot;friends_count&amp;quot;          
## [11] &amp;quot;created_at&amp;quot;              &amp;quot;favourites_count&amp;quot;       
## [13] &amp;quot;verified&amp;quot;                &amp;quot;statuses_count&amp;quot;         
## [15] &amp;quot;profile_image_url_https&amp;quot; &amp;quot;profile_banner_url&amp;quot;     
## [17] &amp;quot;default_profile&amp;quot;         &amp;quot;default_profile_image&amp;quot;  
## [19] &amp;quot;withheld_in_countries&amp;quot;   &amp;quot;entities&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In more details.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(details_followers, max = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	5025 obs. of  20 variables:
##  $ id                     : num  1.03e+18 1.42e+18 1.32e+09 1.42e+18 1.36e+18 ...
##  $ id_str                 : chr  &amp;quot;1026053171024736258&amp;quot; &amp;quot;1419513064697712640&amp;quot; &amp;quot;1317282258&amp;quot; &amp;quot;1419222504367927296&amp;quot; ...
##  $ name                   : chr  &amp;quot;Dr. Zoe Nhleko&amp;quot; &amp;quot;akash Shelke&amp;quot; &amp;quot;Rilwan Ugbedeojo&amp;quot; &amp;quot;Thaana Van Dessel&amp;quot; ...
##  $ screen_name            : chr  &amp;quot;ZoeNhleko&amp;quot; &amp;quot;akashSh88945929&amp;quot; &amp;quot;abuhrilwan&amp;quot; &amp;quot;ThaanaD&amp;quot; ...
##  $ location               : chr  &amp;quot;United States&amp;quot; &amp;quot;&amp;quot; &amp;quot;Lagos, Nigeria&amp;quot; &amp;quot;France&amp;quot; ...
##  $ description            : chr  &amp;quot;Wildlife ecologist with experience on African large mammals. PhD from @UF. Looking for job opportunities in eco&amp;quot;| __truncated__ &amp;quot;&amp;quot; &amp;quot;MSc, Plant Ecology | Seeking PhD Position in Community Ecology | Civic Leader | YALI RLC &amp;amp; African Presidential&amp;quot;| __truncated__ &amp;quot;MSc student majoring in Animal Ecology @WURanimal. \nFocused on Conservation Behavior &amp;amp; Human-Wildlife Coexistence.&amp;quot; ...
##  $ url                    : chr  &amp;quot;https://t.co/uaFY2L8BnL&amp;quot; NA NA NA ...
##  $ protected              : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ followers_count        : int  2878 1 244 0 2856 4 112 6 69 476 ...
##  $ friends_count          : int  1325 223 2030 14 2575 99 344 32 73 1671 ...
##  $ created_at             : chr  &amp;quot;Sun Aug 05 10:31:53 +0000 2018&amp;quot; &amp;quot;Mon Jul 26 04:21:18 +0000 2021&amp;quot; &amp;quot;Sat Mar 30 22:35:27 +0000 2013&amp;quot; &amp;quot;Sun Jul 25 09:06:36 +0000 2021&amp;quot; ...
##  $ favourites_count       : int  8003 0 1843 1 12360 0 484 0 83 713 ...
##  $ verified               : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ statuses_count         : int  7261 0 670 1 2112 0 141 0 30 926 ...
##  $ profile_image_url_https: chr  &amp;quot;https://pbs.twimg.com/profile_images/1306353853571280896/rk9qAxoa_normal.jpg&amp;quot; &amp;quot;https://pbs.twimg.com/profile_images/1419513134142738433/erR2XRwf_normal.png&amp;quot; &amp;quot;https://pbs.twimg.com/profile_images/1413368052012433413/EyzbYfWB_normal.jpg&amp;quot; &amp;quot;https://pbs.twimg.com/profile_images/1419222962566217731/TQT96npR_normal.jpg&amp;quot; ...
##  $ profile_banner_url     : chr  &amp;quot;https://pbs.twimg.com/profile_banners/1026053171024736258/1533465589&amp;quot; NA &amp;quot;https://pbs.twimg.com/profile_banners/1317282258/1624183687&amp;quot; &amp;quot;https://pbs.twimg.com/profile_banners/1419222504367927296/1627205254&amp;quot; ...
##  $ default_profile        : logi  TRUE TRUE FALSE TRUE TRUE TRUE ...
##  $ default_profile_image  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ withheld_in_countries  :List of 5025
##  $ entities               :List of 5025
##  - attr(*, &amp;quot;tweets&amp;quot;)=&#39;data.frame&#39;:	1 obs. of  36 variables:
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-exploration-and-visualisation&#34;&gt;Data exploration and visualisation&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s display the 100 bigger accounts that follow me. First thing I learned. It is humbling to be followed by influential individuals I admire like @MicrobiomDigest, @nathanpsmad, @FrancoisTaddei, @freakonometrics, @allison_horst, @HugePossum, @apreshill and scientific journals, institutions and societies.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;details_followers %&amp;gt;% 
  arrange(-followers_count) %&amp;gt;%
  select(screen_name,
         followers_count, 
         friends_count, 
         favourites_count) %&amp;gt;% 
  head(n = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         screen_name followers_count friends_count favourites_count
## 1     dianefrancis1          230560        132635               70
## 2    simongerman600          218444        209489            56976
## 3       WildlifeMag          191613         23995            14697
## 4     cagrimbakirci          170164          9401            21334
## 5       BernardMarr          130900         29532            15764
## 6          ZotovMax          119625        133610              894
## 7          _atanas_          116717         74266           102899
## 8   MicrobiomDigest          105825         33779            98199
## 9          Eliances           97488         94488            15164
## 10      PLOSBiology           89263          6646             4278
## 11            IPBES           78086         11797            53957
## 12           7wData           76131         84155            74609
## 13    biconnections           63802         52918            91970
## 14   aydintufekci43           60212         22627            99427
## 15      AliceVachet           56381          2320           182435
## 16         Pr_Logos           52482         50520            19852
## 17     DrJoeNyangon           51296         20858             1413
## 18   derekantoncich           48823         51449              347
## 19      Afro_Herper           45215          3392           117693
## 20       KumarAGarg           43461         47795             4052
## 21       GatelyMark           41500         37944             1153
## 22         overleaf           41306          7830            35289
## 23         figshare           40610         38361            10395
## 24         mlamons1           40415         29364             2498
## 25          coraman           40398          2169            16548
## 26   BritishEcolSoc           37394          1143            11685
## 27      nathanpsmad           36760          5312            43729
## 28   WildlifeRescue           34843          7976            61284
## 29       rudyagovic           34063         28147             3708
## 30   grp_resilience           34011          7998             3559
## 31           JNakev           33427         28532            29442
## 32   FrancoisTaddei           32631         17903            23894
## 33      RunEducator           31248         19848            17522
## 34      LarryLLapin           30782         34265              501
## 35      _Alex_Iaco_           30775         26720             2390
## 36           owasow           30562         12932            36909
## 37         JEcology           30534           689             3312
## 38        valmasdel           28972          4512             4334
## 39  freakonometrics           28038         15112            31106
## 40  MethodsEcolEvol           26207         10347             2083
## 41          mmw_lmw           26014         12157             1068
## 42  drmichellelarue           25424          6281            56120
## 43    nicolasberrod           25258          6042            12990
## 44         DD_NaNa_           24253         11051            39916
## 45      callin_bull           24106          2831            10572
## 46        INEE_CNRS           23483          1873             5089
## 47    Datascience__           22793         10695              216
## 48    VisualPersist           22779          5599            79872
## 49     coywolfassoc           22203         19491             9470
## 50        RBGSydney           20259         16049            13435
## 51    allison_horst           19289          2865             7725
## 52         rhskraus           18429         20029             3503
## 53      fred_sallee           18144         18995             3423
## 54         thembauk           17748          2066             3260
## 55     parolesdhist           17257          7136             7497
## 56   CREAF_ecologia           16654          3201            11899
## 57    ChamboncelLea           16609          4059             9797
## 58       HugePossum           16461         12732             7806
## 59  ScientistFemale           16379          9039             2079
## 60    sophie_e_hill           16292          5811            10315
## 61  mohamma64508589           16130         14550            24584
## 62      SamuelHayat           15999          3068             9767
## 63    MathieuAvanzi           15890          1604             8526
## 64          Erky321           15859         16284             4841
## 65       ConLetters           15482          5547             1489
## 66   oceansresearch           14969          1759             7599
## 67      WCSNewsroom           14926         11224             7277
## 68     CMastication           14687          8091            31769
## 69        apreshill           14106          1607            32806
## 70     SumanMalla10           13872         11977            10946
## 71     umontpellier           13667          1360             8745
## 72     WriteNThrive           13615         13523             7631
## 73     abdirashidmd           13586          2389            18417
## 74      JackKinross           13392         11263              774
## 75     PaulREhrlich           13383         14208                1
## 76       teppofelin           13330         12410              264
## 77  ConservOptimism           13249          4306            15648
## 78         jepeteso           13137         13595             3420
## 79         gwintrob           12484          8751            43479
## 80     SallyR_ISLHE           12411         12477            28924
## 81         ActuIAFr           12321          8856             2495
## 82         Booksmag           12282          1785              332
## 83  jusoneoftoomany           12212         11049             2296
## 84    galaxyproject           12208          8757              813
## 85  thepostdoctoral           12186         12802              814
## 86   FrontMarineSci           11498          5733             5832
## 87          jobRxiv           11442           872              786
## 88  EcographyJourna           11381          1465             4938
## 89  davidghamilton1           11242          2231            16244
## 90      5amStartups           11170         12370           147062
## 91   RosmarieKatrin           11079         11159             4060
## 92   ScienceAndMaps           11043          8679             3596
## 93       joelgombin           11012         10335            10868
## 94     SteveLenhert           10990          8328              669
## 95      Naturevolve           10973         11349             7500
## 96   cookinforpeace           10655          2861            17519
## 97     DrPaulEvans1           10537          7927             3131
## 98          Ibycter           10517          4450            47359
## 99  Sciencecomptoir           10436          1268            15800
## 100   SteveAtLogikk           10421          8864            13123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where do my followers come from? Second thing I learnt. Followers are folks from France (my home), USA, UK, Canada, Australia, and other European countries. I was happy to realize that I have followers from other parts of the world too, in India, South Africa, Brazil, Peru, Mexico, Nepal, Argentina, Kenya, Bangladesh, etc. and from Greece (my other home).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;details_followers %&amp;gt;% 
  mutate(location = str_replace(location, &amp;quot;.*France.*&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*England&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*United Kingdom&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*London&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*Scotland&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*Montpellier&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*Canada, BC.*&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*New Caledonia&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*Marseille&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*Grenoble&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*Per√∫&amp;quot;, &amp;quot;Peru&amp;quot;),
         location = str_replace(location, &amp;quot;.*Martinique&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*La Rochelle&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*Grenoble, Rh√¥ne-Alpes&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*Avignon&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*Paris&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*paris&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*france&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*Bordeaux&amp;quot;, &amp;quot;France&amp;quot;),
         location = str_replace(location, &amp;quot;.*Mytilene, Lesvos, Greece&amp;quot;, &amp;quot;Greece&amp;quot;),
         location = str_replace(location, &amp;quot;.*Mytiline, Greece&amp;quot;, &amp;quot;Greece&amp;quot;),
         location = str_replace(location, &amp;quot;.*Germany.*&amp;quot;, &amp;quot;Germany&amp;quot;),
         location = str_replace(location, &amp;quot;.*Vienna, Austria&amp;quot;, &amp;quot;Austria&amp;quot;),
         location = str_replace(location, &amp;quot;.*Arusha, Tanzania&amp;quot;, &amp;quot;Tanzania&amp;quot;),
         location = str_replace(location, &amp;quot;.*Athens&amp;quot;, &amp;quot;Greece&amp;quot;),
         location = str_replace(location, &amp;quot;.*Bangor, ME&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*Bangor, Wales&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*Baton Rouge, LA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Lisboa, Portugal&amp;quot;, &amp;quot;Portugal&amp;quot;),
         location = str_replace(location, &amp;quot;.*Oslo&amp;quot;, &amp;quot;Norway&amp;quot;),
         location = str_replace(location, &amp;quot;.*Islamic Republic of Iran&amp;quot;, &amp;quot;Iran&amp;quot;),
         location = str_replace(location, &amp;quot;.*Madrid, Espa√±a&amp;quot;, &amp;quot;Spain&amp;quot;),
         location = str_replace(location, &amp;quot;.*Madrid&amp;quot;, &amp;quot;Spain&amp;quot;),
         location = str_replace(location, &amp;quot;.*Madrid, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
         location = str_replace(location, &amp;quot;.*Rome, Italy&amp;quot;, &amp;quot;Italy&amp;quot;),
         location = str_replace(location, &amp;quot;.*Santiago, Chile&amp;quot;, &amp;quot;Chile&amp;quot;),
         location = str_replace(location, &amp;quot;.*Bel√©m, Brazil&amp;quot;, &amp;quot;Brazil&amp;quot;),
         location = str_replace(location, &amp;quot;.*Belo Horizonte, Brazil&amp;quot;, &amp;quot;Brazil&amp;quot;),
         location = str_replace(location, &amp;quot;.*Berlin&amp;quot;, &amp;quot;Germany&amp;quot;),
         location = str_replace(location, &amp;quot;.*Mexico, ME&amp;quot;, &amp;quot;Mexico&amp;quot;),
         location = str_replace(location, &amp;quot;.*Mexico City&amp;quot;, &amp;quot;Mexico&amp;quot;),
         location = str_replace(location, &amp;quot;.*Santiago, Chile&amp;quot;, &amp;quot;Chile&amp;quot;),
         location = str_replace(location, &amp;quot;.*Ontario&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Aberdeen&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*Cambridge&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*Bologna, Emilia Romagna&amp;quot;, &amp;quot;Italy&amp;quot;),
         location = str_replace(location, &amp;quot;.*Bogot√°, D.C., Colombia&amp;quot;, &amp;quot;Colombia&amp;quot;),
         location = str_replace(location, &amp;quot;.*Medell√≠n, Colombia&amp;quot;, &amp;quot;Colombia&amp;quot;),
         location = str_replace(location, &amp;quot;.*Bruxelles, Belgique&amp;quot;, &amp;quot;Belgium&amp;quot;),
         location = str_replace(location, &amp;quot;.*Brasil&amp;quot;, &amp;quot;Brazil&amp;quot;),
         location = str_replace(location, &amp;quot;.*Budweis, Czech Republic&amp;quot;, &amp;quot;Czech Republic&amp;quot;),
         location = str_replace(location, &amp;quot;.*Calgary, Alberta&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Canada, BC&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Cardiff, Wales&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*Brisbane&amp;quot;, &amp;quot;Australia&amp;quot;),
         location = str_replace(location, &amp;quot;.*Sydney&amp;quot;, &amp;quot;Australia&amp;quot;),
         location = str_replace(location, &amp;quot;.*Queensland&amp;quot;, &amp;quot;Australia&amp;quot;),
         location = str_replace(location, &amp;quot;.*Australia&amp;quot;, &amp;quot;Australia&amp;quot;),
         location = str_replace(location, &amp;quot;.*Germany&amp;quot;, &amp;quot;Germany&amp;quot;),
         location = str_replace(location, &amp;quot;.*Vancouver&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Ottawa, Ontario&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Qu√©bec, Canada&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Winnipeg, Manitoba&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*New South Wales&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Victoria&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*British Columbia&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Norway&amp;quot;, &amp;quot;Norway&amp;quot;),
         location = str_replace(location, &amp;quot;.*Finland&amp;quot;, &amp;quot;Finland&amp;quot;),
         location = str_replace(location, &amp;quot;.*South Africa&amp;quot;, &amp;quot;South Africa&amp;quot;),
         location = str_replace(location, &amp;quot;.*Switzerland&amp;quot;, &amp;quot;Switzerland&amp;quot;),
         location = str_replace(location, &amp;quot;.*CO&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*OK&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*KS&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*MS&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*CO&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*CO&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*CO&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*CO&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*WA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*MD&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Colorado&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Community of Valencia, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
         location = str_replace(location, &amp;quot;.*Dunedin City, New Zealand&amp;quot;, &amp;quot;New Zealand&amp;quot;),
         location = str_replace(location, &amp;quot;.*Rio Claro, Brasil&amp;quot;, &amp;quot;Brazil&amp;quot;),
         location = str_replace(location, &amp;quot;.*Saskatoon, Saskatchewan&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Sherbrooke, Qu√©bec&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*United Kingdom.*&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*University of Oxford&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*University of St Andrews&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*ID&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*NE&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*United States of America, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Spain, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
         location = str_replace(location, &amp;quot;.*USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Wisconsin, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Florida, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Liege, Belgium&amp;quot;, &amp;quot;Belgium&amp;quot;),
         location = str_replace(location, &amp;quot;.*Ghent, Belgium&amp;quot;, &amp;quot;Belgium&amp;quot;),
         location = str_replace(location, &amp;quot;.*Pune, India&amp;quot;, &amp;quot;India&amp;quot;),
         location = str_replace(location, &amp;quot;.*Hyderabad, India&amp;quot;, &amp;quot;India&amp;quot;),
         location = str_replace(location, &amp;quot;.*Prague, Czech Republic&amp;quot;, &amp;quot;Czech Republic&amp;quot;),
         location = str_replace(location, &amp;quot;.*Canada, BC, Canada&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*CA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*United States.*&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*DC&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*FL&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*GA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*HI&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*ME&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*MA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*MI&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*PA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*NC&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*MO&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*NY&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*NH&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*IL&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*NM&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*MT&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*OR&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*WY&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*WI&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*MN&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*CT&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*TX&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*VA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*OH&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Massachusetts, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*California, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Montr√©al, Qu√©bec&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Edmonton, Alberta&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Toronto, Ontario&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Canada, Canada&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Montreal&amp;quot;, &amp;quot;Canada&amp;quot;),
         location = str_replace(location, &amp;quot;.*Lisbon, Portugal&amp;quot;, &amp;quot;Portugal&amp;quot;),
         location = str_replace(location, &amp;quot;.*Coimbra, Portugal&amp;quot;, &amp;quot;Portugal&amp;quot;),
         location = str_replace(location, &amp;quot;.*Cork, Ireland&amp;quot;, &amp;quot;Ireland&amp;quot;),
         location = str_replace(location, &amp;quot;.*Dublin City, Ireland&amp;quot;, &amp;quot;Ireland&amp;quot;),
         location = str_replace(location, &amp;quot;.*Barcelona, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
         location = str_replace(location, &amp;quot;.*Barcelona&amp;quot;, &amp;quot;Spain&amp;quot;),
         location = str_replace(location, &amp;quot;.*Leipzig&amp;quot;, &amp;quot;Germany&amp;quot;),
         location = str_replace(location, &amp;quot;.*Seville, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
         location = str_replace(location, &amp;quot;.*Seville, Spain&amp;quot;, &amp;quot;Spain&amp;quot;),
         location = str_replace(location, &amp;quot;.*Buenos Aires, Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;),
         location = str_replace(location, &amp;quot;.*Rio de Janeiro, Brazil&amp;quot;, &amp;quot;Brazil&amp;quot;),
         location = str_replace(location, &amp;quot;.*Canberra&amp;quot;, &amp;quot;Australia&amp;quot;),
         location = str_remove(location, &amp;quot;Global&amp;quot;),
         location = str_remove(location, &amp;quot;Earth&amp;quot;),
         location = str_remove(location, &amp;quot;Worldwide&amp;quot;),
         location = str_remove(location, &amp;quot;Europe&amp;quot;),
         location = str_remove(location, &amp;quot;  &amp;quot;),
         location = str_replace(location, &amp;quot;.*Dhaka, Bangladesh&amp;quot;, &amp;quot;Bangladesh&amp;quot;),
         location = str_replace(location, &amp;quot;.*Copenhagen, Denmark&amp;quot;, &amp;quot;Denmark&amp;quot;),
         location = str_replace(location, &amp;quot;.*Amsterdam, The Netherlands&amp;quot;, &amp;quot;The Netherlands&amp;quot;),
         location = str_replace(location, &amp;quot;.*Groningen, Nederland&amp;quot;, &amp;quot;The Netherlands&amp;quot;),
         location = str_replace(location, &amp;quot;.*Wageningen, Nederland&amp;quot;, &amp;quot;The Netherlands&amp;quot;),
         location = str_replace(location, &amp;quot;.*Aarhus, Denmark&amp;quot;, &amp;quot;Denmark&amp;quot;),
         location = str_replace(location, &amp;quot;.*Antwerp, Belgium&amp;quot;, &amp;quot;Belgium&amp;quot;),
         location = str_replace(location, &amp;quot;.*Aveiro, Portugal&amp;quot;, &amp;quot;Portugal&amp;quot;),
         location = str_replace(location, &amp;quot;.*Australia, AUS&amp;quot;, &amp;quot;Australia&amp;quot;),
         location = str_replace(location, &amp;quot;.*Australian National University&amp;quot;, &amp;quot;Australia&amp;quot;),
         location = str_replace(location, &amp;quot;.*Auckland, New Zealand&amp;quot;, &amp;quot;New Zealand&amp;quot;),
         location = str_replace(location, &amp;quot;.*Belfast, Northern Ireland&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*Ireland&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*Hobart, Tasmania&amp;quot;, &amp;quot;Australia&amp;quot;),
         location = str_replace(location, &amp;quot;.*Dhaka, Bangladesh&amp;quot;, &amp;quot;Bangladesh&amp;quot;),
         location = str_replace(location, &amp;quot;.*Nairobi, Kenya&amp;quot;, &amp;quot;Kenya&amp;quot;),
         location = str_replace(location, &amp;quot;.*Dhaka, Bangladesh&amp;quot;, &amp;quot;Bangladesh&amp;quot;),
         location = str_replace(location, &amp;quot;.*Berlin, Deutschland&amp;quot;, &amp;quot;Germany&amp;quot;),
         location = str_replace(location, &amp;quot;.*Munich, Bavaria&amp;quot;, &amp;quot;Germany&amp;quot;),
         location = str_replace(location, &amp;quot;.*Dehradun, India&amp;quot;, &amp;quot;India&amp;quot;),
         location = str_replace(location, &amp;quot;.*Bengaluru, India&amp;quot;, &amp;quot;India&amp;quot;),
         location = str_replace(location, &amp;quot;.*Berlin, Deutschland&amp;quot;, &amp;quot;Germany&amp;quot;),
         location = str_replace(location, &amp;quot;.*Deutschland&amp;quot;, &amp;quot;Germany&amp;quot;),
         location = str_replace(location, &amp;quot;.*Edinburgh&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*Glasgow&amp;quot;, &amp;quot;United Kingdom&amp;quot;),
         location = str_replace(location, &amp;quot;.*New York, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Washington, USA&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*California&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*California&amp;quot;, &amp;quot;United States of America&amp;quot;),
         location = str_replace(location, &amp;quot;.*Christchurch City, New Zealand&amp;quot;, &amp;quot;New Zealand&amp;quot;),
         location = str_replace(location, &amp;quot;.*Harare, Zimbabwe&amp;quot;, &amp;quot;Zimbabwe&amp;quot;),
         location = str_replace(location, &amp;quot;.*Islamabad, Pakistan&amp;quot;, &amp;quot;Pakistan&amp;quot;),
         location = str_replace(location, &amp;quot;.*Kolkata, India&amp;quot;, &amp;quot;India&amp;quot;),
         location = str_replace(location, &amp;quot;.*Lagos, Nigeria&amp;quot;, &amp;quot;Nigeria&amp;quot;),
         location = str_replace(location, &amp;quot;.*Lima, Peru&amp;quot;, &amp;quot;Peru&amp;quot;),
         location = str_replace(location, &amp;quot;.*Valpara√≠so, Chile&amp;quot;, &amp;quot;Chile&amp;quot;),
         location = str_replace(location, &amp;quot;.*Uppsala, Sweden&amp;quot;, &amp;quot;Sweden&amp;quot;),
         location = str_replace(location, &amp;quot;.*Uppsala, Sverige&amp;quot;, &amp;quot;Sweden&amp;quot;),
         location = str_replace(location, &amp;quot;.*Stockholm, Sweden&amp;quot;, &amp;quot;Sweden&amp;quot;),
         location = str_replace(location, &amp;quot;.*Stockholm&amp;quot;, &amp;quot;Sweden&amp;quot;),
         location = str_replace(location, &amp;quot;.*Turin, Piedmont&amp;quot;, &amp;quot;Italy&amp;quot;),
         location = str_replace(location, &amp;quot;.*University of Iceland&amp;quot;, &amp;quot;Iceland&amp;quot;),
         location = str_replace(location, &amp;quot;.*University of Helsinki&amp;quot;, &amp;quot;Finland&amp;quot;),
         location = str_replace(location, &amp;quot;.*Tucum√°n, Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;),
         location = str_replace(location, &amp;quot;.*The Hague, The Netherlands&amp;quot;, &amp;quot;The Netherlands&amp;quot;),
         location = str_replace(location, &amp;quot;.*UK&amp;quot;, &amp;quot;United Kingdom&amp;quot;)
  ) %&amp;gt;%
  count(location, sort = TRUE) %&amp;gt;%
  #slice(-40) %&amp;gt;%
  head(n = 45) -&amp;gt; followers_by_country
followers_by_country
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    location    n
## 1                           1273
## 2  United States of America  606
## 3                    France  571
## 4            United Kingdom  473
## 5                    Canada  166
## 6                   Germany  130
## 7                 Australia  112
## 8                    Norway   50
## 9               Switzerland   43
## 10                    Spain   34
## 11                    India   33
## 12             South Africa   32
## 13                   Brazil   30
## 14                 Portugal   27
## 15                  Finland   26
## 16                   Sweden   18
## 17                  Belgium   17
## 18                    Italy   16
## 19          The Netherlands   14
## 20              New Zealand   13
## 21                     Peru   13
## 22                  Denmark   11
## 23                   Mexico   11
## 24                    Nepal   10
## 25                Argentina    9
## 26                    Kenya    9
## 27               Bangladesh    8
## 28                    Chile    8
## 29                   Greece    8
## 30                 Colombia    6
## 31           Czech Republic    6
## 32                Singapore    6
## 33                  Iceland    5
## 34                  Senegal    5
## 35                     Iran    4
## 36                  Nigeria    4
## 37                 Pakistan    4
## 38                 Tanzania    4
## 39                 Zimbabwe    4
## 40                             3
## 41                  Austria    3
## 42                  Ecuador    3
## 43                Guatemala    3
## 44                Hong Kong    3
## 45               Luxembourg    3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look to the same info on a quick and dirty map. White countries do not appear in the 45 countries with most followers, or do not have any followers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rworldmap)
library(classInt)
spdf &amp;lt;- joinCountryData2Map(followers_by_country, 
                            joinCode=&amp;quot;NAME&amp;quot;,
                            nameJoinColumn=&amp;quot;location&amp;quot;,
                            verbose=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 43 codes from your data successfully matched countries in the map
## 2 codes from your data failed to match with a country code in the map
##      failedCodes failedCountries
## [1,] &amp;quot;&amp;quot;          &amp;quot;&amp;quot;             
## [2,] &amp;quot;&amp;quot;          &amp;quot; &amp;quot;            
## 200 codes from the map weren&#39;t represented in your data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;classInt &amp;lt;- classIntervals(spdf$n,
                           n=9, 
                           style = &amp;quot;jenks&amp;quot;)
catMethod &amp;lt;- classInt[[&amp;quot;brks&amp;quot;]]
library(RColorBrewer)
colourPalette &amp;lt;- brewer.pal(9,&#39;RdPu&#39;)
mapParams &amp;lt;- mapCountryData(spdf,
                            nameColumnToPlot=&amp;quot;n&amp;quot;,
                            addLegend=FALSE,
                            catMethod = catMethod,
                            colourPalette=colourPalette,
                            mapTitle=&amp;quot;Number of followers per country&amp;quot;)
do.call(addMapLegend,
        c(mapParams,
          legendLabels=&amp;quot;all&amp;quot;,
          legendWidth=0.5,
          legendIntervals=&amp;quot;data&amp;quot;,
          legendMar = 2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/twitter-social-network/unnamed-chunk-12-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;How many followers are verified?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;details_followers %&amp;gt;% 
  count(verified)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   verified    n
## 1    FALSE 5009
## 2     TRUE   16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Who are the verified users? Scientific entities, but also some scientists.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;details_followers %&amp;gt;%
  filter(verified==TRUE) %&amp;gt;%
  select(name, screen_name, location, followers_count) %&amp;gt;%
  arrange(-followers_count)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                          name     screen_name
## 1                          √áaƒürƒ± Mert Bakƒ±rcƒ±   cagrimbakirci
## 2                               Elisabeth Bik MicrobiomDigest
## 3                                       ipbes           IPBES
## 4  Dr. Earyn McGee, Lizard lassoer \U0001f98e     Afro_Herper
## 5                                    figshare        figshare
## 6                       Nathan Peiffer-Smadja     nathanpsmad
## 7                           Wildlife Alliance  WildlifeRescue
## 8                          Arthur Charpentier freakonometrics
## 9                          L&#39;√©cologie au CNRS       INEE_CNRS
## 10            The Royal Botanic Garden Sydney       RBGSydney
## 11                                      CREAF  CREAF_ecologia
## 12                          Jean-Paul Carrera    JeanCarrPaul
## 13                                    Deckset      decksetapp
## 14                           ùô≥ùöûùöå-ùöÄùöûùöäùöóùöê ùôΩùöêùöûùö¢ùöéùöó          duc_qn
## 15              Christophe Josset  \U0001f4dd         CJosset
## 16                                 Casey Fung    TheCaseyFung
##                       location followers_count
## 1                   Texas, USA          170164
## 2            San Francisco, CA          105825
## 3                Bonn, Germany           78086
## 4                United States           45215
## 5                       London           40610
## 6               Paris - London           36760
## 7         Phnom Penh, Cambodia           34843
## 8             Montr√©al, Qu√©bec           28038
## 9                                        23483
## 10     Sydney, New South Wales           20259
## 11      Bellaterra (Barcelona)           16654
## 12             Oxford, England            5744
## 13                    Your Mac            3884
## 14       Lausanne, Switzerland            2808
## 15               Paris, France            2002
## 16 Bundjalung Country | Mullum            1612
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How old are my followers, where age is defined as the time elapsed since they created their Twitter account. Twitter was created in 2006. I created my account in July 2016 but started using it only two or three years ago I&amp;rsquo;d say. This dataviz is not too much informative I guess.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;details_followers %&amp;gt;% 
  mutate(month = created_at %&amp;gt;% str_split(&amp;quot; &amp;quot;) %&amp;gt;% map_chr(2),
         day = created_at %&amp;gt;% str_split(&amp;quot; &amp;quot;) %&amp;gt;% map_chr(3),
         year = created_at %&amp;gt;% str_split(&amp;quot; &amp;quot;) %&amp;gt;% map_chr(6),
         created_at = paste0(month,&amp;quot;-&amp;quot;,year),
         created_at = lubridate::my(created_at)) %&amp;gt;%
  group_by(created_at) %&amp;gt;%
  summarise(counts = n()) %&amp;gt;%
  ggplot(aes(x = created_at, y = counts)) +
  geom_line() +
  labs(x = &amp;quot;date of creation&amp;quot;, y = &amp;quot;how many accounts were created this month-year&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/twitter-social-network/unnamed-chunk-15-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;What is the distribution of the number of followers and friends. On average, 400 followers and 400 friends.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;foll &amp;lt;- details_followers %&amp;gt;%
  ggplot() + 
  aes(x = log(followers_count)) + 
  geom_histogram(fill=&amp;quot;#69b3a2&amp;quot;, color=&amp;quot;#e9ecef&amp;quot;, alpha=0.9) + 
  labs(x = &amp;quot;log number of followers&amp;quot;, y = &amp;quot;counts&amp;quot;, title = &amp;quot;followers&amp;quot;)

friends &amp;lt;- details_followers %&amp;gt;%
  ggplot() + 
  aes(x = log(friends_count)) + 
  geom_histogram(fill=&amp;quot;#69b3a2&amp;quot;, color=&amp;quot;#e9ecef&amp;quot;, alpha=0.9) + 
  labs(x = &amp;quot;log number of friends&amp;quot;, y = &amp;quot;&amp;quot;, title = &amp;quot;friends&amp;quot;)

library(patchwork)
foll + friends
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/twitter-social-network/unnamed-chunk-16-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;h2 id=&#34;retrieving-social-links&#34;&gt;Retrieving social links&lt;/h2&gt;
&lt;p&gt;To investigate the social network made of my followers, we need to gather who they follow, and their followers. Unfortunately, the Twitter API imposes some time and quantity constraints in retrieving data.&lt;/p&gt;
&lt;p&gt;Here, for the sake of illustration, I will focus on some accounts only. To do so, I consider accounts with between 1000 and 1500 followers, and more than 30000 tweets users have liked in their account&amp;rsquo;s lifetime (which denotes some activity). The more followers we consider, the more time it would take to gather info on them: it is approx one minute per follower on average, so one hour for sixty followers, and more than eighty hours for all my followers! With the filters I&amp;rsquo;m using, I end up with 35 followers here, this is very few to say infer meaningful about a network, but it&amp;rsquo;ll do the job for now.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;some_followers &amp;lt;- details_followers %&amp;gt;% 
  filter((followers_count &amp;gt; 1000 &amp;amp; followers_count &amp;lt; 1500),
         favourites_count &amp;gt; 30000) # %&amp;gt;% nrow()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create empty list and name it after their screen name.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;foler &amp;lt;- vector(mode = &#39;list&#39;, length = length(some_followers$screen_name))
names(foler) &amp;lt;- some_followers$screen_name
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Get followers of these selected followers. Takes ages, so save and load later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for (i in 1:length(some_followers$screen_name)) {
  message(&amp;quot;Getting followers for user #&amp;quot;, i, &amp;quot;/&amp;quot;, nrow(some_followers))
  foler[[i]] &amp;lt;- get_followers(some_followers$screen_name[i], 
                              n = some_followers$followers_count[i], 
                              retryonratelimit = TRUE)
  if(i %% 5 == 0){
    message(&amp;quot;sleep for 5 minutes&amp;quot;)
    Sys.sleep(5*60)
    }
}
save(foler, file = &amp;quot;foler.RData&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Format the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(&amp;quot;dat/foler.RData&amp;quot;)
folerx &amp;lt;- bind_rows(foler, .id = &amp;quot;screen_name&amp;quot;)
active_fol_x &amp;lt;- some_followers %&amp;gt;% select(id_str, screen_name)
foler_join &amp;lt;- left_join(folerx, some_followers, by = &amp;quot;screen_name&amp;quot;)
algo_follower &amp;lt;- foler_join %&amp;gt;% 
  select(id_str, screen_name) %&amp;gt;%
  setNames(c(&amp;quot;follower&amp;quot;, &amp;quot;active_user&amp;quot;)) %&amp;gt;% 
  na.omit()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Get friends of my followers. Takes ages. Again I save and load later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;friend &amp;lt;- data.frame()
for (i in seq_along(some_followers$screen_name)) {
  message(&amp;quot;Getting following for user #&amp;quot;, i ,&amp;quot;/&amp;quot;,nrow(some_followers))
  kk &amp;lt;- get_friends(some_followers$screen_name[i],
                    n = some_followers$friends_count[i],
                    retryonratelimit = TRUE)
  friend &amp;lt;- rbind(friend, kk)
  if(i %% 15 == 0){
    message(&amp;quot;sleep for 15 minutes&amp;quot;)
    Sys.sleep(15*60+1)
  }
}
save(friend, file = &amp;quot;friend.RData&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Format the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(&amp;quot;dat/friend.RData&amp;quot;)
all_friend &amp;lt;- friend %&amp;gt;% setNames(c(&amp;quot;screen_name&amp;quot;, &amp;quot;user_id&amp;quot;))
all_friendx &amp;lt;- left_join(all_friend, active_fol_x, by=&amp;quot;screen_name&amp;quot;)
algo_friend &amp;lt;- all_friendx %&amp;gt;% select(user_id, screen_name) %&amp;gt;%
  setNames(c(&amp;quot;following&amp;quot;,&amp;quot;active_user&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have all info on followers and friends, we&amp;rsquo;re gonna build the network of people who follow each other.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;un_active &amp;lt;- unique(algo_friend$active_user) %&amp;gt;% 
  data.frame(stringsAsFactors = F) %&amp;gt;%
  setNames(&amp;quot;active_user&amp;quot;)
algo_mutual &amp;lt;- data.frame()
for (i in seq_along(un_active$active_user)){
  aa &amp;lt;- algo_friend %&amp;gt;% 
    filter(active_user == un_active$active_user[i])
  bb &amp;lt;- aa %&amp;gt;% filter(aa$following %in% algo_follower$follower) %&amp;gt;%
    setNames(c(&amp;quot;mutual&amp;quot;,&amp;quot;active_user&amp;quot;))
  
  algo_mutual &amp;lt;- rbind(algo_mutual,bb)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of ids, we use screen names.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;detail_friend &amp;lt;- lookup_users(algo_mutual$mutual)
algo_mutual &amp;lt;- algo_mutual %&amp;gt;% 
  left_join(detail_friend, by = c(&amp;quot;mutual&amp;quot; = &amp;quot;id_str&amp;quot;)) %&amp;gt;% 
  na.omit() %&amp;gt;%
  select(mutual, active_user, screen_name)
algo_mutual
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18 √ó 3
##    mutual             active_user     screen_name    
##    &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;          
##  1 824265743575306243 g33k5p34k       robbie_emmet   
##  2 761735501707567104 g33k5p34k       LauraBlissEco  
##  3 1439272374         alexnicolharper LeafyEricScott 
##  4 824265743575306243 alexnicolharper robbie_emmet   
##  5 21467726           alexnicolharper g33k5p34k      
##  6 4181949437         LegalizeBrain   EvpokPadding   
##  7 824265743575306243 Souzam7139      robbie_emmet   
##  8 888890201673580548 Souzam7139      mellenmartin   
##  9 824265743575306243 mellenmartin    robbie_emmet   
## 10 21467726           LauraBlissEco   g33k5p34k      
## 11 831398755920445440 JosiahParry     RoelandtN42    
## 12 4181949437         gau             EvpokPadding   
## 13 21467726           robbie_emmet    g33k5p34k      
## 14 4159201575         robbie_emmet    alexnicolharper
## 15 888890201673580548 robbie_emmet    mellenmartin   
## 16 29538964           EvpokPadding    gau            
## 17 1439272374         lifedispersing  LeafyEricScott 
## 18 301687349          RoelandtN42     JosiahParry
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add my account to the network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;un_active &amp;lt;- un_active %&amp;gt;% 
  mutate(mutual = rep(&amp;quot;oaggimenez&amp;quot;))
un_active &amp;lt;- un_active[,c(2,1)]
un_active &amp;lt;- un_active %&amp;gt;% 
  setNames(c(&amp;quot;active_user&amp;quot;,&amp;quot;screen_name&amp;quot;))
algo_mutual &amp;lt;- bind_rows(algo_mutual %&amp;gt;% select(-mutual), un_active)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For what follows, we will need packages to work with networks.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(igraph)
library(tidygraph)
library(ggraph)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we create the edges, nodes and build the network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nodes &amp;lt;- data.frame(V = unique(c(algo_mutual$screen_name,algo_mutual$active_user)),
                    stringsAsFactors = F)

edges &amp;lt;- algo_mutual %&amp;gt;% 
  setNames(c(&amp;quot;from&amp;quot;,&amp;quot;to&amp;quot;))
network_ego1 &amp;lt;- graph_from_data_frame(d = edges, vertices = nodes, directed = F) %&amp;gt;%
  as_tbl_graph()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;network-metrics&#34;&gt;Network metrics&lt;/h2&gt;
&lt;p&gt;Create communities using &lt;code&gt;group_louvain()&lt;/code&gt; algorithm, and calculate standard metrics using &lt;code&gt;tidygraph&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(123)
network_ego1 &amp;lt;- network_ego1 %&amp;gt;% 
  activate(nodes) %&amp;gt;% 
  mutate(community = as.factor(group_louvain())) %&amp;gt;%
  mutate(degree_c = centrality_degree()) %&amp;gt;%
  mutate(betweenness_c = centrality_betweenness(directed = F,normalized = T)) %&amp;gt;%
  mutate(closeness_c = centrality_closeness(normalized = T)) %&amp;gt;%
  mutate(eigen = centrality_eigen(directed = F))
network_ego_df &amp;lt;- as.data.frame(network_ego1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Identify key nodes with respect to network metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;network_ego_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               name community degree_c betweenness_c closeness_c     eigen
## 1     robbie_emmet         2        8  0.0049299720   0.5384615 0.5238676
## 2    LauraBlissEco         2        3  0.0000000000   0.5147059 0.2717415
## 3   LeafyEricScott         5        3  0.0008403361   0.5223881 0.2287709
## 4        g33k5p34k         2        6  0.0024649860   0.5303030 0.4342056
## 5     EvpokPadding         3        4  0.0011204482   0.5223881 0.2336052
## 6     mellenmartin         2        4  0.0000000000   0.5223881 0.3371889
## 7      RoelandtN42         4        3  0.0000000000   0.5147059 0.2050991
## 8  alexnicolharper         2        5  0.0019607843   0.5303030 0.3942456
## 9              gau         3        3  0.0000000000   0.5147059 0.2133909
## 10     JosiahParry         4        3  0.0000000000   0.5147059 0.2050991
## 11     genius_c137         1        1  0.0000000000   0.5072464 0.1454399
## 12            Zjbb         1        1  0.0000000000   0.5072464 0.1454399
## 13       SJRAfloat         1        1  0.0000000000   0.5072464 0.1454399
## 14 BMPARMA17622540         1        1  0.0000000000   0.5072464 0.1454399
## 15    maryam_adeli         1        1  0.0000000000   0.5072464 0.1454399
## 16       waywardaf         1        1  0.0000000000   0.5072464 0.1454399
## 17         SusyVF6         1        1  0.0000000000   0.5072464 0.1454399
## 18       Sinalo_NM         1        1  0.0000000000   0.5072464 0.1454399
## 19      Leila_Lula         1        1  0.0000000000   0.5072464 0.1454399
## 20       CARThorpe         1        1  0.0000000000   0.5072464 0.1454399
## 21  j_wilson_white         1        1  0.0000000000   0.5072464 0.1454399
## 22   LegalizeBrain         3        2  0.0000000000   0.5147059 0.1794154
## 23    JaishriJuice         1        1  0.0000000000   0.5072464 0.1454399
## 24     jaguaretepy         1        1  0.0000000000   0.5072464 0.1454399
## 25      Souzam7139         2        3  0.0000000000   0.5223881 0.2706719
## 26        LaLince_         1        1  0.0000000000   0.5072464 0.1454399
## 27      Nina_Ella_         1        1  0.0000000000   0.5072464 0.1454399
## 28          samcox         1        1  0.0000000000   0.5072464 0.1454399
## 29     TAdamsBio42         1        1  0.0000000000   0.5072464 0.1454399
## 30       brubakerl         1        1  0.0000000000   0.5072464 0.1454399
## 31         robanhk         1        1  0.0000000000   0.5072464 0.1454399
## 32       InesCCarv         1        1  0.0000000000   0.5072464 0.1454399
## 33   groundhog0202         1        1  0.0000000000   0.5072464 0.1454399
## 34         zenmart         1        1  0.0000000000   0.5072464 0.1454399
## 35  lifedispersing         5        2  0.0000000000   0.5147059 0.1787123
## 36      oaggimenez         1       35  0.9685154062   1.0000000 1.0000000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Get users with highest values of each metric.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;kp_ego &amp;lt;- data.frame(
  network_ego_df %&amp;gt;% arrange(-degree_c) %&amp;gt;% select(name),
  network_ego_df %&amp;gt;% arrange(-betweenness_c) %&amp;gt;% select(name),
  network_ego_df %&amp;gt;% arrange(-closeness_c) %&amp;gt;% select(name),
  network_ego_df %&amp;gt;% arrange(-eigen) %&amp;gt;% select(name)) %&amp;gt;% 
  setNames(c(&amp;quot;degree&amp;quot;,&amp;quot;betweenness&amp;quot;,&amp;quot;closeness&amp;quot;,&amp;quot;eigen&amp;quot;))
kp_ego[-1,]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             degree     betweenness       closeness           eigen
## 2     robbie_emmet    robbie_emmet    robbie_emmet    robbie_emmet
## 3        g33k5p34k       g33k5p34k       g33k5p34k       g33k5p34k
## 4  alexnicolharper alexnicolharper alexnicolharper alexnicolharper
## 5     EvpokPadding    EvpokPadding  LeafyEricScott    mellenmartin
## 6     mellenmartin  LeafyEricScott    EvpokPadding   LauraBlissEco
## 7    LauraBlissEco   LauraBlissEco    mellenmartin      Souzam7139
## 8   LeafyEricScott    mellenmartin      Souzam7139    EvpokPadding
## 9      RoelandtN42     RoelandtN42   LauraBlissEco  LeafyEricScott
## 10             gau             gau     RoelandtN42             gau
## 11     JosiahParry     JosiahParry             gau     RoelandtN42
## 12      Souzam7139     genius_c137     JosiahParry     JosiahParry
## 13   LegalizeBrain            Zjbb   LegalizeBrain   LegalizeBrain
## 14  lifedispersing       SJRAfloat  lifedispersing  lifedispersing
## 15     genius_c137 BMPARMA17622540     genius_c137         robanhk
## 16            Zjbb    maryam_adeli            Zjbb       Sinalo_NM
## 17       SJRAfloat       waywardaf       SJRAfloat          samcox
## 18 BMPARMA17622540         SusyVF6 BMPARMA17622540     genius_c137
## 19    maryam_adeli       Sinalo_NM    maryam_adeli            Zjbb
## 20       waywardaf      Leila_Lula       waywardaf       SJRAfloat
## 21         SusyVF6       CARThorpe         SusyVF6 BMPARMA17622540
## 22       Sinalo_NM  j_wilson_white       Sinalo_NM       waywardaf
## 23      Leila_Lula   LegalizeBrain      Leila_Lula         SusyVF6
## 24       CARThorpe    JaishriJuice       CARThorpe      Leila_Lula
## 25  j_wilson_white     jaguaretepy  j_wilson_white  j_wilson_white
## 26    JaishriJuice      Souzam7139    JaishriJuice    JaishriJuice
## 27     jaguaretepy        LaLince_     jaguaretepy     jaguaretepy
## 28        LaLince_      Nina_Ella_        LaLince_        LaLince_
## 29      Nina_Ella_          samcox      Nina_Ella_      Nina_Ella_
## 30          samcox     TAdamsBio42          samcox       brubakerl
## 31     TAdamsBio42       brubakerl     TAdamsBio42   groundhog0202
## 32       brubakerl         robanhk       brubakerl         zenmart
## 33         robanhk       InesCCarv         robanhk    maryam_adeli
## 34       InesCCarv   groundhog0202       InesCCarv       CARThorpe
## 35   groundhog0202         zenmart   groundhog0202       InesCCarv
## 36         zenmart  lifedispersing         zenmart     TAdamsBio42
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Robbie Emmet has the highest degree, betweenness, closeness centrality and eigenvector centrality. He has the most relations with the other nodes in the network. He also can spread information the further away and faster than anyone, and is also surrounded by important persons in the network. This is within the network we&amp;rsquo;ve just built.&lt;/p&gt;
&lt;p&gt;By the way, Robbie has just passed his dissertation defense, congrats! Follow him on Twitter at @robbie_emmet, he is awesome!&lt;/p&gt;
&lt;h2 id=&#34;visualize-network&#34;&gt;Visualize Network&lt;/h2&gt;
&lt;p&gt;We visualize the network using clusters or communities (first three only).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;options(ggrepel.max.overlaps = Inf)
network_viz &amp;lt;- network_ego1 %&amp;gt;%
  filter(community %in% 1:3) %&amp;gt;%
  mutate(node_size = ifelse(degree_c &amp;gt;= 50,degree_c,0)) %&amp;gt;%
  mutate(node_label = ifelse(betweenness_c &amp;gt;= 0.01,name,NA))
plot_ego &amp;lt;- network_viz %&amp;gt;% 
  ggraph(layout = &amp;quot;stress&amp;quot;) +
  geom_edge_fan(alpha = 0.05) +
  geom_node_point(aes(color = as.factor(community),size = node_size)) +
  geom_node_label(aes(label = node_label),nudge_y = 0.1,
                 show.legend = F, fontface = &amp;quot;bold&amp;quot;, fill = &amp;quot;#ffffff66&amp;quot;) +
  theme_graph() + 
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;Top 3 communities&amp;quot;)
plot_ego
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/twitter-social-network/unnamed-chunk-31-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use an alternative dataviz to identify the nodes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;network_viz %&amp;gt;% 
  mutate(community = group_spinglass()) %&amp;gt;%
  ggraph(layout = &amp;quot;nicely&amp;quot;) +
  geom_edge_fan(alpha = 0.25) +
  geom_node_point(aes(color = factor(community)),size = 5, show.legend = F) +
  geom_node_text(aes(label = name),repel = T) +
  theme_graph() + theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;Network with named nodes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/blog/twitter-social-network/unnamed-chunk-32-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;h2 id=&#34;concluding-words&#34;&gt;Concluding words&lt;/h2&gt;
&lt;p&gt;Not sure what the communities are about. Due to limitation with the Twitter API I did not attempt to retrieve the info on all the 5,000 followers, but I used only 35 of them, which explains why there is not much to say about this network I guess. I might take some time to try and retrieve all the info (will take days), or not. I might also use some information I already have, like how followers describe themselves and do some topic modelling to identify common themes of interest.&lt;/p&gt;
&lt;p&gt;Here I have simply followed the steps that Joe Cristian illustrated so brillantly in his post at 
&lt;a href=&#34;https://algotech.netlify.app/blog/social-network-analysis-in-r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://algotech.netlify.app/blog/social-network-analysis-in-r/&lt;/a&gt;. Make sure you read his post for more details on the code and theory. In particular, he illustrates how information spreads through the network.&lt;/p&gt;
&lt;p&gt;For more about social network analyses and Twitter, see this thread 
&lt;a href=&#34;https://twitter.com/Mehdi_Moussaid/status/1389174715990876160?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://twitter.com/Mehdi_Moussaid/status/1389174715990876160?s=20&lt;/a&gt; and this video 
&lt;a href=&#34;https://www.youtube.com/watch?v=UX7YQ6m2r_o&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=UX7YQ6m2r_o&lt;/a&gt; in French, this is what inspired me to do the analysis. See also this 
&lt;a href=&#34;https://github.com/eleurent/twitter-graph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/eleurent/twitter-graph&lt;/a&gt; for a much better analysis than what I could do (with Python for data retrieving and manipulation, and 
&lt;a href=&#34;https://gephi.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gephi&lt;/a&gt; for network visualisation).&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;This post was also published on 
&lt;a href=&#34;https://www.r-bloggers.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.r-bloggers.com&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Workshop on reproducible science</title>
      <link>https://oliviergimenez.github.io/blog/reproscience2020/</link>
      <pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/reproscience2020/</guid>
      <description>&lt;p&gt;Workshop to come on reproducible science in our lab.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üßë‚Äçüî¨üíª‚ôªÔ∏è Planning a 1-day workshop on reproducible science for our lab &lt;a href=&#34;https://twitter.com/cefemontpellier?ref_src=twsrc%5Etfw&#34;&gt;@cefemontpellier&lt;/a&gt;. We&amp;#39;ll use R and RStudio, Git, GitHub, R Markdown, tidyverse and sf ü§©&lt;br&gt;&lt;br&gt;Material here &lt;a href=&#34;https://t.co/QApKQsl1FU&#34;&gt;https://t.co/QApKQsl1FU&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ReproducibleScience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ReproducibleScience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/twitthair1?ref_src=twsrc%5Etfw&#34;&gt;@twitthair1&lt;/a&gt; &lt;br&gt;&lt;br&gt;pix by &lt;a href=&#34;https://twitter.com/scriberian?ref_src=twsrc%5Etfw&#34;&gt;@scriberian&lt;/a&gt; &lt;a href=&#34;https://t.co/LFBwZmelit&#34;&gt;pic.twitter.com/LFBwZmelit&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1339896916109373440?ref_src=twsrc%5Etfw&#34;&gt;December 18, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;</description>
    </item>
    
    <item>
      <title>Some random piece of code</title>
      <link>https://oliviergimenez.github.io/blog/pieceofcode/</link>
      <pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/pieceofcode/</guid>
      <description>&lt;p&gt;Gathered some code on occupancy, capture-recapture &amp;amp; epidemiological models, social networks, spatial stuff, textual analyses, reproducible science, etc&amp;hellip;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üë©‚Äçüíªüë®‚Äçüíª In hope it&amp;#39;s useful, I gathered random pieces of code I wrote on occupancy, capture-recapture &amp;amp; epidemiological models, social networks, spatial stuff, textual analyses, &lt;a href=&#34;https://twitter.com/hashtag/DeepLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DeepLearning&lt;/a&gt; and the &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; ‚û°Ô∏è &lt;a href=&#34;https://t.co/T1PlbmDLZO&#34;&gt;https://t.co/T1PlbmDLZO&lt;/a&gt; I also went full purple &lt;a href=&#34;https://twitter.com/MoonbeamLevels?ref_src=twsrc%5Etfw&#34;&gt;@MoonbeamLevels&lt;/a&gt; üíú &lt;a href=&#34;https://t.co/q6vK4LYVS7&#34;&gt;pic.twitter.com/q6vK4LYVS7&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1291749139596955650?ref_src=twsrc%5Etfw&#34;&gt;August 7, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;</description>
    </item>
    
    <item>
      <title>Statistical ecology</title>
      <link>https://oliviergimenez.github.io/my-project/external-project/methods/</link>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/my-project/external-project/methods/</guid>
      <description>&lt;p&gt;Trained in mathematics and statistics in particular, I have developed a bottomless taste for questions in 
&lt;a href=&#34;http://rsbl.royalsocietypublishing.org/content/10/12/20140698&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statistical ecology&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I often resort to 
&lt;a href=&#34;https://oliviergimenez.github.io/publication/books/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;state-space models&lt;/a&gt;, and hidden Markov models (e.g., 
&lt;a href=&#34;https://oliviergimenez.github.io/pubs/Gimenezetal2014.pdf&#34;&gt;here&lt;/a&gt; and 
&lt;a href=&#34;https://oliviergimenez.github.io/pubs/Gimenezetal2012TPB.pdf&#34;&gt;here&lt;/a&gt;) in particular, to develop statistical methods for capture-recapture, occupancy and integrated population models.&lt;/p&gt;
&lt;p&gt;In our group, we address questions in conservation biology to assess species viability, in ecology to study the effect of climate change on animal demography, in evolution to examine life-history tactics and individual heterogeneity and in wildlife management to develop strategies for conservation and conflict mitigation. To account for the human dimension inherent to these questions, I went back to the university to study sociology. A nice illustration of this interdisciplinary work is 
&lt;a href=&#34;https://tel.archives-ouvertes.fr/tel-01834575/document&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gilles Maurer&amp;rsquo;s PhD&lt;/a&gt; combining anthropology, demography, economics and genetics to better understand the interactions between captive and wild population of Asian elephants.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m doing my best to teach statistics (including the Bayesian way) to the students of the 
&lt;a href=&#34;https://www.masters-biologie-ecologie.com/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ecology and Evolution Master in Montpellier&lt;/a&gt;. We also make efforts to structure the community in France through our 
&lt;a href=&#34;https://sites.google.com/site/gdrecostat/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;national research group in statistical ecology&lt;/a&gt;. Last but not least, every now and then, our group runs 
&lt;a href=&#34;https://oliviergimenez.github.io/talks/workshop/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;workshops&lt;/a&gt; to diffuse quantitative methods in the ecological community.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interactive visualisation of bias in occupancy models</title>
      <link>https://oliviergimenez.github.io/blog/interactiveocc/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/interactiveocc/</guid>
      <description>&lt;p&gt;Interactive data visualisation of bias in occupancy models w/ flexdashboard.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Interactive &lt;a href=&#34;https://twitter.com/hashtag/dataviz?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dataviz&lt;/a&gt; of bias in occupancy models w/ &lt;a href=&#34;https://twitter.com/hashtag/flexdashboard?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#flexdashboard&lt;/a&gt; (aka &lt;a href=&#34;https://twitter.com/hashtag/shinyapps?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#shinyapps&lt;/a&gt; for the rest of us) &lt;a href=&#34;https://t.co/fkYiHfAaEU&#34;&gt;https://t.co/fkYiHfAaEU&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/rstudio?ref_src=twsrc%5Etfw&#34;&gt;@rstudio&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/unmarked?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#unmarked&lt;/a&gt; &lt;a href=&#34;https://t.co/KKGTPE8eJh&#34;&gt;https://t.co/KKGTPE8eJh&lt;/a&gt; &lt;a href=&#34;https://t.co/Rstpdv8CqJ&#34;&gt;pic.twitter.com/Rstpdv8CqJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1254806087108354048?ref_src=twsrc%5Etfw&#34;&gt;April 27,020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;</description>
    </item>
    
    <item>
      <title>Introduction to spatial analyses in R</title>
      <link>https://oliviergimenez.github.io/blog/intro_spatial/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/intro_spatial/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;fr&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üë©
üíªüó∫Ô∏èüë®
üíª The slides of my introduction to &lt;a href=&#34;https://twitter.com/hashtag/GIS?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#GIS&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/hashtag/mapping?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#mapping&lt;/a&gt; in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; using the &lt;a href=&#34;https://twitter.com/hashtag/sf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#sf&lt;/a&gt; üì¶ and brown üêª distribution in the &lt;a href=&#34;https://twitter.com/hashtag/pyrenees?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#pyrenees&lt;/a&gt; as a case study &lt;a href=&#34;https://t.co/SKQOCzbxHn&#34;&gt;https://t.co/SKQOCzbxHn&lt;/a&gt; - raw material on &lt;a href=&#34;https://twitter.com/hashtag/github?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#github&lt;/a&gt; &lt;a href=&#34;https://t.co/dHoMz6I2Kp&#34;&gt;https://t.co/dHoMz6I2Kp&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rspatial?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rspatial&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/spatial?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#spatial&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ggplot2?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ggplot2&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/dataviz?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dataviz&lt;/a&gt; &lt;a href=&#34;https://t.co/22eD1Y55d3&#34;&gt;pic.twitter.com/22eD1Y55d3&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1093882504560414721?ref_src=twsrc%5Etfw&#34;&gt;8 f√©vrier 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script asyncc=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Procrastination...</title>
      <link>https://oliviergimenez.github.io/blog/procrastination/</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/procrastination/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;fr&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Procrastination at its highest level üòú My first attempt to design hex stickers for our &lt;a href=&#34;https://twitter.com/hashtag/R2ucare?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#R2ucare&lt;/a&gt; üì¶ &lt;a href=&#34;https://t.co/ZbclwJKB5W&#34;&gt;https://t.co/ZbclwJKB5W&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; code on &lt;a href=&#34;https://twitter.com/hashtag/github?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#github&lt;/a&gt; &lt;a href=&#34;https://t.co/TbEgjQf7iC&#34;&gt;https://t.co/TbEgjQf7iC&lt;/a&gt; Comments more than welcome üòÅ &lt;a href=&#34;https://t.co/vVdVnJ9B79&#34;&gt;pic.twitter.com/vVdVnJ9B79&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1093076050656063488?ref_src=twsrc%5Etfw&#34;&gt;6 f√©vrier 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;sync src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to the Tidyverse</title>
      <link>https://oliviergimenez.github.io/blog/intro_tidyverse/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/intro_tidyverse/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;fr&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My introduction to the &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; for our lab meeting to manipulate and visualise data in &lt;a href=&#34;https://twitter.com/hashtag/rstat?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstat&lt;/a&gt; &lt;a href=&#34;https://t.co/As9bkXY9GZ&#34;&gt;https://t.co/As9bkXY9GZ&lt;/a&gt;. Feel free to steal and modify this material for your own use. Be advised, this is work in progress &amp;amp; a mix of üá¨üáß/üá´üá∑üòã Comments welcome! &lt;a href=&#34;https://twitter.com/hashtag/datascience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#datascience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/davaviz?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#davaviz&lt;/a&gt; &lt;a href=&#34;https://t.co/2vrrdbzuWh&#34;&gt;pic.twitter.com/2vrrdbzuWh&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1085492126404751360?ref_src=twsrc%5Etfw&#34;&gt;16 janvier 2019&lt;/a&gt;&lt;/blockote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>New paper!</title>
      <link>https://oliviergimenez.github.io/blog/articlenina/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/articlenina/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;fr&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üê¨üá¨üá∑üáÆüáπüá´üá∑ New paper by &lt;a href=&#34;https://twitter.com/NSantostasi?ref_src=twsrc%5Etfw&#34;&gt;@NSantostasi&lt;/a&gt; &lt;a href=&#34;https://twitter.com/INEE_CNRS?ref_src=twsrc%5Etfw&#34;&gt;@INEE_CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_OccitaniE?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_OccitaniE&lt;/a&gt; &lt;a href=&#34;https://twitter.com/IsiteMUSE?ref_src=twsrc%5Etfw&#34;&gt;@IsiteMUSE&lt;/a&gt; &lt;a href=&#34;https://twitter.com/umontpellier?ref_src=twsrc%5Etfw&#34;&gt;@umontpellier&lt;/a&gt; ü§©üëè &lt;a href=&#34;https://t.co/6vQ6d9HevV&#34;&gt;https://t.co/6vQ6d9HevV&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üí§ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1060966359348269058?ref_src=twsrc%5Etfw&#34;&gt;9 novembre 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Workshop on spatio-temporal models with INLA</title>
      <link>https://oliviergimenez.github.io/blog/inla_workshop/</link>
      <pubDate>Tue, 06 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/inla_workshop/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;fr&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/INLA?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#INLA&lt;/a&gt; workshop on spatio-temporal models in the beautiful city of &lt;a href=&#34;https://twitter.com/hashtag/Avignon?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Avignon&lt;/a&gt; ü§© &lt;a href=&#34;https://twitter.com/hashtag/RESSTE?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#RESSTE&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/GdREcoStat?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#GdREcoStat&lt;/a&gt; &lt;a href=&#34;https://twitter.com/oksanagrente?ref_src=twsrc%5Etfw&#34;&gt;@oksanagrente&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CREEM_cake?ref_src=twsrc%5Etfw&#34;&gt;@CREEM_cake&lt;/a&gt; &lt;a href=&#34;https://t.co/nuLhIkMiwO&#34;&gt;pic.twitter.com/nuLhIkMiwO&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üí§ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1060085610793394177?ref_src=twsrc%5Etfw&#34;&gt;718&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Crash course on individual-based models using NetLogoR</title>
      <link>https://oliviergimenez.github.io/blog/crashcourse_netlogor/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/crashcourse_netlogor/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;fr&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The famous &lt;a href=&#34;https://twitter.com/hashtag/NetLogo?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#NetLogo&lt;/a&gt; butterfly example coded in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; w/ &lt;a href=&#34;https://twitter.com/hashtag/NetLogoR?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#NetLogoR&lt;/a&gt; &lt;a href=&#34;https://t.co/VbUUa5vIep&#34;&gt;pic.twitter.com/VbUUa5vIep&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üö∏ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1059445943689510913?ref_src=twsrc%5Etfw&#34;&gt;5 novembre 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;fr&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Wolf model from Marucco &amp;amp; &lt;a href=&#34;https://twitter.com/eliotmcintire?ref_src=twsrc%5Etfw&#34;&gt;@eliotmcintire&lt;/a&gt; coded in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; w/ &lt;a href=&#34;https://twitter.com/hashtag/NetLogoR?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#NetLogoR&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/hashtag/SpaDES?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#SpaDES&lt;/a&gt; &lt;a href=&#34;https://t.co/ERx71CIgZv&#34;&gt;https://t.co/ERx71CIgZv&lt;/a&gt; &lt;a href=&#34;https://t.co/f0YTHuKGSS&#34;&gt;pic.twitter.com/f0YTHuKGSS&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üö∏ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1059450575660687361?ref_src=twsrc%5Etfw&#34;&gt;5 novembre 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Running OpenBUGS in parallel</title>
      <link>https://oliviergimenez.github.io/blog/run_openbugs_parallel/</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/run_openbugs_parallel/</guid>
      <description>&lt;img style=&#34;float:left;margin-right:10px;margin-top:10px;width:200px;margin-bottom:5px&#34; src=&#34;https://oliviergimenez.github.io/img/parcomp.jpg&#34;&gt; 
Recently, I have been using `OpenBUGS` for some analyses that `JAGS` cannot do. However, `JAGS` can be run in parallel through [the `jagsUI` package](https://github.com/kenkellner/jagsUI), which can save you some precious time. So the question is how to run several chains in parallel with `OpenBUGS`. 
&lt;p&gt;Well, first you&amp;rsquo;ll need to install &lt;code&gt;OpenBUGS&lt;/code&gt; (if you&amp;rsquo;re on a Mac, check out 
&lt;a href=&#34;https://oliviergimenez.github.io/post/run_openbugs_on_mac/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this short tutorial&lt;/a&gt;). Then, you&amp;rsquo;ll need to run &lt;code&gt;OpenBUGS&lt;/code&gt; from &lt;code&gt;R&lt;/code&gt; through the pacage &lt;code&gt;R2OpenBUGS&lt;/code&gt;, which you can install via:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(!require(R2OpenBUGS)) install.packages(&amp;quot;R2OpenBUGS&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: R2OpenBUGS
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;standard-analysis&#34;&gt;Standard analysis&lt;/h2&gt;
&lt;p&gt;Now let&amp;rsquo;s run the classical &lt;code&gt;BUGS&lt;/code&gt; &lt;code&gt;school&lt;/code&gt; example:&lt;/p&gt;
&lt;p&gt;Load the &lt;code&gt;OpenBUGS&lt;/code&gt; Package&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(R2OpenBUGS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load the data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(schools)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the model, write it to a text file and have a look&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nummodel &amp;lt;- function(){
for (j in 1:J){
  y[j] ~ dnorm (theta[j], tau.y[j])
  theta[j] ~ dnorm (mu.theta, tau.theta)
  tau.y[j] &amp;lt;- pow(sigma.y[j], -2)}
mu.theta ~ dnorm (0.0, 1.0E-6)
tau.theta &amp;lt;- pow(sigma.theta, -2)
sigma.theta ~ dunif (0, 1000)
}
write.model(nummodel, &amp;quot;nummodel.txt&amp;quot;)
model.file1 = paste(getwd(),&amp;quot;nummodel.txt&amp;quot;, sep=&amp;quot;/&amp;quot;)
file.show(&amp;quot;nummodel.txt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare the data for input into OpenBUGS&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;J &amp;lt;- nrow(schools)
y &amp;lt;- schools$estimate
sigma.y &amp;lt;- schools$sd
data &amp;lt;- list (&amp;quot;J&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;sigma.y&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initialization of variables&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;inits &amp;lt;- function(){
  list(theta = rnorm(J, 0, 100), mu.theta = rnorm(1, 0, 100), sigma.theta = runif(1, 0, 100))}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Set the &lt;code&gt;Wine&lt;/code&gt; working directory and the directory to &lt;code&gt;OpenBUGS&lt;/code&gt;, and change the OpenBUGS.exe location as necessary:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;WINE=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/wine&amp;quot;
WINEPATH=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/winepath&amp;quot;
OpenBUGS.pgm=&amp;quot;/Applications/OpenBUGS323/OpenBUGS.exe&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The are the parameters to save&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parameters = c(&amp;quot;theta&amp;quot;, &amp;quot;mu.theta&amp;quot;, &amp;quot;sigma.theta&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the model&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ptm &amp;lt;- proc.time()
schools.sim &amp;lt;- bugs(data, inits, model.file = model.file1,parameters=parameters,n.chains = 2, n.iter = 500000, n.burnin = 10000, OpenBUGS.pgm=OpenBUGS.pgm, WINE=WINE, WINEPATH=WINEPATH,useWINE=T)
elapsed_time &amp;lt;- proc.time() - ptm
elapsed_time 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##  50.835   2.053  55.010
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;print(schools.sim)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Bugs model at &amp;quot;/Users/oliviergimenez/Desktop/nummodel.txt&amp;quot;, 
## Current: 2 chains, each with 5e+05 iterations (first 10000 discarded)
## Cumulative: n.sims = 980000 iterations saved
##             mean  sd  2.5%  25%  50%  75% 97.5% Rhat  n.eff
## theta[1]    11.6 8.4  -1.9  6.1 10.5 15.8  32.0    1 370000
## theta[2]     8.0 6.4  -4.8  4.0  8.0 12.0  20.9    1  67000
## theta[3]     6.4 7.8 -11.3  2.2  6.8 11.2  20.9    1  55000
## theta[4]     7.7 6.6  -5.7  3.7  7.8 11.8  20.9    1  70000
## theta[5]     5.5 6.5  -8.8  1.6  5.9  9.8  17.1    1  26000
## theta[6]     6.2 6.9  -8.9  2.3  6.6 10.7  18.9    1  23000
## theta[7]    10.7 6.9  -1.4  6.0 10.1 14.7  26.2    1 480000
## theta[8]     8.7 7.9  -6.8  4.0  8.4 13.0  25.7    1  76000
## mu.theta     8.1 5.3  -2.0  4.7  8.1 11.4  18.5    1  30000
## sigma.theta  6.6 5.7   0.2  2.5  5.2  9.1  20.9    1  12000
## deviance    60.5 2.2  57.0 59.1 60.1 61.4  66.0    1 980000
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = Dbar-Dhat)
## pD = 2.8 and DIC = 63.2
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;parallel-computations&#34;&gt;Parallel computations&lt;/h2&gt;
&lt;p&gt;To run several chains in parallel, we&amp;rsquo;ll follow the steps described in 
&lt;a href=&#34;http://www.petrkeil.com/?p=63&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this nice post&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# loading packages
library(snow)
library(snowfall)

# setting the number of CPUs to be 2
sfInit(parallel=TRUE, cpus=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in searchCommandline(parallel, cpus = cpus, type
## = type, socketHosts = socketHosts, : Unknown option on
## commandline: rmarkdown::render(&#39;/Users/oliviergimenez/Desktop/
## run_openbugs_in_parallel.Rmd&#39;,~+~~+~encoding~+~
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R Version:  R version 3.4.3 (2017-11-30)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## snowfall 1.84-6.1 initialized (using snow 0.4-2): parallel execution on 2 CPUs.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# and assigning the R2OpenBUGS library to each CPU
sfLibrary(R2OpenBUGS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Library R2OpenBUGS loaded.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Library R2OpenBUGS loaded in cluster.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# create list of data
J &amp;lt;- nrow(schools)
y &amp;lt;- schools$estimate
sigma.y &amp;lt;- schools$sd
x.data &amp;lt;- list (J=J, y=y, sigma.y=sigma.y)

# creating separate directory for each CPU process
folder1 &amp;lt;- paste(getwd(), &amp;quot;/chain1&amp;quot;, sep=&amp;quot;&amp;quot;)
folder2 &amp;lt;- paste(getwd(), &amp;quot;/chain2&amp;quot;, sep=&amp;quot;&amp;quot;)
dir.create(folder1); dir.create(folder2); 
 
# sinking the model into a file in each directory
for (folder in c(folder1, folder2))
{
  sink(paste(folder, &amp;quot;/nummodel.txt&amp;quot;, sep=&amp;quot;&amp;quot;))
cat(&amp;quot;
	model{
for (j in 1:J){
  y[j] ~ dnorm (theta[j], tau.y[j])
  theta[j] ~ dnorm (mu.theta, tau.theta)
  tau.y[j] &amp;lt;- pow(sigma.y[j], -2)}
mu.theta ~ dnorm (0.0, 1.0E-6)
tau.theta &amp;lt;- pow(sigma.theta, -2)
sigma.theta ~ dunif (0, 1000)
	}
&amp;quot;)
  sink()
}
 
# defining the function that will run MCMC on each CPU
# Arguments:
# chain - will be 1 or 2
# x.data - the data list
# params - parameters to be monitored
parallel.bugs &amp;lt;- function(chain, x.data, params)
{
  # a. defining directory for each CPU
  sub.folder &amp;lt;- paste(getwd(),&amp;quot;/chain&amp;quot;, chain, sep=&amp;quot;&amp;quot;)
 
  # b. specifying the initial MCMC values
  inits &amp;lt;- function()list(theta = rnorm(x.data$J, 0, 100), mu.theta = rnorm(1, 0, 100), sigma.theta = runif(1, 0, 100))
 
  # c. calling OpenBugs
  # (you may need to change the OpenBUGS.pgm directory)
  # je suis sous Mac, je fais tourner OpenBUGS via Wine
  bugs(data=x.data, inits=inits, parameters.to.save=params,
             n.iter = 500000, n.burnin = 10000, n.chains=1,
             model.file=&amp;quot;nummodel.txt&amp;quot;, debug=FALSE, codaPkg=TRUE,
             useWINE=TRUE, OpenBUGS.pgm = &amp;quot;/Applications/OpenBUGS323/OpenBUGS.exe&amp;quot;,
             working.directory = sub.folder,
             WINE=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/wine&amp;quot;, 
             WINEPATH=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/winepath&amp;quot;)
}
 
# setting the parameters to be monitored
params &amp;lt;- c(&amp;quot;theta&amp;quot;, &amp;quot;mu.theta&amp;quot;, &amp;quot;sigma.theta&amp;quot;)
 
# calling the sfLapply function that will run
# parallel.bugs on each of the 2 CPUs
ptm &amp;lt;- proc.time()
sfLapply(1:2, fun=parallel.bugs, x.data=x.data, params=params)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;/Users/oliviergimenez/Desktop/chain1/CODAchain1.txt&amp;quot;
## 
## [[2]]
## [1] &amp;quot;/Users/oliviergimenez/Desktop/chain2/CODAchain1.txt&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;elapsed_time = proc.time() - ptm
elapsed_time
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.013   0.000  32.157
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# locating position of each CODA chain
chain1 &amp;lt;- paste(folder1, &amp;quot;/CODAchain1.txt&amp;quot;, sep=&amp;quot;&amp;quot;)
chain2 &amp;lt;- paste(folder2, &amp;quot;/CODAchain1.txt&amp;quot;, sep=&amp;quot;&amp;quot;)
 
# and, finally, getting the results
res &amp;lt;- read.bugs(c(chain1, chain2))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Abstracting deviance ... 490000 valid values
## Abstracting mu.theta ... 490000 valid values
## Abstracting sigma.theta ... 490000 valid values
## Abstracting theta[1] ... 490000 valid values
## Abstracting theta[2] ... 490000 valid values
## Abstracting theta[3] ... 490000 valid values
## Abstracting theta[4] ... 490000 valid values
## Abstracting theta[5] ... 490000 valid values
## Abstracting theta[6] ... 490000 valid values
## Abstracting theta[7] ... 490000 valid values
## Abstracting theta[8] ... 490000 valid values
## Abstracting deviance ... 490000 valid values
## Abstracting mu.theta ... 490000 valid values
## Abstracting sigma.theta ... 490000 valid values
## Abstracting theta[1] ... 490000 valid values
## Abstracting theta[2] ... 490000 valid values
## Abstracting theta[3] ... 490000 valid values
## Abstracting theta[4] ... 490000 valid values
## Abstracting theta[5] ... 490000 valid values
## Abstracting theta[6] ... 490000 valid values
## Abstracting theta[7] ... 490000 valid values
## Abstracting theta[8] ... 490000 valid values
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(res)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Iterations = 10001:5e+05
## Thinning interval = 1 
## Number of chains = 2 
## Sample size per chain = 490000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##               Mean    SD Naive SE Time-series SE
## deviance    60.453 2.221 0.002243       0.005737
## mu.theta     8.109 5.261 0.005315       0.020596
## sigma.theta  6.610 5.682 0.005740       0.027336
## theta[1]    11.697 8.407 0.008493       0.027974
## theta[2]     8.023 6.395 0.006460       0.019264
## theta[3]     6.365 7.866 0.007946       0.022485
## theta[4]     7.735 6.601 0.006668       0.019766
## theta[5]     5.467 6.504 0.006570       0.022580
## theta[6]     6.234 6.885 0.006955       0.021332
## theta[7]    10.727 6.891 0.006961       0.023304
## theta[8]     8.648 7.892 0.007972       0.021541
## 
## 2. Quantiles for each variable:
## 
##                 2.5%    25%    50%    75% 97.5%
## deviance     57.0200 59.120 60.040 61.430 65.99
## mu.theta     -2.0600  4.784  8.066 11.410 18.50
## sigma.theta   0.2275  2.456  5.275  9.190 20.82
## theta[1]     -1.8850  6.195 10.560 15.880 32.04
## theta[2]     -4.8350  4.049  8.004 11.980 20.96
## theta[3]    -11.4800  2.194  6.871 11.170 20.90
## theta[4]     -5.7500  3.711  7.784 11.820 20.92
## theta[5]     -8.8490  1.603  5.938  9.834 17.14
## theta[6]     -8.8940  2.255  6.632 10.680 18.95
## theta[7]     -1.3450  6.125 10.140 14.680 26.27
## theta[8]     -6.8910  4.037  8.409 12.960 25.72
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Run OpenBUGS on a Mac</title>
      <link>https://oliviergimenez.github.io/blog/run_openbugs_on_mac/</link>
      <pubDate>Sat, 13 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/run_openbugs_on_mac/</guid>
      <description>&lt;img style=&#34;float:left;margin-right:10px;margin-top:10px;width:200px;margin-bottom:5px&#34; src=&#34;https://oliviergimenez.github.io/img/bugs.jpg&#34;&gt; 
I had to use the good old `OpenBUGS` for some analyses that cannot be done in `JAGS`. Below are the steps to install `OpenBUGS` then to run it from your Mac either natively or from `R`. This tutorial is an adaptation of [this post](https://sites.google.com/site/mmeclimate/-bayesmet/openbugs-on-mac-os-x) and [that one](http://www.davideagle.org/r-2/bayesian-modeling-using-winbugs-and-openbugs/running-openbugs-on-mac-using-wine). 
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If not done already, install 
&lt;a href=&#34;https://brew.sh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Homebrew&lt;/a&gt;. This program will make the installation of any other programs on your Mac so easy!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install 
&lt;a href=&#34;https://www.winehq.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wine&lt;/a&gt; which will allow you to run any Windows programs (.exe) on your Mac. To do so, start by 
&lt;a href=&#34;http://blog.teamtreehouse.com/introduction-to-the-mac-os-x-command-line&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;opening Terminal&lt;/a&gt;, then type in the command: &lt;em&gt;brew install wine&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, download the Windows version of &lt;code&gt;OpenBUGS&lt;/code&gt; 
&lt;a href=&#34;https://www.mrc-bsu.cam.ac.uk/training/short-courses/bayescourse/download/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To install &lt;code&gt;OpenBUGS&lt;/code&gt;, still in Terminal, go to the directory where the file was downloaded and type (you might need to unzip the file you downloaded first): &lt;em&gt;wine OpenBUGS323setup.exe&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;OpenBUGS&lt;/code&gt; is now installed and ready to be used! You can run it by first going to the directory where &lt;code&gt;OpenBUGS&lt;/code&gt; was installed. On my laptop, it can be achieved via the command: &lt;em&gt;cd /Applications/OpenBUGS323&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then, you just need to tye in the following command in the Terminal, and you should see an OpenBUGS windows poping up: &lt;em&gt;wine OpenBUGS&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now we would like to run &lt;code&gt;OpenBUGS&lt;/code&gt; from &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;Install the package &lt;code&gt;R2OpenBUGS&lt;/code&gt; by typing in the &lt;code&gt;R&lt;/code&gt; console:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(!require(R2OpenBUGS)) install.packages(&amp;quot;R2OpenBUGS&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: R2OpenBUGS
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Now let&amp;rsquo;s see whether everything works well by running the classical &lt;code&gt;BUGS&lt;/code&gt; &lt;code&gt;school&lt;/code&gt; example:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Load the &lt;code&gt;OpenBUGS&lt;/code&gt; Package&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(R2OpenBUGS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load the data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(schools)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the model, write it to a text file and have a look&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nummodel &amp;lt;- function(){
for (j in 1:J){
  y[j] ~ dnorm (theta[j], tau.y[j])
  theta[j] ~ dnorm (mu.theta, tau.theta)
  tau.y[j] &amp;lt;- pow(sigma.y[j], -2)}
mu.theta ~ dnorm (0.0, 1.0E-6)
tau.theta &amp;lt;- pow(sigma.theta, -2)
sigma.theta ~ dunif (0, 1000)
}
write.model(nummodel, &amp;quot;nummodel.txt&amp;quot;)
model.file1 = paste(getwd(),&amp;quot;nummodel.txt&amp;quot;, sep=&amp;quot;/&amp;quot;)
file.show(&amp;quot;nummodel.txt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare the data for input into OpenBUGS&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;J &amp;lt;- nrow(schools)
y &amp;lt;- schools$estimate
sigma.y &amp;lt;- schools$sd
data &amp;lt;- list (&amp;quot;J&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;sigma.y&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initialization of variables&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;inits &amp;lt;- function(){
  list(theta = rnorm(J, 0, 100), mu.theta = rnorm(1, 0, 100), sigma.theta = runif(1, 0, 100))}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Set the &lt;code&gt;Wine&lt;/code&gt; working directory and the directory to &lt;code&gt;OpenBUGS&lt;/code&gt;, and change the OpenBUGS.exe location as necessary:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;WINE=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/wine&amp;quot;
WINEPATH=&amp;quot;/usr/local/Cellar/wine/2.0.4/bin/winepath&amp;quot;
OpenBUGS.pgm=&amp;quot;/Applications/OpenBUGS323/OpenBUGS.exe&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The are the parameters to save&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parameters = c(&amp;quot;theta&amp;quot;, &amp;quot;mu.theta&amp;quot;, &amp;quot;sigma.theta&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the model&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;schools.sim &amp;lt;- bugs(data, inits, model.file = model.file1,parameters=parameters,n.chains = 3, n.iter = 1000, OpenBUGS.pgm=OpenBUGS.pgm, WINE=WINE, WINEPATH=WINEPATH,useWINE=T)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; will pause. You might get a weird message starting by err:ole, just ignore it. When the run is complete, a prompt will reappear, then just type the following command to get the result:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;print(schools.sim)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Bugs model at &amp;quot;/Users/oliviergimenez/Desktop/nummodel.txt&amp;quot;, 
## Current: 3 chains, each with 1000 iterations (first 500 discarded)
## Cumulative: n.sims = 1500 iterations saved
##             mean  sd 2.5%  25%  50%  75% 97.5% Rhat n.eff
## theta[1]    12.2 7.9 -1.3  7.5 11.2 16.4  32.1  1.0    62
## theta[2]     9.1 6.5 -4.0  5.1  9.4 13.2  21.4  1.0   150
## theta[3]     7.8 7.7 -9.4  3.6  8.5 12.6  21.1  1.0   360
## theta[4]     8.8 6.6 -4.5  4.5  9.2 13.3  20.4  1.0   110
## theta[5]     6.8 6.9 -8.2  2.3  7.5 11.4  17.7  1.0   410
## theta[6]     7.3 7.2 -8.6  2.7  8.2 11.8  18.9  1.0   190
## theta[7]    11.5 6.4 -0.3  7.5 11.2 15.7  25.0  1.1    42
## theta[8]     9.7 7.6 -4.7  5.1  9.6 14.4  25.1  1.0   130
## mu.theta     9.2 5.2 -1.2  5.8  9.3 12.5  18.2  1.0    88
## sigma.theta  5.9 5.6  0.2  1.7  4.4  8.5  20.2  1.1    51
## deviance    60.7 2.2 57.2 59.2 60.1 61.9  65.6  1.0   120
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = Dbar-Dhat)
## pD = 2.8 and DIC = 63.4
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When run natively, &lt;code&gt;WinBUGS&lt;/code&gt; and &lt;code&gt;OpenBUGS&lt;/code&gt; have nice debugging capabilities; also, you can see what is going on, I mean the program reading the data, generating inits, and so on. To get the &lt;code&gt;OpenBUGS&lt;/code&gt; window with a bunch of useful info, just add &lt;code&gt;debug=T&lt;/code&gt; to the call of the &lt;code&gt;bugs&lt;/code&gt; function, and re-run the model&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;schools.sim &amp;lt;- bugs(data, inits, model.file = model.file1,parameters=parameters,n.chains = 3, n.iter = 1000, OpenBUGS.pgm=OpenBUGS.pgm, WINE=WINE, WINEPATH=WINEPATH,useWINE=T,debug=T)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## arguments &#39;show.output.on.console&#39;, &#39;minimized&#39; and &#39;invisible&#39; are for Windows only
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will have to close the &lt;code&gt;OpenBUGS&lt;/code&gt; window to get the prompt back.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Simulating data with JAGS</title>
      <link>https://oliviergimenez.github.io/blog/sim_with_jags/</link>
      <pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/sim_with_jags/</guid>
      <description>&lt;img style=&#34;float:left;margin-right:10px;margin-top:10px;width:200px;margin-bottom:5px&#34; src=&#34;https://oliviergimenez.github.io/img/posterior_plots_lr.png&#34;&gt;
Here, I illustrate the possibility to use `JAGS` to simulate data with two examples that might be of interest to population ecologists: first a linear regression, second a Cormack-Jolly-Seber capture-recapture model to estimate animal survival (formulated as a state-space model). The code is available from [GitHub](https://github.com/oliviergimenez/simul_with_jags).
&lt;p&gt;Recently, I have been struggling with simulating data from complex hierarchical models. After several unsuccessful attempts in &lt;code&gt;R&lt;/code&gt;, I remembered the good old times when I was using &lt;code&gt;WinBUGS&lt;/code&gt; (more than 10 years already!) and the possibility to simulate data with it. I&amp;rsquo;m using &lt;code&gt;JAGS&lt;/code&gt; now, and a quick search in Google with &amp;lsquo;simulating data with jags&amp;rsquo; led me to 
&lt;a href=&#34;https://www.georg-hosoya.de/wordpress/?p=799&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a complex example&lt;/a&gt; and 
&lt;a href=&#34;https://stackoverflow.com/questions/38295839/simulate-data-in-jags-r2jags&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a simple example&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Simulating data with &lt;code&gt;JAGS&lt;/code&gt; is convenient because you can use (almost) the same code for simulation and inference, and you can carry out simulation studies (bias, precision, interval coverage) in the same environment (namely &lt;code&gt;JAGS&lt;/code&gt;).&lt;/p&gt;
&lt;h2 id=&#34;linear-regression-example&#34;&gt;Linear regression example&lt;/h2&gt;
&lt;p&gt;We first load the packages we need for this tutorial:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(R2jags)
library(runjags)
library(mcmcplots)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then straight to the point, let&amp;rsquo;s generate data from a linear regression model. The trick is to use a &lt;code&gt;data&lt;/code&gt; block, have the simplest &lt;code&gt;model&lt;/code&gt; block you could think of and pass the parameters as if they were data. Note that it&amp;rsquo;d be possible to use only a model block, see comment 
&lt;a href=&#34;https://stackoverflow.com/questions/38295839/simulate-data-in-jags-r2jags&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;txtstring &amp;lt;- &#39;
data{
# Likelihood:
for (i in 1:N){
y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
mu[i] &amp;lt;- alpha + beta * x[i]
}
}
model{
fake &amp;lt;- 0
}
&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;beta&lt;/code&gt; are the intercept and slope, &lt;code&gt;tau&lt;/code&gt; the precision or the inverse of the variance, &lt;code&gt;y&lt;/code&gt; the response variable and &lt;code&gt;x&lt;/code&gt; the explanatory variable.&lt;/p&gt;
&lt;p&gt;We pick some values for the model parameters that we will use as data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# parameters for simulations 
N = 30 # nb of observations
x &amp;lt;- 1:N # predictor
alpha = 0.5 # intercept
beta = 1 # slope
sigma &amp;lt;- .1 # residual sd
tau &amp;lt;- 1/(sigma*sigma) # precision
# parameters are treated as data for the simulation step
data&amp;lt;-list(N=N,x=x,alpha=alpha,beta=beta,tau=tau)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now call &lt;code&gt;JAGS&lt;/code&gt;; note that we monitor the response variable instead of parameters as we would do when conducting standard inference:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# run jags
out &amp;lt;- run.jags(txtstring, data = data,monitor=c(&amp;quot;y&amp;quot;),sample=1, n.chains=1, summarise=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling rjags model...
## Calling the simulation using the rjags method...
## Note: the model did not require adaptation
## Burning in the model for 4000 iterations...
## Running the model for 1 iterations...
## Simulation complete
## Finished running the simulation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is a bit messy and needs to be formatted appropriately:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# reformat the outputs
Simulated &amp;lt;- coda::as.mcmc(out)
Simulated
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Markov Chain Monte Carlo (MCMC) output:
## Start = 5001 
## End = 5001 
## Thinning interval = 1 
##          y[1]    y[2]    y[3]     y[4]     y[5]     y[6]     y[7]     y[8]
## 5001 1.288399 2.52408 3.61516 4.583587 5.600675 6.566052 7.593407 8.457497
##         y[9]    y[10]   y[11]    y[12]    y[13]    y[14]    y[15]    y[16]
## 5001 9.70847 10.38035 11.5105 12.55048 13.49143 14.46356 15.45641 16.56148
##         y[17]    y[18]    y[19]    y[20]    y[21]   y[22]    y[23]
## 5001 17.50935 18.51501 19.66197 20.49477 21.57079 22.6199 23.48232
##         y[24]    y[25]    y[26]    y[27]    y[28]    y[29]    y[30]
## 5001 24.57923 25.47368 26.33674 27.46525 28.35525 29.60279 30.42952
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Simulated)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1 30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat = as.vector(Simulated)
dat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1.288399  2.524080  3.615160  4.583587  5.600675  6.566052  7.593407
##  [8]  8.457497  9.708470 10.380351 11.510500 12.550482 13.491435 14.463564
## [15] 15.456410 16.561483 17.509350 18.515005 19.661969 20.494767 21.570790
## [22] 22.619899 23.482317 24.579228 25.473676 26.336736 27.465251 28.355248
## [29] 29.602791 30.429517
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s fit the model we used to simulate to the data we just generated. I won&amp;rsquo;t go into the details and assume that the reader is familiar with &lt;code&gt;JAGS&lt;/code&gt; and linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# specify model in BUGS language
model &amp;lt;- 	
paste(&amp;quot;	
model {
# Likelihood:
for (i in 1:N){
y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
mu[i] &amp;lt;- alpha + beta * x[i]
}
# Priors:
alpha ~ dnorm(0, 0.01) # intercept
beta ~ dnorm(0, 0.01) # slope
sigma ~ dunif(0, 100) # standard deviation
tau &amp;lt;- 1 / (sigma * sigma) 
}
&amp;quot;)
writeLines(model,&amp;quot;lin_reg.jags&amp;quot;)	

# data
jags.data &amp;lt;- list(y = dat, N = length(dat), x = x)

# initial values
inits &amp;lt;- function(){list(alpha = rnorm(1), beta = rnorm(1), sigma = runif(1,0,10))}  

# parameters monitored
parameters &amp;lt;- c(&amp;quot;alpha&amp;quot;, &amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;)

# MCMC settings
ni &amp;lt;- 10000
nt &amp;lt;- 6
nb &amp;lt;- 5000
nc &amp;lt;- 2

# call JAGS from R
res &amp;lt;- jags(jags.data, inits, parameters, &amp;quot;lin_reg.jags&amp;quot;, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb, working.directory = getwd())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## module glm loaded
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 30
##    Unobserved stochastic nodes: 3
##    Total graph size: 130
## 
## Initializing model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look to the results and compare with the parameters we used to simulate the data (see above):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# summarize posteriors
print(res, digits = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Bugs model at &amp;quot;lin_reg.jags&amp;quot;, fit using jags,
##  2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 6
##  n.sims = 1668 iterations saved
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat
## alpha      0.544   0.038   0.469   0.518   0.545   0.570   0.617 1.000
## beta       0.998   0.002   0.994   0.997   0.998   1.000   1.003 1.001
## sigma      0.102   0.015   0.078   0.091   0.100   0.110   0.138 1.002
## deviance -53.810   2.724 -56.867 -55.808 -54.516 -52.641 -46.676 1.001
##          n.eff
## alpha     1700
## beta      1700
## sigma      780
## deviance  1700
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 3.7 and DIC = -50.1
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty close!&lt;/p&gt;
&lt;p&gt;Check convergence:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# trace plots
traplot(res,c(&amp;quot;alpha&amp;quot;, &amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/unnamed-chunk-8-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Plot the posterior distribution of the regression parameters and residual standard deviation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# posterior distributions
denplot(res,c(&amp;quot;alpha&amp;quot;, &amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/unnamed-chunk-9-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;h2 id=&#34;capture-recapture-example&#34;&gt;Capture-recapture example&lt;/h2&gt;
&lt;p&gt;I now illustrate the use of &lt;code&gt;JAGS&lt;/code&gt; to simulate data from a Cormack-Jolly-Seber model with constant survival and recapture probabilities. I assume that the reader is familiar with this model and its formulation as a state-space model.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;txtstring &amp;lt;- &#39;
data{
# Constant survival and recapture probabilities
for (i in 1:nind){
   for (t in f[i]:(n.occasions-1)){
      phi[i,t] &amp;lt;- mean.phi
      p[i,t] &amp;lt;- mean.p
      } #t
   } #i
# Likelihood 
for (i in 1:nind){
   # Define latent state and obs at first capture
   z[i,f[i]] &amp;lt;- 1
   mu2[i,1] &amp;lt;- 1 * z[i,f[i]] # detection is 1 at first capture (&amp;quot;conditional on first capture&amp;quot;)
   y[i,1] ~ dbern(mu2[i,1])
   # then deal w/ subsequent occasions
   for (t in (f[i]+1):n.occasions){
      # State process
      z[i,t] ~ dbern(mu1[i,t])
      mu1[i,t] &amp;lt;- phi[i,t-1] * z[i,t-1]
      # Observation process
      y[i,t] ~ dbern(mu2[i,t])
      mu2[i,t] &amp;lt;- p[i,t-1] * z[i,t]
      } #t
   } #i
}
model{
fake &amp;lt;- 0
}
&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s pick some values for parameters and store them in a data list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# parameter for simulations 
n.occasions = 10 # nb of occasions
nind = 100 # nb of individuals
mean.phi &amp;lt;- 0.8 # survival
mean.p &amp;lt;- 0.6 # recapture
f = rep(1,nind) # date of first capture
data&amp;lt;-list(n.occasions = n.occasions, mean.phi = mean.phi, mean.p = mean.p, f = f, nind = nind)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now run &lt;code&gt;JAGS&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out &amp;lt;- run.jags(txtstring, data = data,monitor=c(&amp;quot;y&amp;quot;),sample=1, n.chains=1, summarise=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling rjags model...
## Calling the simulation using the rjags method...
## Note: the model did not require adaptation
## Burning in the model for 4000 iterations...
## Running the model for 1 iterations...
## Simulation complete
## Finished running the simulation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Format the output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Simulated &amp;lt;- coda::as.mcmc(out)
dim(Simulated)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]    1 1000
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat = matrix(Simulated,nrow=nind)
head(dat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    1    1    0    0    0    0    0    0    0     0
## [2,]    1    1    1    1    0    0    0    0    0     0
## [3,]    1    0    0    0    0    0    0    0    0     0
## [4,]    1    0    0    0    0    0    0    0    0     0
## [5,]    1    0    0    0    0    0    0    0    0     0
## [6,]    1    1    1    1    0    0    1    0    1     1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I monitored only the detections and non-detections, but it is also possible to get the simulated values for the states, i.e. whether an individual is alive or dead at each occasion. You just need to amend the call to &lt;code&gt;JAGS&lt;/code&gt; with &lt;code&gt;monitor=c(&amp;quot;y&amp;quot;,&amp;quot;x&amp;quot;)&lt;/code&gt; and to amend the output accordingly.&lt;/p&gt;
&lt;p&gt;Now we fit a Cormack-Jolly-Seber model to the data we&amp;rsquo;ve just simulated, assuming constant parameters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- 	
paste(&amp;quot;	
model {
# Priors and constraints
for (i in 1:nind){
   for (t in f[i]:(n.occasions-1)){
      phi[i,t] &amp;lt;- mean.phi
      p[i,t] &amp;lt;- mean.p
      } #t
   } #i
mean.phi ~ dunif(0, 1)         # Prior for mean survival
mean.p ~ dunif(0, 1)           # Prior for mean recapture
# Likelihood 
for (i in 1:nind){
   # Define latent state at first capture
   z[i,f[i]] &amp;lt;- 1
   for (t in (f[i]+1):n.occasions){
      # State process
      z[i,t] ~ dbern(mu1[i,t])
      mu1[i,t] &amp;lt;- phi[i,t-1] * z[i,t-1]
      # Observation process
      y[i,t] ~ dbern(mu2[i,t])
      mu2[i,t] &amp;lt;- p[i,t-1] * z[i,t]
      } #t
   } #i
}
&amp;quot;)
writeLines(model,&amp;quot;cjs.jags&amp;quot;)	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# vector with occasion of marking
get.first &amp;lt;- function(x) min(which(x!=0))
f &amp;lt;- apply(dat, 1, get.first)
# data
jags.data &amp;lt;- list(y = dat, f = f, nind = dim(dat)[1], n.occasions = dim(dat)[2])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Initial values
known.state.cjs &amp;lt;- function(ch){
   state &amp;lt;- ch
   for (i in 1:dim(ch)[1]){
      n1 &amp;lt;- min(which(ch[i,]==1))
      n2 &amp;lt;- max(which(ch[i,]==1))
      state[i,n1:n2] &amp;lt;- 1
      state[i,n1] &amp;lt;- NA
      }
   state[state==0] &amp;lt;- NA
   return(state)
   }
inits &amp;lt;- function(){list(mean.phi = runif(1, 0, 1), mean.p = runif(1, 0, 1), z = known.state.cjs(dat))}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;d like to carry out inference about survival and recapture probabilities:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parameters &amp;lt;- c(&amp;quot;mean.phi&amp;quot;, &amp;quot;mean.p&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Standard MCMC settings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ni &amp;lt;- 10000
nt &amp;lt;- 6
nb &amp;lt;- 5000
nc &amp;lt;- 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ready to run &lt;code&gt;JAGS&lt;/code&gt;!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Call JAGS from R (BRT 1 min)
cjs &amp;lt;- jags(jags.data, inits, parameters, &amp;quot;cjs.jags&amp;quot;, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb, working.directory = getwd())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 900
##    Unobserved stochastic nodes: 902
##    Total graph size: 3707
## 
## Initializing model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Summarize posteriors and compare to the values we used to simulate the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;print(cjs, digits = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Bugs model at &amp;quot;cjs.jags&amp;quot;, fit using jags,
##  2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 6
##  n.sims = 1668 iterations saved
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat
## mean.p     0.596   0.033   0.531   0.574   0.597   0.618   0.660 1.000
## mean.phi   0.784   0.021   0.742   0.770   0.785   0.799   0.824 1.001
## deviance 440.611  18.374 408.121 427.569 438.662 452.512 479.608 1.001
##          n.eff
## mean.p    1700
## mean.phi  1700
## deviance  1700
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 168.9 and DIC = 609.5
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again pretty close!&lt;/p&gt;
&lt;p&gt;Trace plots&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;traplot(cjs,c(&amp;quot;mean.phi&amp;quot;, &amp;quot;mean.p&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/unnamed-chunk-21-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Posterior distribution plots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;denplot(cjs,c(&amp;quot;mean.phi&amp;quot;, &amp;quot;mean.p&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/unnamed-chunk-22-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fitting dynamic occupancy models with TMB</title>
      <link>https://oliviergimenez.github.io/blog/occupancy_in_tmb/</link>
      <pubDate>Thu, 24 Aug 2017 12:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/occupancy_in_tmb/</guid>
      <description>&lt;img style=&#34;float:left;margin-right:10px;margin-top:10px;width:200px;margin-bottom:5px&#34; src=&#34;https://oliviergimenez.github.io/img/occ_darryl.png&#34;&gt;
Following my recent attempt to [fit a HMM model to capture-recapture data with TMB](https://oliviergimenez.github.io/post/multievent_in_tmb/) and the rather estonishing outcome (the code was &gt; 300 time faster than the equivalent R code!), I was curious to add TMB to the [list of options I tried to fit dynamic occupancy models](https://oliviergimenez.github.io/post/occupancy_in_admb/). Well, the least I can say is that TMB is fast, damn fast!
&lt;p&gt;The reasons for trying TMB were the same as before: TMB is said to be fast, allows for parallel computations, works with R, accomodates spatial stuff, allows easy implementation of random effects).&lt;/p&gt;
&lt;p&gt;I found materials on the internet to teach myself TMB, at least what I needed to implement a simple HMM model. See 
&lt;a href=&#34;http://seananderson.ca/2014/10/17/tmb.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for a linear regression and a Gompertz state space model examples, 
&lt;a href=&#34;https://www.youtube.com/watch?v=A5CLrhzNzVU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for the same linear regression example on Youtube (that&amp;rsquo;s awesome!) and many other examples 
&lt;a href=&#34;http://kaskr.github.io/adcomp/examples.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The R code is available on my GitHub 
&lt;a href=&#34;https://github.com/oliviergimenez/occupancy_tmb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. TMB was&amp;hellip; wait for it&amp;hellip; &amp;gt; 300 times faster than ADMB, &amp;gt; 140 times than Unmarked and &amp;gt; 6000 times faster than Jags (although the comparison with the latter is a bit unfair I suppose). The results are available 
&lt;a href=&#34;http://rpubs.com/ogimenez/301798&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m new to TMB, but I&amp;rsquo;m gonna definitely dig into it. Congrats to 
&lt;a href=&#34;https://github.com/kaskr/adcomp/graphs/contributors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the developers&lt;/a&gt;! Check out the 
&lt;a href=&#34;http://tmb-project.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TMB website&lt;/a&gt; as well as 
&lt;a href=&#34;https://www.jstatsoft.org/article/view/v070i05&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the paper&lt;/a&gt; that comes with it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fitting HMM/multievent capture-recapture models with TMB</title>
      <link>https://oliviergimenez.github.io/blog/multievent_in_tmb/</link>
      <pubDate>Mon, 21 Aug 2017 12:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/multievent_in_tmb/</guid>
      <description>&lt;img style=&#34;float:left;margin-right:10px;margin-top:10px;&#34; src=&#34;https://oliviergimenez.github.io/img/tmb.png&#34;&gt;
Following my attempts to fit a HMM model to [capture-recapture data with Rcpp](http://localhost:1313/post/multievent_in_rcpp/) and to [occupancy data with ADMB](http://localhost:1313/post/occupancy_in_admb/), a few colleagues suggested TMB as a potential alternative for several reasons (fast, allows for parallel computations, works with R, accomodates spatial stuff, easy implementation of random effects, and probably other reasons that I don&#39;t know).
&lt;p&gt;I found materials on the internet to teach myself TMB, at least what I needed to implement a simple HMM model. See 
&lt;a href=&#34;http://seananderson.ca/2014/10/17/tmb.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for a linear regression and a Gompertz state space model examples, 
&lt;a href=&#34;https://www.youtube.com/watch?v=A5CLrhzNzVU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for the same linear regression example on Youtube (that&amp;rsquo;s awesome!) and many other examples 
&lt;a href=&#34;http://kaskr.github.io/adcomp/examples.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. However, I got stuck and posted my desperate request for help on the 
&lt;a href=&#34;https://groups.google.com/forum/#!forum/tmb-users&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TMB forum&lt;/a&gt;. Guess what, I got an answer less than a few hours after - thank you Mollie Brooks!&lt;/p&gt;
&lt;p&gt;The R code is available on my GitHub 
&lt;a href=&#34;https://github.com/oliviergimenez/hmm_tmb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Minimizing the deviance coded with TMB was&amp;hellip; wait for it&amp;hellip; &amp;gt; 300 times faster than using the deviance coded in standard R.&lt;/p&gt;
&lt;p&gt;Hope this is useful.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fitting multievent capture-recapture models with Rcpp</title>
      <link>https://oliviergimenez.github.io/blog/multievent_in_rcpp/</link>
      <pubDate>Fri, 11 Aug 2017 12:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/multievent_in_rcpp/</guid>
      <description>&lt;img style=&#34;float:left;margin-right:10px;margin-top:10px;&#34; src=&#34;https://oliviergimenez.github.io/img/seamless.png&#34;&gt;
Following my previous post on [using ADMB to fit hidden Markov models](https://oliviergimenez.github.io/post/occupancy_in_admb/), I took some time to learn how to use Rcpp ([Eddelbuettel &amp; Francois 2011](https://www.jstatsoft.org/article/view/v040i08); [Eddelbuettel 2013](http://www.springer.com/us/book/9781461468677)), a package that gives friendly access to the power of C++ and increase the speed of your R programs. Kudos to Dirk Eddelbuettel, Romain Francois and their colleagues, Rcpp is awesome! 
&lt;p&gt;I started with the excellent 
&lt;a href=&#34;http://adv-r.had.co.nz/Rcpp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rcpp chapter&lt;/a&gt; in the 
&lt;a href=&#34;http://adv-r.had.co.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advanced R&lt;/a&gt; book by Hadley Wickham which I complemented with the various 
&lt;a href=&#34;https://cran.r-project.org/web/packages/Rcpp/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vignettes&lt;/a&gt; that come with the package. As always, I googled the problems I had and often ended up finding the solution on 
&lt;a href=&#34;https://stackoverflow.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stackoverflow&lt;/a&gt;. The 
&lt;a href=&#34;http://lists.r-forge.r-project.org/mailman/listinfo/rcpp-devel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rcpp-devel discussion list&lt;/a&gt; is the place where questions should be asked about Rcpp.&lt;/p&gt;
&lt;p&gt;My objective was to implement the likelihood of a relatively simple multievent capture-recapture model (
&lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1541-0420.2005.00318.x/abstract&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pradel 2005&lt;/a&gt;) with Rcpp. I recycled some R code I had and a dataset on shearwaters I used in a paper (
&lt;a href=&#34;https://dl.dropboxusercontent.com/u/23160641/my-pubs/Gimenezetal2012TPB.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gimenez et al. 2012&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The R code is available on my GitHub 
&lt;a href=&#34;https://github.com/oliviergimenez/multieventRcpp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. To run it, you just need to type Rcpp::sourceCpp(&amp;lsquo;multi event.cpp&amp;rsquo;) in the console. I&amp;rsquo;m convinced that the code can be improved, but this simple exercise showed that minimizing the deviance coded with Rcpp and calculating the Hessian was 10 times faster than using the deviance coded in standard R.&lt;/p&gt;
&lt;p&gt;Next steps will be to go for RcppArmadillo for matrix computations and RcppNumerical for optimisation (and numerical integration for random effects).&lt;/p&gt;
&lt;p&gt;Hope this is useful.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Following an advice from Romain Francois and Dirk Eddelbuettel (the Rcpp gurus), I have switched to 
&lt;a href=&#34;https://cran.r-project.org/web/packages/RcppArmadillo/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RcppArmadillo&lt;/a&gt; to rely on the code developed by professionals and decades of testing. Now the RcppArmadillo code is 50 times faster than basic R! The code is available on 
&lt;a href=&#34;https://github.com/oliviergimenez/multieventRcpp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my GitHub&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fitting occupancy models in ADMB</title>
      <link>https://oliviergimenez.github.io/blog/occupancy_in_admb/</link>
      <pubDate>Sun, 06 Aug 2017 12:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/occupancy_in_admb/</guid>
      <description>&lt;img style=&#34;float:left;margin-right:10px;margin-top:10px;margin-bottom:10px;&#34; src=&#34;https://oliviergimenez.github.io/img/admb.png&#34;&gt;
Some time ago, a student of mine got stuck when fitting dynamic occupancy models to real data in Jags because of the computational burden. 
&lt;p&gt;We had a dataset with several thousands sites, more than 20 seasons and 4 surveys per season (yeah!).&lt;/p&gt;
&lt;p&gt;We thought of using Unmarked instead (the likelihood is written in C++ and used through Rcpp), but dynamic models with false positives and/or random effects are not (yet?) implemented, and we were interested in considering both in our analysis. Some years ago, I had the opportunity to learn ADMB in a NCEAS meeting (thanks Hans Skaug!), I thought I would give it a try. ADMB allows you to write down any likelihood functions yourself and to incorporate random effects in an efficient way. It&amp;rsquo;s known to be fast for reasons I won&amp;rsquo;t go into here. Last but not least, ADMB can be run from R like JAGS and Unmarked (thanks Ben Bolker!).&lt;/p&gt;
&lt;p&gt;Here we go. I first simulate some data, then fit a dynamic model using ADMB, JAGS and Unmarked and finally perform a quick benchmarking. I&amp;rsquo;m going for a standard dynamic model, because the aims are i) to verify that JAGS is slower than Unmarked, ii) that ADMB is closer to Unmarked than JAGS in terms of time computation. If ii) is verified, then it will be worth the effort coding everything in ADMB.&lt;/p&gt;
&lt;p&gt;The results are available on RPub 
&lt;a href=&#34;http://rpubs.com/ogimenez/297167&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The code is available on GitHub 
&lt;a href=&#34;https://github.com/oliviergimenez/occupancy_in_ADMB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hope this is useful.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Working group on animal/human demography</title>
      <link>https://oliviergimenez.github.io/blog/working_group_demo/</link>
      <pubDate>Thu, 26 Jan 2017 12:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/working_group_demo/</guid>
      <description>&lt;p&gt;We organised a 2-day workshop on the metrics of longevity used in human and
animal demography. The idea is to explore potential bridges between the two fields.
Next meeting in April. The group is led by S. Cubaynes, a former PhD student of mine,
who now holds a lecturer position at Montpellier University.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Workshop on issues in fitting hierarchical models in ecology</title>
      <link>https://oliviergimenez.github.io/blog/working_group_hm/</link>
      <pubDate>Mon, 28 Nov 2016 12:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/working_group_hm/</guid>
      <description>&lt;p&gt;Frederic Gosselin, Etienne Rivot and I organised a 2-day workshop on issues in
fitting hierarchical models in ecology. I gave a talk on &amp;ldquo;Local minima and multistate
capture-recapture models&amp;rdquo;, slides and R code available on

&lt;a href=&#34;https://github.com/oliviergimenez/multistate_local_minima&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;iframe src=&#34;https://widgets.figshare.com/articles/4833524/embed?show_title=1&#34; width=&#34;568&#34; height=&#34;351&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;The idea is to consider several problematic case studies and explore the issues using
several computing platforms (R, Jags, Nimble, Stan, Admb). Next meeting in May.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Analysing the social Star Wars network in The Attack of the Clones with R</title>
      <link>https://oliviergimenez.github.io/blog/starwars_network/</link>
      <pubDate>Sun, 07 Aug 2016 12:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/starwars_network/</guid>
      <description>&lt;p&gt;This is a free adaptation of two (very) clever analyses made by others:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;http://evelinag.com/blog/2015/12-15-star-wars-social-network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Star Wars Social Network by Evelina Gabasov&lt;/a&gt; in which program F# was mostly used to analyse the Star wars social networks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;http://varianceexplained.org/r/love-actually-network/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Analyzing networks of characters in &amp;lsquo;Love Actually&amp;rsquo; by David Robinson&lt;/a&gt; in which R was used to analyse the links between the characters of the movie Love Actually.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The aim here is to try and reproduce Evelina&amp;rsquo;s analysis using R only, using David&amp;rsquo;s contribution plus several tweaks I found here and there on the internet. The R code and data are available on my 
&lt;a href=&#34;https://github.com/oliviergimenez/starwars_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Disclaimer&lt;/em&gt;: The original blog posts are awesome and full of relevant details, check them out! My objective here was to teach myself how to manipulate data using trendy R packages and do some network analyses. Some comments below have been copied and pasted from these blogs, the credits entirely go to the authors Evelina and David. Last but not least, my code comes with mistakes probably.&lt;/p&gt;
&lt;h1 id=&#34;read-and-format-data&#34;&gt;Read and format data&lt;/h1&gt;
&lt;p&gt;First, read in data. I found the movie script in doc format 
&lt;a href=&#34;theforce.net/timetales/ep2se.doc&#34;&gt;here&lt;/a&gt;, which I converted in txt format for convenience. Then, apply various treatments to have the data ready for analysis. I use the old school way for modifying the original dataframe. 
&lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Piping&lt;/a&gt; would have made the code more readable, but I do not feel confident with this approach yet.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# load convenient packages
library(dplyr)
library(stringr)
library(tidyr)

# read file line by line 
raw &amp;lt;- readLines(&amp;quot;attack-of-the-clones.txt&amp;quot;)

# create data frame
lines &amp;lt;- data_frame(raw = raw) 

# get rid of leading and trailing white spaces
# http://stackoverflow.com/questions/2261079/how-to-trim-leading-and-trailing-whitespace-in-r
trim &amp;lt;- function (x) gsub(&amp;quot;^\\s+|\\s+$&amp;quot;, &amp;quot;&amp;quot;, x)
lines &amp;lt;- mutate(lines,raw=trim(raw))

# get rid of the empty lines
lines2 &amp;lt;- filter(lines, raw != &amp;quot;&amp;quot;)

# detect scenes: begin by EXT. or INT.
lines3 &amp;lt;-  mutate(lines2, is_scene = str_detect(raw, &amp;quot;T.&amp;quot;),scene = cumsum(is_scene)) 

# drop lines that start with EXT. or INT.
lines4 &amp;lt;- filter(lines3,!is_scene)

# distinguish characters from what they say
lines5 &amp;lt;- separate(lines4, raw, c(&amp;quot;speaker&amp;quot;, &amp;quot;dialogue&amp;quot;), sep = &amp;quot;:&amp;quot;, fill = &amp;quot;left&amp;quot;,extra=&#39;drop&#39;)

# read in aliases (from Evelina&#39;s post)
aliases &amp;lt;- read.table(&#39;aliases.csv&#39;,sep=&#39;,&#39;,header=T,colClasses = &amp;quot;character&amp;quot;)
aliases$Alias
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;BEN&amp;quot;           &amp;quot;SEE-THREEPIO&amp;quot;  &amp;quot;THREEPIO&amp;quot;      &amp;quot;ARTOO-DETOO&amp;quot;  
##  [5] &amp;quot;ARTOO&amp;quot;         &amp;quot;PALPATINE&amp;quot;     &amp;quot;DARTH SIDIOUS&amp;quot; &amp;quot;BAIL&amp;quot;         
##  [9] &amp;quot;MACE&amp;quot;          &amp;quot;WINDU&amp;quot;         &amp;quot;MACE-WINDU&amp;quot;    &amp;quot;NUTE&amp;quot;         
## [13] &amp;quot;AUNT BERU&amp;quot;     &amp;quot;DOOKU&amp;quot;         &amp;quot;BOBA&amp;quot;          &amp;quot;JANGO&amp;quot;        
## [17] &amp;quot;PANAKA&amp;quot;        &amp;quot;NUTE&amp;quot;          &amp;quot;KI-ADI&amp;quot;        &amp;quot;BIBBLE&amp;quot;       
## [21] &amp;quot;BIB&amp;quot;           &amp;quot;CHEWIE&amp;quot;        &amp;quot;VADER&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;aliases$Name
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;OBI-WAN&amp;quot;        &amp;quot;C-3PO&amp;quot;          &amp;quot;C-3PO&amp;quot;          &amp;quot;R2-D2&amp;quot;         
##  [5] &amp;quot;R2-D2&amp;quot;          &amp;quot;EMPEROR&amp;quot;        &amp;quot;EMPEROR&amp;quot;        &amp;quot;BAIL ORGANA&amp;quot;   
##  [9] &amp;quot;MACE WINDU&amp;quot;     &amp;quot;MACE WINDU&amp;quot;     &amp;quot;MACE WINDU&amp;quot;     &amp;quot;NUTE GUNRAY&amp;quot;   
## [13] &amp;quot;BERU&amp;quot;           &amp;quot;COUNT DOOKU&amp;quot;    &amp;quot;BOBA FETT&amp;quot;      &amp;quot;JANGO FETT&amp;quot;    
## [17] &amp;quot;CAPTAIN PANAKA&amp;quot; &amp;quot;NUTE GUNRAY&amp;quot;    &amp;quot;KI-ADI-MUNDI&amp;quot;   &amp;quot;SIO BIBBLE&amp;quot;    
## [21] &amp;quot;BIB FORTUNA&amp;quot;    &amp;quot;CHEWBACCA&amp;quot;      &amp;quot;DARTH VADER&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# assign unique name to characters
# http://stackoverflow.com/questions/28593265/is-there-a-function-like-switch-which-works-inside-of-dplyrmutate
multipleReplace &amp;lt;- function(x, what, by) {
  stopifnot(length(what)==length(by))               
  ind &amp;lt;- match(x, what)
  ifelse(is.na(ind),x,by[ind])
}
lines6 &amp;lt;- mutate(lines5,speaker=multipleReplace(speaker,what=aliases$Alias,by=aliases$Name))

# read in actual names (from Evelina&#39;s post)
actual.names &amp;lt;- read.csv(&#39;characters.csv&#39;,header=F,colClasses = &amp;quot;character&amp;quot;)
actual.names &amp;lt;- c(as.matrix(actual.names))
# filter out non-characters
lines7 &amp;lt;- filter(lines6,speaker %in% actual.names)

# group by scene
lines8 &amp;lt;- group_by(lines7, scene, line = cumsum(!is.na(speaker))) 

lines9 &amp;lt;- summarize(lines8, speaker = speaker[1], dialogue = str_c(dialogue, collapse = &amp;quot; &amp;quot;))

# Count the lines-per-scene-per-character
# Turn the result into a binary speaker-by-scene matrix
by_speaker_scene &amp;lt;- count(lines9, scene, speaker)
by_speaker_scene
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 447 x 3
## # Groups:   scene [321]
##    scene    speaker     n
##    &amp;lt;int&amp;gt;      &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;
##  1    26      PADME     1
##  2    27      PADME     1
##  3    29      PADME     1
##  4    48      PADME     1
##  5    50      PADME     2
##  6    66 MACE WINDU     1
##  7    67 MACE WINDU     1
##  8    69       YODA     1
##  9    70 MACE WINDU     1
## 10    74       YODA     1
## # ... with 437 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(reshape2)
speaker_scene_matrix &amp;lt;-acast(by_speaker_scene , speaker ~ scene, fun.aggregate = length)
dim(speaker_scene_matrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  19 321
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;analyses&#34;&gt;Analyses&lt;/h1&gt;
&lt;h2 id=&#34;hierarchical-clustering&#34;&gt;Hierarchical clustering&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;norm &amp;lt;- speaker_scene_matrix / rowSums(speaker_scene_matrix)
h &amp;lt;- hclust(dist(norm, method = &amp;quot;manhattan&amp;quot;))
plot(h)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/starwars_network_files/figure-html/unnamed-chunk-2-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;p&gt;Use tree to give an ordering that puts similar characters close together&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ordering &amp;lt;- h$labels[h$order]
ordering
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;MACE WINDU&amp;quot;  &amp;quot;YODA&amp;quot;        &amp;quot;SHMI&amp;quot;        &amp;quot;QUI-GON&amp;quot;     &amp;quot;PLO KOON&amp;quot;   
##  [6] &amp;quot;LAMA SU&amp;quot;     &amp;quot;OBI-WAN&amp;quot;     &amp;quot;BAIL ORGANA&amp;quot; &amp;quot;JAR JAR&amp;quot;     &amp;quot;POGGLE&amp;quot;     
## [11] &amp;quot;ANAKIN&amp;quot;      &amp;quot;PADME&amp;quot;       &amp;quot;CLIEGG&amp;quot;      &amp;quot;BERU&amp;quot;        &amp;quot;OWEN&amp;quot;       
## [16] &amp;quot;SIO BIBBLE&amp;quot;  &amp;quot;RUWEE&amp;quot;       &amp;quot;JOBAL&amp;quot;       &amp;quot;SOLA&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This ordering can be used to make other graphs more informative. For instance, we can visualize a timeline of all scenes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;scenes &amp;lt;-  filter(by_speaker_scene, n() &amp;gt; 1) # scenes with &amp;gt; 1 character
scenes2 &amp;lt;- ungroup(scenes)
scenes3 &amp;lt;- mutate(scenes2, scene = as.numeric(factor(scene)),
           character = factor(speaker, levels = ordering))
library(ggplot2)
ggplot(scenes3, aes(scene, character)) +
    geom_point() +
    geom_path(aes(group = scene))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/starwars_network_files/figure-html/unnamed-chunk-4-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Create a cooccurence matrix (see 
&lt;a href=&#34;http://stackoverflow.com/questions/13281303/creating-co-occurrence-matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) containing how many times two characters share scenes&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cooccur &amp;lt;- speaker_scene_matrix %*% t(speaker_scene_matrix)
heatmap(cooccur)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/starwars_network_files/figure-html/unnamed-chunk-5-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;h2 id=&#34;social-network-analyses&#34;&gt;Social network analyses&lt;/h2&gt;
&lt;h3 id=&#34;graphical-representation-of-the-network&#34;&gt;Graphical representation of the network&lt;/h3&gt;
&lt;p&gt;Here the nodes represent characters in the movies. The characters are connected by a link if they both speak in the same scene. And the more the characters speak together, the thicker the link between them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(igraph)
g &amp;lt;- graph.adjacency(cooccur, weighted = TRUE, mode = &amp;quot;undirected&amp;quot;, diag = FALSE)
plot(g, edge.width = E(g)$weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/starwars_network_files/figure-html/unnamed-chunk-6-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Compute standard network features, degree and betweeness.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;degree(g)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      ANAKIN BAIL ORGANA        BERU      CLIEGG     JAR JAR       JOBAL 
##          12           1           4           4           4           4 
##     LAMA SU  MACE WINDU     OBI-WAN        OWEN       PADME    PLO KOON 
##           1           5           6           4          12           0 
##      POGGLE     QUI-GON       RUWEE        SHMI  SIO BIBBLE        SOLA 
##           1           1           4           1           0           4 
##        YODA 
##           4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;betweenness(g)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      ANAKIN BAIL ORGANA        BERU      CLIEGG     JAR JAR       JOBAL 
##   42.600000    0.000000    1.750000    0.500000   22.000000    0.000000 
##     LAMA SU  MACE WINDU     OBI-WAN        OWEN       PADME    PLO KOON 
##    0.000000   18.366667   15.000000    5.250000   55.133333    0.000000 
##      POGGLE     QUI-GON       RUWEE        SHMI  SIO BIBBLE        SOLA 
##    0.000000    0.000000    0.700000    0.000000    0.000000    5.000000 
##        YODA 
##    3.366667
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get a nicer representation of the network, see 
&lt;a href=&#34;http://tagteam.harvard.edu/hub_feeds/1981/feed_items/1388531&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and the formating from igraph to d3Network. Below is the code you‚Äôd need:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(d3Network)
library(networkD3)
sg &amp;lt;- simplify(g)
df &amp;lt;- get.edgelist(g, names=TRUE)
df &amp;lt;- as.data.frame(df)
colnames(df) &amp;lt;- c(&#39;source&#39;, &#39;target&#39;)
df$value &amp;lt;- rep(1, nrow(df))
# get communities
fc &amp;lt;- fastgreedy.community(g)
com &amp;lt;- membership(fc)
node.info &amp;lt;- data.frame(name=names(com), group=as.vector(com))
links &amp;lt;- data.frame(source=match(df$source, node.info$name)-1,target=match(df$target, node.info$name)-1,value=df$value)

forceNetwork(Links = links, Nodes = node.info,Source = &amp;quot;source&amp;quot;, Target = &amp;quot;target&amp;quot;,Value = &amp;quot;value&amp;quot;, NodeID = &amp;quot;name&amp;quot;,Group = &amp;quot;group&amp;quot;, opacity = 1, opacityNoHover=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The nodes represent characters in the movies. The characters are connected by a link if they both speak in the same scene. The colors are for groups obtained by some algorithms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Self teaching reproducible research</title>
      <link>https://oliviergimenez.github.io/blog/reproducible_research/</link>
      <pubDate>Sun, 10 Jul 2016 12:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/reproducible_research/</guid>
      <description>&lt;p&gt;I decided to teach myself how to do proper reproducible research. Many reasons to
that: save time on the mid/long term, make my analyses open and criticizable, share with others, &amp;hellip;&lt;/p&gt;
&lt;p&gt;There are tons of resources on  the web. I use the 
&lt;a href=&#34;http://kbroman.org/pages/tutorials.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorials&lt;/a&gt;
cooked by guru 
&lt;a href=&#34;http://kbroman.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Karl Broman&lt;/a&gt;.
For people like me who learned S-plus, he has some 
&lt;a href=&#34;http://kbroman.org/hipsteR/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;advice&lt;/a&gt; to switch to modern tools.
Among others, I try and use more and more RStudio in connection with KnitR and R Markdown and
share my codes on 
&lt;a href=&#34;https://github.com/oliviergimenez&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;. I am more a Fortran guy,
but Rcpp makes me wonder about C++.  Version control, piping and the tidy universe are
not yet entirely familiar to me, I‚Äôm making baby steps (or grandpa steps I should say). Let‚Äôs see how it goes.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New kids on the Bayesian block</title>
      <link>https://oliviergimenez.github.io/blog/nimble/</link>
      <pubDate>Thu, 10 Jul 2014 12:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/nimble/</guid>
      <description>&lt;p&gt;I attended Daniel Turek&amp;rsquo;s talk at ISEC about &lt;a href=&#34;http://r-nimble.org/&#34; target=&#34;_blank&#34;&gt;NIMBLE&lt;/a&gt;
a neat alternative to WinBUGS and JAGS. It is developed by
&lt;a href=&#34;http://nature.berkeley.edu/~pdevalpine/&#34; target=&#34;_blank&#34;&gt;Perry de Valpine&amp;rsquo;s
group&lt;/a&gt; at Berkeley and &lt;em&gt;&amp;lsquo;lets you use BUGS models natively in R, program functions
that use them, and compile everything via C++ for faster computing&amp;rsquo;.&lt;/em&gt; I played around
with NIMBLE a bit, &lt;a href=&#34;https://dl.dropboxusercontent.com/u/23160641/my-codes/cjs-nimble.R&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;
is an example of fitting a classic capture-recapture model to simulated data - thanks to Perry
and Daniel for their help! NIMBLE seems a lot faster than its competitors, and much more flexible.
I&amp;rsquo;ll continue my investigations with more complex models - stay tuned.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
