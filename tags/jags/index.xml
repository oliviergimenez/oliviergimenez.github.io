<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>jags | Olivier Gimenez</title>
    <link>https://oliviergimenez.github.io/tags/jags/</link>
      <atom:link href="https://oliviergimenez.github.io/tags/jags/index.xml" rel="self" type="application/rss+xml" />
    <description>jags</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>¬© Olivier Gimenez 2021</copyright><lastBuildDate>Fri, 07 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://oliviergimenez.github.io/img/flyfishing.jpg</url>
      <title>jags</title>
      <link>https://oliviergimenez.github.io/tags/jags/</link>
    </image>
    
    <item>
      <title>Some random piece of code</title>
      <link>https://oliviergimenez.github.io/blog/pieceofcode/</link>
      <pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/pieceofcode/</guid>
      <description>&lt;p&gt;Gathered some code on occupancy, capture-recapture &amp;amp; epidemiological models, social networks, spatial stuff, textual analyses, reproducible science, etc&amp;hellip;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üë©‚Äçüíªüë®‚Äçüíª In hope it&amp;#39;s useful, I gathered random pieces of code I wrote on occupancy, capture-recapture &amp;amp; epidemiological models, social networks, spatial stuff, textual analyses, &lt;a href=&#34;https://twitter.com/hashtag/DeepLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DeepLearning&lt;/a&gt; and the &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; ‚û°Ô∏è &lt;a href=&#34;https://t.co/T1PlbmDLZO&#34;&gt;https://t.co/T1PlbmDLZO&lt;/a&gt; I also went full purple &lt;a href=&#34;https://twitter.com/MoonbeamLevels?ref_src=twsrc%5Etfw&#34;&gt;@MoonbeamLevels&lt;/a&gt; üíú &lt;a href=&#34;https://t.co/q6vK4LYVS7&#34;&gt;pic.twitter.com/q6vK4LYVS7&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1291749139596955650?ref_src=twsrc%5Etfw&#34;&gt;August 7, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;</description>
    </item>
    
    <item>
      <title>Jags code to fit a multistate single-season occupancy model</title>
      <link>https://oliviergimenez.github.io/blog/coding/</link>
      <pubDate>Sat, 13 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/coding/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;fr&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Quick and dirty code to fit a multistate single-season &lt;a href=&#34;https://twitter.com/hashtag/occupancy?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#occupancy&lt;/a&gt; model in Jags using a &lt;a href=&#34;https://twitter.com/hashtag/HMM?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#HMM&lt;/a&gt; formulation, in case it&amp;#39;s useful &lt;a href=&#34;https://t.co/vUlrP0jjIU&#34;&gt;https://t.co/vUlrP0jjIU&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/JAGS?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#JAGS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; w/ &lt;a href=&#34;https://twitter.com/ArnaudLyet?ref_src=twsrc%5Etfw&#34;&gt;@ArnaudLyet&lt;/a&gt; &lt;a href=&#34;https://t.co/dSr0BlICb6&#34;&gt;pic.twitter.com/dSr0BlICb6&lt;/a&gt;&lt;/p&gt;&amp;mdash; Olivier Gimenez üññ (@oaggimenez) &lt;a href=&#34;https://twitter.com/oaggimenez/status/1117064436898988033?ref_src=twsrc%5Etfw&#34;&gt;13l 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Simulating data with JAGS</title>
      <link>https://oliviergimenez.github.io/blog/sim_with_jags/</link>
      <pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/sim_with_jags/</guid>
      <description>&lt;img style=&#34;float:left;margin-right:10px;margin-top:10px;width:200px;margin-bottom:5px&#34; src=&#34;https://oliviergimenez.github.io/img/posterior_plots_lr.png&#34;&gt;
Here, I illustrate the possibility to use `JAGS` to simulate data with two examples that might be of interest to population ecologists: first a linear regression, second a Cormack-Jolly-Seber capture-recapture model to estimate animal survival (formulated as a state-space model). The code is available from [GitHub](https://github.com/oliviergimenez/simul_with_jags).
&lt;p&gt;Recently, I have been struggling with simulating data from complex hierarchical models. After several unsuccessful attempts in &lt;code&gt;R&lt;/code&gt;, I remembered the good old times when I was using &lt;code&gt;WinBUGS&lt;/code&gt; (more than 10 years already!) and the possibility to simulate data with it. I&amp;rsquo;m using &lt;code&gt;JAGS&lt;/code&gt; now, and a quick search in Google with &amp;lsquo;simulating data with jags&amp;rsquo; led me to 
&lt;a href=&#34;https://www.georg-hosoya.de/wordpress/?p=799&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a complex example&lt;/a&gt; and 
&lt;a href=&#34;https://stackoverflow.com/questions/38295839/simulate-data-in-jags-r2jags&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a simple example&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Simulating data with &lt;code&gt;JAGS&lt;/code&gt; is convenient because you can use (almost) the same code for simulation and inference, and you can carry out simulation studies (bias, precision, interval coverage) in the same environment (namely &lt;code&gt;JAGS&lt;/code&gt;).&lt;/p&gt;
&lt;h2 id=&#34;linear-regression-example&#34;&gt;Linear regression example&lt;/h2&gt;
&lt;p&gt;We first load the packages we need for this tutorial:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(R2jags)
library(runjags)
library(mcmcplots)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then straight to the point, let&amp;rsquo;s generate data from a linear regression model. The trick is to use a &lt;code&gt;data&lt;/code&gt; block, have the simplest &lt;code&gt;model&lt;/code&gt; block you could think of and pass the parameters as if they were data. Note that it&amp;rsquo;d be possible to use only a model block, see comment 
&lt;a href=&#34;https://stackoverflow.com/questions/38295839/simulate-data-in-jags-r2jags&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;txtstring &amp;lt;- &#39;
data{
# Likelihood:
for (i in 1:N){
y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
mu[i] &amp;lt;- alpha + beta * x[i]
}
}
model{
fake &amp;lt;- 0
}
&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;beta&lt;/code&gt; are the intercept and slope, &lt;code&gt;tau&lt;/code&gt; the precision or the inverse of the variance, &lt;code&gt;y&lt;/code&gt; the response variable and &lt;code&gt;x&lt;/code&gt; the explanatory variable.&lt;/p&gt;
&lt;p&gt;We pick some values for the model parameters that we will use as data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# parameters for simulations 
N = 30 # nb of observations
x &amp;lt;- 1:N # predictor
alpha = 0.5 # intercept
beta = 1 # slope
sigma &amp;lt;- .1 # residual sd
tau &amp;lt;- 1/(sigma*sigma) # precision
# parameters are treated as data for the simulation step
data&amp;lt;-list(N=N,x=x,alpha=alpha,beta=beta,tau=tau)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now call &lt;code&gt;JAGS&lt;/code&gt;; note that we monitor the response variable instead of parameters as we would do when conducting standard inference:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# run jags
out &amp;lt;- run.jags(txtstring, data = data,monitor=c(&amp;quot;y&amp;quot;),sample=1, n.chains=1, summarise=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling rjags model...
## Calling the simulation using the rjags method...
## Note: the model did not require adaptation
## Burning in the model for 4000 iterations...
## Running the model for 1 iterations...
## Simulation complete
## Finished running the simulation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is a bit messy and needs to be formatted appropriately:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# reformat the outputs
Simulated &amp;lt;- coda::as.mcmc(out)
Simulated
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Markov Chain Monte Carlo (MCMC) output:
## Start = 5001 
## End = 5001 
## Thinning interval = 1 
##          y[1]    y[2]    y[3]     y[4]     y[5]     y[6]     y[7]     y[8]
## 5001 1.288399 2.52408 3.61516 4.583587 5.600675 6.566052 7.593407 8.457497
##         y[9]    y[10]   y[11]    y[12]    y[13]    y[14]    y[15]    y[16]
## 5001 9.70847 10.38035 11.5105 12.55048 13.49143 14.46356 15.45641 16.56148
##         y[17]    y[18]    y[19]    y[20]    y[21]   y[22]    y[23]
## 5001 17.50935 18.51501 19.66197 20.49477 21.57079 22.6199 23.48232
##         y[24]    y[25]    y[26]    y[27]    y[28]    y[29]    y[30]
## 5001 24.57923 25.47368 26.33674 27.46525 28.35525 29.60279 30.42952
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Simulated)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1 30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat = as.vector(Simulated)
dat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1.288399  2.524080  3.615160  4.583587  5.600675  6.566052  7.593407
##  [8]  8.457497  9.708470 10.380351 11.510500 12.550482 13.491435 14.463564
## [15] 15.456410 16.561483 17.509350 18.515005 19.661969 20.494767 21.570790
## [22] 22.619899 23.482317 24.579228 25.473676 26.336736 27.465251 28.355248
## [29] 29.602791 30.429517
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s fit the model we used to simulate to the data we just generated. I won&amp;rsquo;t go into the details and assume that the reader is familiar with &lt;code&gt;JAGS&lt;/code&gt; and linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# specify model in BUGS language
model &amp;lt;- 	
paste(&amp;quot;	
model {
# Likelihood:
for (i in 1:N){
y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
mu[i] &amp;lt;- alpha + beta * x[i]
}
# Priors:
alpha ~ dnorm(0, 0.01) # intercept
beta ~ dnorm(0, 0.01) # slope
sigma ~ dunif(0, 100) # standard deviation
tau &amp;lt;- 1 / (sigma * sigma) 
}
&amp;quot;)
writeLines(model,&amp;quot;lin_reg.jags&amp;quot;)	

# data
jags.data &amp;lt;- list(y = dat, N = length(dat), x = x)

# initial values
inits &amp;lt;- function(){list(alpha = rnorm(1), beta = rnorm(1), sigma = runif(1,0,10))}  

# parameters monitored
parameters &amp;lt;- c(&amp;quot;alpha&amp;quot;, &amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;)

# MCMC settings
ni &amp;lt;- 10000
nt &amp;lt;- 6
nb &amp;lt;- 5000
nc &amp;lt;- 2

# call JAGS from R
res &amp;lt;- jags(jags.data, inits, parameters, &amp;quot;lin_reg.jags&amp;quot;, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb, working.directory = getwd())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## module glm loaded
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 30
##    Unobserved stochastic nodes: 3
##    Total graph size: 130
## 
## Initializing model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look to the results and compare with the parameters we used to simulate the data (see above):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# summarize posteriors
print(res, digits = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Bugs model at &amp;quot;lin_reg.jags&amp;quot;, fit using jags,
##  2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 6
##  n.sims = 1668 iterations saved
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat
## alpha      0.544   0.038   0.469   0.518   0.545   0.570   0.617 1.000
## beta       0.998   0.002   0.994   0.997   0.998   1.000   1.003 1.001
## sigma      0.102   0.015   0.078   0.091   0.100   0.110   0.138 1.002
## deviance -53.810   2.724 -56.867 -55.808 -54.516 -52.641 -46.676 1.001
##          n.eff
## alpha     1700
## beta      1700
## sigma      780
## deviance  1700
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 3.7 and DIC = -50.1
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty close!&lt;/p&gt;
&lt;p&gt;Check convergence:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# trace plots
traplot(res,c(&amp;quot;alpha&amp;quot;, &amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/unnamed-chunk-8-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Plot the posterior distribution of the regression parameters and residual standard deviation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# posterior distributions
denplot(res,c(&amp;quot;alpha&amp;quot;, &amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/unnamed-chunk-9-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;h2 id=&#34;capture-recapture-example&#34;&gt;Capture-recapture example&lt;/h2&gt;
&lt;p&gt;I now illustrate the use of &lt;code&gt;JAGS&lt;/code&gt; to simulate data from a Cormack-Jolly-Seber model with constant survival and recapture probabilities. I assume that the reader is familiar with this model and its formulation as a state-space model.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;txtstring &amp;lt;- &#39;
data{
# Constant survival and recapture probabilities
for (i in 1:nind){
   for (t in f[i]:(n.occasions-1)){
      phi[i,t] &amp;lt;- mean.phi
      p[i,t] &amp;lt;- mean.p
      } #t
   } #i
# Likelihood 
for (i in 1:nind){
   # Define latent state and obs at first capture
   z[i,f[i]] &amp;lt;- 1
   mu2[i,1] &amp;lt;- 1 * z[i,f[i]] # detection is 1 at first capture (&amp;quot;conditional on first capture&amp;quot;)
   y[i,1] ~ dbern(mu2[i,1])
   # then deal w/ subsequent occasions
   for (t in (f[i]+1):n.occasions){
      # State process
      z[i,t] ~ dbern(mu1[i,t])
      mu1[i,t] &amp;lt;- phi[i,t-1] * z[i,t-1]
      # Observation process
      y[i,t] ~ dbern(mu2[i,t])
      mu2[i,t] &amp;lt;- p[i,t-1] * z[i,t]
      } #t
   } #i
}
model{
fake &amp;lt;- 0
}
&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s pick some values for parameters and store them in a data list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# parameter for simulations 
n.occasions = 10 # nb of occasions
nind = 100 # nb of individuals
mean.phi &amp;lt;- 0.8 # survival
mean.p &amp;lt;- 0.6 # recapture
f = rep(1,nind) # date of first capture
data&amp;lt;-list(n.occasions = n.occasions, mean.phi = mean.phi, mean.p = mean.p, f = f, nind = nind)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now run &lt;code&gt;JAGS&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out &amp;lt;- run.jags(txtstring, data = data,monitor=c(&amp;quot;y&amp;quot;),sample=1, n.chains=1, summarise=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling rjags model...
## Calling the simulation using the rjags method...
## Note: the model did not require adaptation
## Burning in the model for 4000 iterations...
## Running the model for 1 iterations...
## Simulation complete
## Finished running the simulation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Format the output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Simulated &amp;lt;- coda::as.mcmc(out)
dim(Simulated)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]    1 1000
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat = matrix(Simulated,nrow=nind)
head(dat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    1    1    0    0    0    0    0    0    0     0
## [2,]    1    1    1    1    0    0    0    0    0     0
## [3,]    1    0    0    0    0    0    0    0    0     0
## [4,]    1    0    0    0    0    0    0    0    0     0
## [5,]    1    0    0    0    0    0    0    0    0     0
## [6,]    1    1    1    1    0    0    1    0    1     1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I monitored only the detections and non-detections, but it is also possible to get the simulated values for the states, i.e. whether an individual is alive or dead at each occasion. You just need to amend the call to &lt;code&gt;JAGS&lt;/code&gt; with &lt;code&gt;monitor=c(&amp;quot;y&amp;quot;,&amp;quot;x&amp;quot;)&lt;/code&gt; and to amend the output accordingly.&lt;/p&gt;
&lt;p&gt;Now we fit a Cormack-Jolly-Seber model to the data we&amp;rsquo;ve just simulated, assuming constant parameters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- 	
paste(&amp;quot;	
model {
# Priors and constraints
for (i in 1:nind){
   for (t in f[i]:(n.occasions-1)){
      phi[i,t] &amp;lt;- mean.phi
      p[i,t] &amp;lt;- mean.p
      } #t
   } #i
mean.phi ~ dunif(0, 1)         # Prior for mean survival
mean.p ~ dunif(0, 1)           # Prior for mean recapture
# Likelihood 
for (i in 1:nind){
   # Define latent state at first capture
   z[i,f[i]] &amp;lt;- 1
   for (t in (f[i]+1):n.occasions){
      # State process
      z[i,t] ~ dbern(mu1[i,t])
      mu1[i,t] &amp;lt;- phi[i,t-1] * z[i,t-1]
      # Observation process
      y[i,t] ~ dbern(mu2[i,t])
      mu2[i,t] &amp;lt;- p[i,t-1] * z[i,t]
      } #t
   } #i
}
&amp;quot;)
writeLines(model,&amp;quot;cjs.jags&amp;quot;)	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# vector with occasion of marking
get.first &amp;lt;- function(x) min(which(x!=0))
f &amp;lt;- apply(dat, 1, get.first)
# data
jags.data &amp;lt;- list(y = dat, f = f, nind = dim(dat)[1], n.occasions = dim(dat)[2])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Initial values
known.state.cjs &amp;lt;- function(ch){
   state &amp;lt;- ch
   for (i in 1:dim(ch)[1]){
      n1 &amp;lt;- min(which(ch[i,]==1))
      n2 &amp;lt;- max(which(ch[i,]==1))
      state[i,n1:n2] &amp;lt;- 1
      state[i,n1] &amp;lt;- NA
      }
   state[state==0] &amp;lt;- NA
   return(state)
   }
inits &amp;lt;- function(){list(mean.phi = runif(1, 0, 1), mean.p = runif(1, 0, 1), z = known.state.cjs(dat))}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;d like to carry out inference about survival and recapture probabilities:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parameters &amp;lt;- c(&amp;quot;mean.phi&amp;quot;, &amp;quot;mean.p&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Standard MCMC settings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ni &amp;lt;- 10000
nt &amp;lt;- 6
nb &amp;lt;- 5000
nc &amp;lt;- 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ready to run &lt;code&gt;JAGS&lt;/code&gt;!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Call JAGS from R (BRT 1 min)
cjs &amp;lt;- jags(jags.data, inits, parameters, &amp;quot;cjs.jags&amp;quot;, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb, working.directory = getwd())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 900
##    Unobserved stochastic nodes: 902
##    Total graph size: 3707
## 
## Initializing model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Summarize posteriors and compare to the values we used to simulate the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;print(cjs, digits = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Bugs model at &amp;quot;cjs.jags&amp;quot;, fit using jags,
##  2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 6
##  n.sims = 1668 iterations saved
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat
## mean.p     0.596   0.033   0.531   0.574   0.597   0.618   0.660 1.000
## mean.phi   0.784   0.021   0.742   0.770   0.785   0.799   0.824 1.001
## deviance 440.611  18.374 408.121 427.569 438.662 452.512 479.608 1.001
##          n.eff
## mean.p    1700
## mean.phi  1700
## deviance  1700
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 168.9 and DIC = 609.5
## DIC is an estimate of expected predictive error (lower deviance is better).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again pretty close!&lt;/p&gt;
&lt;p&gt;Trace plots&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;traplot(cjs,c(&amp;quot;mean.phi&amp;quot;, &amp;quot;mean.p&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/unnamed-chunk-21-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Posterior distribution plots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;denplot(cjs,c(&amp;quot;mean.phi&amp;quot;, &amp;quot;mean.p&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://oliviergimenez.github.io/img/unnamed-chunk-22-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fitting occupancy models in ADMB</title>
      <link>https://oliviergimenez.github.io/blog/occupancy_in_admb/</link>
      <pubDate>Sun, 06 Aug 2017 12:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/occupancy_in_admb/</guid>
      <description>&lt;img style=&#34;float:left;margin-right:10px;margin-top:10px;margin-bottom:10px;&#34; src=&#34;https://oliviergimenez.github.io/img/admb.png&#34;&gt;
Some time ago, a student of mine got stuck when fitting dynamic occupancy models to real data in Jags because of the computational burden. 
&lt;p&gt;We had a dataset with several thousands sites, more than 20 seasons and 4 surveys per season (yeah!).&lt;/p&gt;
&lt;p&gt;We thought of using Unmarked instead (the likelihood is written in C++ and used through Rcpp), but dynamic models with false positives and/or random effects are not (yet?) implemented, and we were interested in considering both in our analysis. Some years ago, I had the opportunity to learn ADMB in a NCEAS meeting (thanks Hans Skaug!), I thought I would give it a try. ADMB allows you to write down any likelihood functions yourself and to incorporate random effects in an efficient way. It&amp;rsquo;s known to be fast for reasons I won&amp;rsquo;t go into here. Last but not least, ADMB can be run from R like JAGS and Unmarked (thanks Ben Bolker!).&lt;/p&gt;
&lt;p&gt;Here we go. I first simulate some data, then fit a dynamic model using ADMB, JAGS and Unmarked and finally perform a quick benchmarking. I&amp;rsquo;m going for a standard dynamic model, because the aims are i) to verify that JAGS is slower than Unmarked, ii) that ADMB is closer to Unmarked than JAGS in terms of time computation. If ii) is verified, then it will be worth the effort coding everything in ADMB.&lt;/p&gt;
&lt;p&gt;The results are available on RPub 
&lt;a href=&#34;http://rpubs.com/ogimenez/297167&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The code is available on GitHub 
&lt;a href=&#34;https://github.com/oliviergimenez/occupancy_in_ADMB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hope this is useful.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
