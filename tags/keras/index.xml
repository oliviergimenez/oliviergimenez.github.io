<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>keras | Olivier Gimenez</title>
    <link>https://oliviergimenez.github.io/tags/keras/</link>
      <atom:link href="https://oliviergimenez.github.io/tags/keras/index.xml" rel="self" type="application/rss+xml" />
    <description>keras</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Olivier Gimenez 2022</copyright><lastBuildDate>Sun, 02 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://oliviergimenez.github.io/img/flyfishing.jpg</url>
      <title>keras</title>
      <link>https://oliviergimenez.github.io/tags/keras/</link>
    </image>
    
    <item>
      <title>Binary image classification using Keras in R: Using CT scans to predict patients with Covid</title>
      <link>https://oliviergimenez.github.io/blog/image-classif/</link>
      <pubDate>Sun, 02 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://oliviergimenez.github.io/blog/image-classif/</guid>
      <description>&lt;p&gt;Here I illustrate how to train a CNN with Keras in R to predict from patients&#39; CT scans those who will develop severe illness from Covid.&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Michael Blum 
&lt;a href=&#34;https://twitter.com/mblum_g/status/1475940763716444161?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tweeted&lt;/a&gt; about the 
&lt;a href=&#34;https://stoic2021.grand-challenge.org/stoic2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;STOIC2021 - COVID-19 AI challenge&lt;/a&gt;. The main goal of this challenge is to predict from the patients&#39; 
&lt;a href=&#34;https://en.wikipedia.org/wiki/CT_scan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CT scans&lt;/a&gt; who will develop severe illness from Covid.&lt;/p&gt;
&lt;p&gt;Given my 
&lt;a href=&#34;https://oliviergimenez.github.io/blog/learning-machine-learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent interest in machine learning&lt;/a&gt;, this challenge peaked my interest. Although &lt;code&gt;Python&lt;/code&gt; is the machine learning &lt;em&gt;lingua franca&lt;/em&gt;, it is possible to 
&lt;a href=&#34;https://github.com/oliviergimenez/computo-deeplearning-occupany-lynx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;train a convolutional neural network (CNN) in &lt;code&gt;R&lt;/code&gt;&lt;/a&gt; and perform (binary) image classification.&lt;/p&gt;
&lt;p&gt;Here, I will use an 
&lt;a href=&#34;https://keras.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;R&lt;/code&gt; interface to &lt;code&gt;Keras&lt;/code&gt;&lt;/a&gt; that allows training neural networks. Note that the 
&lt;a href=&#34;https://stoic2021.grand-challenge.org/stoic-db/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dataset shared for the challenge&lt;/a&gt; is big, like 280Go big, and it took me a day to download it. For the sake of illustration, I will use a similar but much lighter dataset from a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Kaggle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle&lt;/a&gt; repository &lt;a href=&#34;https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset&#34;&gt;https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code is available on GitHub as usual &lt;a href=&#34;https://github.com/oliviergimenez/bin-image-classif&#34;&gt;https://github.com/oliviergimenez/bin-image-classif&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First things first, load the packages we will need.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
theme_set(theme_light())
library(keras)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;read-in-and-process-data&#34;&gt;Read in and process data&lt;/h1&gt;
&lt;p&gt;We will need a function to process images, I&amp;rsquo;m stealing 
&lt;a href=&#34;https://rpubs.com/spalladino14/653239&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;that one&lt;/a&gt; written by 
&lt;a href=&#34;https://www.linkedin.com/in/spencer-palladino/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spencer Palladino&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;process_pix &amp;lt;- function(lsf) {
  img &amp;lt;- lapply(lsf, image_load, grayscale = TRUE) # grayscale the image
  arr &amp;lt;- lapply(img, image_to_array) # turns it into an array
  arr_resized &amp;lt;- lapply(arr, image_array_resize, 
                        height = 100, 
                        width = 100) # resize
  arr_normalized &amp;lt;- normalize(arr_resized, axis = 1) #normalize to make small numbers 
  return(arr_normalized)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s process images for patients with Covid, and do some reshaping. Idem with images for patients without Covid.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# with covid
lsf &amp;lt;- list.files(&amp;quot;dat/COVID/&amp;quot;, full.names = TRUE) 
covid &amp;lt;- process_pix(lsf)
covid &amp;lt;- covid[,,,1] # get rid of last dim
covid_reshaped &amp;lt;- array_reshape(covid, c(nrow(covid), 100*100))
# without covid
lsf &amp;lt;- list.files(&amp;quot;dat/non-COVID/&amp;quot;, full.names = TRUE) 
ncovid &amp;lt;- process_pix(lsf)
ncovid &amp;lt;- ncovid[,,,1] # get rid of last dim
ncovid_reshaped &amp;lt;- array_reshape(ncovid, c(nrow(ncovid), 100*100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have 1252 CT scans of patients with Covid, and 1229 without.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s visualise these scans. Let&amp;rsquo;s pick a patient with Covid, and another one without.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;scancovid &amp;lt;- reshape2::melt(covid[10,,])
plotcovid &amp;lt;- scancovid %&amp;gt;%
  ggplot() +
  aes(x = Var1, y = Var2, fill = value) + 
  geom_raster() +
  labs(x = NULL, y = NULL, title = &amp;quot;CT scan of a patient with covid&amp;quot;) + 
  scale_fill_viridis_c() + 
  theme(legend.position = &amp;quot;none&amp;quot;)

scanncovid &amp;lt;- reshape2::melt(ncovid[10,,])
plotncovid &amp;lt;- scanncovid %&amp;gt;%
  ggplot() +
  aes(x = Var1, y = Var2, fill = value) + 
  geom_raster() +
  labs(x = NULL, y = NULL, title = &amp;quot;CT scan of a patient without covid&amp;quot;) + 
  scale_fill_viridis_c() + 
  theme(legend.position = &amp;quot;none&amp;quot;)

library(patchwork)
plotcovid + plotncovid
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;unnamed-chunk-4-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Put altogether and shuffle.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- rbind(cbind(covid_reshaped, 1), # 1 = covid
            cbind(ncovid_reshaped, 0)) # 0 = no covid
set.seed(1234)
shuffle &amp;lt;- sample(nrow(df), replace = F)
df &amp;lt;- df[shuffle, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sounds great. We have everything we need to start training a convolutional neural network model.&lt;/p&gt;
&lt;h1 id=&#34;convolutional-neural-network-cnn&#34;&gt;Convolutional neural network (CNN)&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s build our training and testing datasets using a 80/20 split.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(2022)
split &amp;lt;- sample(2, nrow(df), replace = T, prob = c(0.8, 0.2))
train &amp;lt;- df[split == 1,]
test &amp;lt;- df[split == 2,]
train_target &amp;lt;- df[split == 1, 10001] # label in training dataset
test_target &amp;lt;- df[split == 2, 10001] # label in testing dataset
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now build our model. I use three layers (&lt;code&gt;layer_dense()&lt;/code&gt; function) that I put one after the other with piping. I also use regularization (&lt;code&gt;layer_dropout()&lt;/code&gt; function) to avoid overfitting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 512, activation = &amp;quot;relu&amp;quot;) %&amp;gt;% 
  layer_dropout(0.4) %&amp;gt;%
  layer_dense(units = 256, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(0.3) %&amp;gt;%
  layer_dense(units = 128, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(0.2) %&amp;gt;%
  layer_dense(units = 2, activation = &#39;softmax&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model with defaults specific to binary classification.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model %&amp;gt;%
  compile(optimizer = &#39;adam&#39;,
          loss = &#39;binary_crossentropy&#39;, 
          metrics = c(&#39;accuracy&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use one-hot encoding (&lt;code&gt;to_categorical()&lt;/code&gt; function) aka dummy coding in statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train_label &amp;lt;- to_categorical(train_target)
test_label &amp;lt;- to_categorical(test_target)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s fit our model to the training dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fit_covid &amp;lt;- model %&amp;gt;%
  fit(x = train,
      y = train_label, 
      epochs = 25,
      batch_size = 512, # try also 128 and 256
      verbose = 2,
      validation_split = 0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quick visualization of the performances shows that the algorithm is doing not too bad. No over/under-fitting. Accuracy and loss are fine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(fit_covid)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;unnamed-chunk-11-1.png&#34; alt=&#34;&#34;&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;What about the performances on the testing dataset?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model %&amp;gt;%
  evaluate(test, test_label)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       loss   accuracy 
## 0.02048795 0.99795920
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s do some predictions on the testing dataset, and compare with ground truth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;predictedclasses &amp;lt;- model %&amp;gt;%
  predict_classes(test)
table(Prediction = predictedclasses, 
      Actual = test_target)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Actual
## Prediction   0   1
##          0 243   0
##          1   1 246
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty cool. Only one healthy patient is misclassified as being sick. Let&amp;rsquo;s save our model for further use.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;save_model_tf(model, &amp;quot;model/covidmodel&amp;quot;) # save the model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I&amp;rsquo;m happy with these results. In general however, we need to find ways to improve the performances. Check out some tips 
&lt;a href=&#34;https://machinelearningmastery.com/improve-deep-learning-performance/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; with examples implemented in &lt;code&gt;Keras&lt;/code&gt; with &lt;code&gt;R&lt;/code&gt; 
&lt;a href=&#34;https://keras.rstudio.com/articles/examples/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;there&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
