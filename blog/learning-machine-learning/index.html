<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=description content="I would like to familiarize myself with machine learning (ML) techniques in R. So I have been reading and learning by doing. I thought I&rsquo;d share my experience for others who&rsquo;d like to give it a try1."><link rel=alternate hreflang=en-us href=https://oliviergimenez.github.io/blog/learning-machine-learning/><meta name=theme-color content="#8486B2"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/tomorrow.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/tomorrow.min.css crossorigin=anonymous title=hl-dark disabled><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i%7CRaleway:400,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/academic.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-96999184-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
function trackOutboundLink(url,target){gtag('event','click',{'event_category':'outbound','event_label':url,'transport_type':'beacon','event_callback':function(){if(target!=='_blank'){document.location=url;}}});console.debug("Outbound link clicked: "+url);}
function onClickCallback(event){if((event.target.tagName!=='A')||(event.target.host===window.location.host)){return;}
trackOutboundLink(event.target,event.target.getAttribute('target'));}
gtag('js',new Date());gtag('config','UA-96999184-1',{});document.addEventListener('click',onClickCallback,false);</script><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hub63230815190acd33b27a0f36ae996e9_48855_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hub63230815190acd33b27a0f36ae996e9_48855_192x192_fill_lanczos_center_2.png><link rel=canonical href=https://oliviergimenez.github.io/blog/learning-machine-learning/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@oaggimenez"><meta property="twitter:creator" content="@oaggimenez"><meta property="og:site_name" content="Olivier Gimenez"><meta property="og:url" content="https://oliviergimenez.github.io/blog/learning-machine-learning/"><meta property="og:title" content="Experimenting with machine learning in R with tidymodels and the Kaggle titanic dataset | Olivier Gimenez"><meta property="og:description" content="I would like to familiarize myself with machine learning (ML) techniques in R. So I have been reading and learning by doing. I thought I&rsquo;d share my experience for others who&rsquo;d like to give it a try1."><meta property="og:image" content="https://oliviergimenez.github.io/blog/learning-machine-learning/featured.png"><meta property="twitter:image" content="https://oliviergimenez.github.io/blog/learning-machine-learning/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2021-08-13T00:00:00+00:00"><meta property="article:modified_time" content="2021-08-13T00:00:00+00:00"><title>Experimenting with machine learning in R with tidymodels and the Kaggle titanic dataset | Olivier Gimenez</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Olivier Gimenez</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Olivier Gimenez</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/about><span>About</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>People</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/people/current/><span>Current</span></a>
<a class=dropdown-item href=/people/alumni/><span>Former</span></a></div></li><li class=nav-item><a class=nav-link href=/project_landing><span>Projects</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Publications</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/publication/papers/><span>Papers</span></a>
<a class=dropdown-item href=/publication/books/><span>Books and book chapter</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Talks & workshops</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/talks/talk><span>Talks</span></a>
<a class=dropdown-item href=/talks/workshop><span>Workshops</span></a></div></li><li class=nav-item><a class=nav-link href=/coding><span>Codes</span></a></li><li class=nav-item><a class="nav-link active" href=/blog><span>News</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav><article class=article><div class="article-container pt-3"><h1>Experimenting with machine learning in R with tidymodels and the Kaggle titanic dataset</h1><div class=article-metadata><span class=article-date>Aug 13, 2021</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:514px><div style=position:relative><img src=/blog/learning-machine-learning/featured_hua4f5c3d28a526db8b584e76e872902ed_685757_720x0_resize_lanczos_2.png alt class=featured-image></div></div><div class=article-container><div class=article-style><p>I would like to familiarize myself with machine learning (ML) techniques in <code>R</code>. So I have been reading and learning by doing. I thought I&rsquo;d share my experience for others who&rsquo;d like to give it a try<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><h1 id=motivation>Motivation</h1><p>The two great books I&rsquo;m using are:</p><ul><li><p><a href=https://www.statlearning.com/ target=_blank rel=noopener>An Introduction to Statistical Learning with Applications in R</a> by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani</p></li><li><p><a href=https://www.tmwr.org/ target=_blank rel=noopener>Tidy models in R</a> by Max Kuhn and Julia Silge</p></li></ul><p>I also recommend checking out the material (codes, screencasts) shared by
<a href=http://varianceexplained.org/r/sliced-ml/ target=_blank rel=noopener>David Robinson</a> and
<a href=https://juliasilge.com/ target=_blank rel=noopener>Julia Silge</a> from whom I picked some useful tricks that I put to use below.</p><p>To try things, I&rsquo;ve joined the
<a href=https://en.wikipedia.org/wiki/Kaggle target=_blank rel=noopener>Kaggle</a> online community which gathers folks with lots of experience in ML from whom you can learn. Kaggle also hosts public datasets that can be used for playing around.</p><p>Let&rsquo;s start with the famous
<a href=https://www.kaggle.com/c/titanic/overview target=_blank rel=noopener>Titanic dataset</a>. We need to predict if a passenger survived the sinking of the Titanic (1) or not (0). A dataset is provided for training our models (train.csv). Another dataset is provided (test.csv) for which we do not know the answer. We will predict survival for each passenger, submit our answer to Kaggle and see how well we did compared to other folks. The metric for comparison is the percentage of passengers we correctly predict &ndash; aka as accuracy.</p><p>Data and codes are available from <a href=https://github.com/oliviergimenez/learning-machine-learning>https://github.com/oliviergimenez/learning-machine-learning</a>.</p><p>First things first, let&rsquo;s load some packages to get us started.</p><pre><code class=language-r>library(tidymodels) # metapackage for ML 
library(tidyverse) # metapackage for data manipulation and visulaisation
library(stacks) # stack ML models for better perfomance
theme_set(theme_light())
doParallel::registerDoParallel(cores = 4) # parallel computations
</code></pre><h1 id=data>Data</h1><p>Read in training data.</p><pre><code class=language-r>rawdata &lt;- read_csv(&quot;dat/titanic/train.csv&quot;)
glimpse(rawdata)
</code></pre><pre><code>## Rows: 891
## Columns: 12
## $ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…
## $ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…
## $ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…
## $ Name        &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bradley (Fl…
## $ Sex         &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;mal…
## $ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …
## $ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…
## $ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…
## $ Ticket      &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;113803&quot;, &quot;37…
## $ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…
## $ Cabin       &lt;chr&gt; NA, &quot;C85&quot;, NA, &quot;C123&quot;, NA, NA, &quot;E46&quot;, NA, NA, NA, &quot;G6&quot;, &quot;C…
## $ Embarked    &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;…
</code></pre><pre><code class=language-r>naniar::miss_var_summary(rawdata)
</code></pre><pre><code>## # A tibble: 12 × 3
##    variable    n_miss pct_miss
##    &lt;chr&gt;        &lt;int&gt;    &lt;dbl&gt;
##  1 Cabin          687   77.1  
##  2 Age            177   19.9  
##  3 Embarked         2    0.224
##  4 PassengerId      0    0    
##  5 Survived         0    0    
##  6 Pclass           0    0    
##  7 Name             0    0    
##  8 Sex              0    0    
##  9 SibSp            0    0    
## 10 Parch            0    0    
## 11 Ticket           0    0    
## 12 Fare             0    0
</code></pre><p>After some data exploration (not shown), I decided to take care of missing values, gather the two family variables in a single variable, and create a variable title.</p><pre><code class=language-r># Get most frequent port of embarkation
uniqx &lt;- unique(na.omit(rawdata$Embarked))
mode_embarked &lt;- as.character(fct_drop(uniqx[which.max(tabulate(match(rawdata$Embarked, uniqx)))]))

# Build function for data cleaning and handling NAs
process_data &lt;- function(tbl){
  
  tbl %&gt;%
    mutate(class = case_when(Pclass == 1 ~ &quot;first&quot;,
                             Pclass == 2 ~ &quot;second&quot;,
                             Pclass == 3 ~ &quot;third&quot;),
           class = as_factor(class),
           gender = factor(Sex),
           fare = Fare,
           age = Age,
           alone = if_else(SibSp + Parch == 0, &quot;yes&quot;, &quot;no&quot;), # alone variable
           alone = as_factor(alone),
           port = factor(Embarked), # rename embarked as port
           title = str_extract(Name, &quot;[A-Za-z]+\\.&quot;), # title variable
           title = fct_lump(title, 4)) %&gt;% # keep only most frequent levels of title
    mutate(port = ifelse(is.na(port), mode_embarked, port), # deal w/ NAs in port (replace by mode)
           port = as_factor(port)) %&gt;%
    group_by(title) %&gt;%
    mutate(median_age_title = median(age, na.rm = T)) %&gt;%
    ungroup() %&gt;%
    mutate(age = if_else(is.na(age), median_age_title, age)) # deal w/ NAs in age (replace by median in title)
}

# Process the data
dataset &lt;- rawdata %&gt;%
  process_data() %&gt;%
  mutate(survived = as_factor(if_else(Survived == 1, &quot;yes&quot;, &quot;no&quot;))) %&gt;%
  select(survived, class, gender, age, alone, fare, port, title) 

# Have a look again
glimpse(dataset)
</code></pre><pre><code>## Rows: 891
## Columns: 8
## $ survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no…
## $ class    &lt;fct&gt; third, first, third, first, third, third, first, third, third…
## $ gender   &lt;fct&gt; male, female, female, female, male, male, male, male, female,…
## $ age      &lt;dbl&gt; 22, 38, 26, 35, 35, 30, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55,…
## $ alone    &lt;fct&gt; no, no, yes, no, yes, yes, yes, no, no, no, no, yes, yes, no,…
## $ fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21…
## $ port     &lt;fct&gt; 3, 1, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3…
## $ title    &lt;fct&gt; Mr., Mrs., Miss., Mrs., Mr., Mr., Mr., Master., Mrs., Mrs., M…
</code></pre><pre><code class=language-r>naniar::miss_var_summary(dataset)
</code></pre><div data-pagedtable=false><script data-pagedtable-source type=application/json>{"columns":[{"label":["variable"],"name":[1],"type":["chr"],"align":["left"]},{"label":["n_miss"],"name":[2],"type":["int"],"align":["right"]},{"label":["pct_miss"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"survived","2":"0","3":"0"},{"1":"class","2":"0","3":"0"},{"1":"gender","2":"0","3":"0"},{"1":"age","2":"0","3":"0"},{"1":"alone","2":"0","3":"0"},{"1":"fare","2":"0","3":"0"},{"1":"port","2":"0","3":"0"},{"1":"title","2":"0","3":"0"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}</script></div><p>Let&rsquo;s apply the same treatment to the test dataset.</p><pre><code class=language-r>rawdata &lt;- read_csv(&quot;dat/titanic/test.csv&quot;) 
holdout &lt;- rawdata %&gt;%
  process_data() %&gt;%
  select(PassengerId, class, gender, age, alone, fare, port, title) 

glimpse(holdout)
</code></pre><pre><code>## Rows: 418
## Columns: 8
## $ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…
## $ class       &lt;fct&gt; third, third, second, third, third, third, third, second, …
## $ gender      &lt;fct&gt; male, female, male, male, female, male, female, male, fema…
## $ age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…
## $ alone       &lt;fct&gt; yes, no, yes, yes, no, yes, yes, no, yes, no, yes, yes, no…
## $ fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…
## $ port        &lt;fct&gt; 2, 3, 2, 3, 3, 3, 2, 3, 1, 3, 3, 3, 3, 3, 3, 1, 2, 1, 3, 1…
## $ title       &lt;fct&gt; Mr., Mrs., Mr., Mr., Mrs., Mr., Miss., Mr., Mrs., Mr., Mr.…
</code></pre><pre><code class=language-r>naniar::miss_var_summary(holdout)
</code></pre><pre><code>## # A tibble: 8 × 3
##   variable    n_miss pct_miss
##   &lt;chr&gt;        &lt;int&gt;    &lt;dbl&gt;
## 1 fare             1    0.239
## 2 PassengerId      0    0    
## 3 class            0    0    
## 4 gender           0    0    
## 5 age              0    0    
## 6 alone            0    0    
## 7 port             0    0    
## 8 title            0    0
</code></pre><h1 id=exploratory-data-analysis>Exploratory data analysis</h1><pre><code class=language-r>skimr::skim(dataset)
</code></pre><p>Table: Data summary</p><table><thead><tr><th style=text-align:left></th><th style=text-align:left></th></tr></thead><tbody><tr><td style=text-align:left>Name</td><td style=text-align:left>dataset</td></tr><tr><td style=text-align:left>Number of rows</td><td style=text-align:left>891</td></tr><tr><td style=text-align:left>Number of columns</td><td style=text-align:left>8</td></tr><tr><td style=text-align:left>_______________________</td><td style=text-align:left></td></tr><tr><td style=text-align:left>Column type frequency:</td><td style=text-align:left></td></tr><tr><td style=text-align:left>factor</td><td style=text-align:left>6</td></tr><tr><td style=text-align:left>numeric</td><td style=text-align:left>2</td></tr><tr><td style=text-align:left>________________________</td><td style=text-align:left></td></tr><tr><td style=text-align:left>Group variables</td><td style=text-align:left>None</td></tr></tbody></table><p><strong>Variable type: factor</strong></p><table><thead><tr><th style=text-align:left>skim_variable</th><th style=text-align:right>n_missing</th><th style=text-align:right>complete_rate</th><th style=text-align:left>ordered</th><th style=text-align:right>n_unique</th><th style=text-align:left>top_counts</th></tr></thead><tbody><tr><td style=text-align:left>survived</td><td style=text-align:right>0</td><td style=text-align:right>1</td><td style=text-align:left>FALSE</td><td style=text-align:right>2</td><td style=text-align:left>no: 549, yes: 342</td></tr><tr><td style=text-align:left>class</td><td style=text-align:right>0</td><td style=text-align:right>1</td><td style=text-align:left>FALSE</td><td style=text-align:right>3</td><td style=text-align:left>thi: 491, fir: 216, sec: 184</td></tr><tr><td style=text-align:left>gender</td><td style=text-align:right>0</td><td style=text-align:right>1</td><td style=text-align:left>FALSE</td><td style=text-align:right>2</td><td style=text-align:left>mal: 577, fem: 314</td></tr><tr><td style=text-align:left>alone</td><td style=text-align:right>0</td><td style=text-align:right>1</td><td style=text-align:left>FALSE</td><td style=text-align:right>2</td><td style=text-align:left>yes: 537, no: 354</td></tr><tr><td style=text-align:left>port</td><td style=text-align:right>0</td><td style=text-align:right>1</td><td style=text-align:left>FALSE</td><td style=text-align:right>4</td><td style=text-align:left>3: 644, 1: 168, 2: 77, S: 2</td></tr><tr><td style=text-align:left>title</td><td style=text-align:right>0</td><td style=text-align:right>1</td><td style=text-align:left>FALSE</td><td style=text-align:right>5</td><td style=text-align:left>Mr.: 517, Mis: 182, Mrs: 125, Mas: 40</td></tr></tbody></table><p><strong>Variable type: numeric</strong></p><table><thead><tr><th style=text-align:left>skim_variable</th><th style=text-align:right>n_missing</th><th style=text-align:right>complete_rate</th><th style=text-align:right>mean</th><th style=text-align:right>sd</th><th style=text-align:right>p0</th><th style=text-align:right>p25</th><th style=text-align:right>p50</th><th style=text-align:right>p75</th><th style=text-align:right>p100</th><th style=text-align:left>hist</th></tr></thead><tbody><tr><td style=text-align:left>age</td><td style=text-align:right>0</td><td style=text-align:right>1</td><td style=text-align:right>29.39</td><td style=text-align:right>13.26</td><td style=text-align:right>0.42</td><td style=text-align:right>21.00</td><td style=text-align:right>30.00</td><td style=text-align:right>35</td><td style=text-align:right>80.00</td><td style=text-align:left>▂▇▃▁▁</td></tr><tr><td style=text-align:left>fare</td><td style=text-align:right>0</td><td style=text-align:right>1</td><td style=text-align:right>32.20</td><td style=text-align:right>49.69</td><td style=text-align:right>0.00</td><td style=text-align:right>7.91</td><td style=text-align:right>14.45</td><td style=text-align:right>31</td><td style=text-align:right>512.33</td><td style=text-align:left>▇▁▁▁▁</td></tr></tbody></table><pre><code class=language-r>dataset %&gt;%
  group_by(gender) %&gt;%
  summarize(n = n(),
            n_surv = sum(survived == &quot;yes&quot;),
            pct_surv = n_surv / n)
</code></pre><pre><code>## # A tibble: 2 × 4
##   gender     n n_surv pct_surv
##   &lt;fct&gt;  &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;
## 1 female   314    233    0.742
## 2 male     577    109    0.189
</code></pre><pre><code class=language-r>dataset %&gt;%
  group_by(title) %&gt;%
  summarize(n = n(),
            n_surv = sum(survived == &quot;yes&quot;),
            pct_surv = n_surv / n) %&gt;%
  arrange(desc(pct_surv))
</code></pre><pre><code>## # A tibble: 5 × 4
##   title       n n_surv pct_surv
##   &lt;fct&gt;   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;
## 1 Mrs.      125     99    0.792
## 2 Miss.     182    127    0.698
## 3 Master.    40     23    0.575
## 4 Other      27     12    0.444
## 5 Mr.       517     81    0.157
</code></pre><pre><code class=language-r>dataset %&gt;%
  group_by(class, gender) %&gt;%
  summarize(n = n(),
            n_surv = sum(survived == &quot;yes&quot;),
            pct_surv = n_surv / n) %&gt;%
  arrange(desc(pct_surv))
</code></pre><pre><code>## # A tibble: 6 × 5
## # Groups:   class [3]
##   class  gender     n n_surv pct_surv
##   &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;
## 1 first  female    94     91    0.968
## 2 second female    76     70    0.921
## 3 third  female   144     72    0.5  
## 4 first  male     122     45    0.369
## 5 second male     108     17    0.157
## 6 third  male     347     47    0.135
</code></pre><pre><code class=language-r>dataset %&gt;%
  group_by(class, gender) %&gt;%
  summarize(n = n(),
            n_surv = sum(survived == &quot;yes&quot;),
            pct_surv = n_surv / n) %&gt;%
    mutate(class = fct_reorder(class, pct_surv)) %&gt;%
    ggplot(aes(pct_surv, class, fill = class, color = class)) +
    geom_col(position = position_dodge()) +
    scale_x_continuous(labels = percent) +
    labs(x = &quot;% in category that survived&quot;, fill = NULL, color = NULL, y = NULL) +
  facet_wrap(~gender)
</code></pre><p><img src=unnamed-chunk-7-1.png alt></p><pre><code class=language-r>dataset %&gt;%
  mutate(age = cut(age, breaks = c(0, 20, 40, 60, 80))) %&gt;%
  group_by(age, gender) %&gt;%
  summarize(n = n(),
            n_surv = sum(survived == &quot;yes&quot;),
            pct_surv = n_surv / n) %&gt;%
    mutate(age = fct_reorder(age, pct_surv)) %&gt;%
    ggplot(aes(pct_surv, age, fill = age, color = age)) +
    geom_col(position = position_dodge()) +
    scale_x_continuous(labels = percent) +
    labs(x = &quot;% in category that survived&quot;, fill = NULL, color = NULL, y = NULL) +
  facet_wrap(~gender)
</code></pre><p><img src=unnamed-chunk-7-2.png alt></p><pre><code class=language-r>dataset %&gt;%
    ggplot(aes(fare, group = survived, color = survived, fill = survived)) +
    geom_histogram(alpha = .4, position = position_dodge()) +
    labs(x = &quot;fare&quot;, y = NULL, color = &quot;survived?&quot;, fill = &quot;survived?&quot;)
</code></pre><p><img src=unnamed-chunk-7-3.png alt></p><h1 id=gradient-boosting-algorithms>Gradient boosting algorithms</h1><p>Let&rsquo;s start with
<a href=https://en.wikipedia.org/wiki/XGBoost target=_blank rel=noopener>gradient boosting methods</a> which are very population in the ML community.</p><h2 id=trainingtesting-datasets>Training/testing datasets</h2><p>Split our dataset in two, one dataset for training and the other one for testing. We will use an additionnal splitting step for cross-validation.</p><pre><code class=language-r>set.seed(2021)
spl &lt;- initial_split(dataset)
train &lt;- training(spl)
test &lt;- testing(spl)

train_5fold &lt;- train %&gt;%
  vfold_cv(5)
</code></pre><h2 id=tuning>Tuning</h2><p>Set up defaults.</p><pre><code class=language-r>mset &lt;- metric_set(accuracy) # metric is accuracy
control &lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
</code></pre><p>First a recipe.</p><pre><code class=language-r>xg_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
</code></pre><p>Then specify a gradient boosting model.</p><pre><code class=language-r>xg_model &lt;- boost_tree(mode = &quot;classification&quot;, # binary response
                       trees = tune(),
                       mtry = tune(),
                       tree_depth = tune(),
                       learn_rate = tune()) # parameters to be tuned
</code></pre><p>Now set our workflow.</p><pre><code class=language-r>xg_wf &lt;- 
  workflow() %&gt;% 
  add_model(xg_model) %&gt;% 
  add_recipe(xg_rec)
</code></pre><p>Use cross-validation to evaluate our model with different param config.</p><pre><code class=language-r>xg_tune &lt;- xg_wf %&gt;%
  tune_grid(train_5fold,
            metrics = mset,
            control = control,
            grid = crossing(trees = seq(500, 2500, 500),
                            mtry = c(3, 5, 8), # finalize(mtry(), train)
                            tree_depth = c(5, 10, 15),
                            learn_rate = c(0.01, 0.005)))
</code></pre><p>Visualize the results.</p><pre><code class=language-r>autoplot(xg_tune)
</code></pre><p><img src=unnamed-chunk-14-1.png alt></p><p>Collect metrics.</p><pre><code class=language-r>xg_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))
</code></pre><pre><code>## # A tibble: 90 × 10
##     mtry trees tree_depth learn_rate .metric  .estimator  mean     n std_err
##    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1     3  2000          5      0.01  accuracy binary     0.849     5  0.0139
##  2     3  1500         15      0.01  accuracy binary     0.849     5  0.0139
##  3     3  1500          5      0.01  accuracy binary     0.847     5  0.0131
##  4     3  2500          5      0.01  accuracy binary     0.847     5  0.0122
##  5     3  2500         15      0.005 accuracy binary     0.847     5  0.0146
##  6     3  1500         10      0.01  accuracy binary     0.846     5  0.0147
##  7     3  2000         10      0.01  accuracy binary     0.846     5  0.0132
##  8     3  1000         15      0.01  accuracy binary     0.844     5  0.0137
##  9     3  2000         10      0.005 accuracy binary     0.844     5  0.0139
## 10     8  1500          5      0.005 accuracy binary     0.844     5  0.0127
## # … with 80 more rows, and 1 more variable: .config &lt;chr&gt;
</code></pre><h2 id=fit-model>Fit model</h2><p>Use best config to fit model to training data.</p><pre><code class=language-r>xg_fit &lt;- xg_wf %&gt;%
  finalize_workflow(select_best(xg_tune)) %&gt;%
  fit(train)
</code></pre><pre><code>## [13:52:45] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
</code></pre><p>Check out accuracy on testing dataset to see if we overfitted.</p><pre><code class=language-r>xg_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)
</code></pre><pre><code>## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.798
</code></pre><p>Check out important features (aka predictors).</p><pre><code class=language-r>importances &lt;- xgboost::xgb.importance(model = extract_fit_engine(xg_fit))
importances %&gt;%
  mutate(Feature = fct_reorder(Feature, Gain)) %&gt;%
  ggplot(aes(Gain, Feature)) +
  geom_col()
</code></pre><p><img src=unnamed-chunk-18-1.png alt></p><h2 id=make-predictions>Make predictions</h2><p>Now we&rsquo;re ready to predict survival for the holdout dataset and submit to Kaggle.</p><pre><code class=language-r>xg_fit %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/xgboost.csv&quot;)
</code></pre><p>I got and accuracy of 0.76794. Cool. Let&rsquo;s train a random forest model now.</p><h1 id=random-forests>Random forests</h1><p>Let&rsquo;s continue with
<a href=https://en.wikipedia.org/wiki/Random_forest target=_blank rel=noopener>random forest methods</a>.</p><h2 id=tuning-1>Tuning</h2><p>First a recipe.</p><pre><code class=language-r>rf_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
</code></pre><p>Then specify a gradient boosting model.</p><pre><code class=language-r>rf_model &lt;- rand_forest(mode = &quot;classification&quot;, # binary response
                        engine = &quot;ranger&quot;, # by default
                        mtry = tune(),
                        trees = tune(),
                        min_n = tune()) # parameters to be tuned
</code></pre><p>Now set our workflow.</p><pre><code class=language-r>rf_wf &lt;- 
  workflow() %&gt;% 
  add_model(rf_model) %&gt;% 
  add_recipe(rf_rec)
</code></pre><p>Use cross-validation to evaluate our model with different param config.</p><pre><code class=language-r>rf_tune &lt;- rf_wf %&gt;%
  tune_grid(train_5fold,
            metrics = mset,
            control = control,
            grid = crossing(trees = seq(500, 2500, 500),
                            mtry = c(3, 5, 8), # finalize(mtry(), train)
                            min_n = seq(1, 20, 5)))
</code></pre><p>Visualize the results.</p><pre><code class=language-r>autoplot(rf_tune)
</code></pre><p><img src=unnamed-chunk-24-1.png alt></p><p>Collect metrics.</p><pre><code class=language-r>rf_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))
</code></pre><pre><code>## # A tibble: 60 × 9
##     mtry trees min_n .metric  .estimator  mean     n std_err .config            
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;              
##  1     8  1000    16 accuracy binary     0.843     5  0.0196 Preprocessor1_Mode…
##  2     5  2000     1 accuracy binary     0.841     5  0.0172 Preprocessor1_Mode…
##  3     8   500    16 accuracy binary     0.840     5  0.0176 Preprocessor1_Mode…
##  4     5  1000     1 accuracy binary     0.840     5  0.0163 Preprocessor1_Mode…
##  5     5  2500     1 accuracy binary     0.840     5  0.0163 Preprocessor1_Mode…
##  6     5   500     1 accuracy binary     0.840     5  0.0178 Preprocessor1_Mode…
##  7     8  1500    16 accuracy binary     0.838     5  0.0168 Preprocessor1_Mode…
##  8     5  1500     1 accuracy binary     0.838     5  0.0168 Preprocessor1_Mode…
##  9     3  1000     1 accuracy binary     0.837     5  0.0169 Preprocessor1_Mode…
## 10     5   500    16 accuracy binary     0.837     5  0.0185 Preprocessor1_Mode…
## # … with 50 more rows
</code></pre><h2 id=fit-model-1>Fit model</h2><p>Use best config to fit model to training data.</p><pre><code class=language-r>rf_fit &lt;- rf_wf %&gt;%
  finalize_workflow(select_best(rf_tune)) %&gt;%
  fit(train)
</code></pre><p>Check out accuracy on testing dataset to see if we overfitted.</p><pre><code class=language-r>rf_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)
</code></pre><pre><code>## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.816
</code></pre><p>Check out important features (aka predictors).</p><pre><code class=language-r>library(vip)
finalize_model(
  x = rf_model,
  parameters = select_best(rf_tune)) %&gt;%
  set_engine(&quot;ranger&quot;, importance = &quot;permutation&quot;) %&gt;%
  fit(survived ~ ., data = juice(prep(rf_rec))) %&gt;%
  vip(geom = &quot;point&quot;)
</code></pre><p><img src=unnamed-chunk-28-1.png alt></p><h2 id=make-predictions-1>Make predictions</h2><p>Now we&rsquo;re ready to predict survival for the holdout dataset and submit to Kaggle.</p><pre><code class=language-r>rf_fit %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/randomforest.csv&quot;)
</code></pre><p>I got and accuracy of 0.77033, a bit better than gradient boosting.</p><h1 id=regularization-methods-elastic-net-here>Regularization methods (elastic net here)</h1><p>Let&rsquo;s continue with
<a href=https://en.wikipedia.org/wiki/Elastic_net_regularization target=_blank rel=noopener>elastic net regularization </a>.</p><h2 id=tuning-2>Tuning</h2><p>First a recipe.</p><pre><code class=language-r>en_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) %&gt;% # all factors var are split into binary terms (factor disj coding)
  step_normalize(all_predictors()) # normalize
</code></pre><p>Then specify a gradient boosting model.</p><pre><code class=language-r>en_model &lt;- logistic_reg(penalty = tune()) %&gt;% # penalty to be tuned
  set_engine(&quot;glmnet&quot;) %&gt;% # elastic net
  set_mode(&quot;classification&quot;) # binary response
</code></pre><p>Now set our workflow.</p><pre><code class=language-r>en_wf &lt;- 
  workflow() %&gt;% 
  add_model(en_model) %&gt;% 
  add_recipe(en_rec)
</code></pre><p>Use cross-validation to evaluate our model with different param config.</p><pre><code class=language-r>en_tune &lt;- en_wf %&gt;%
  tune_grid(train_5fold,
            metrics = mset,
            control = control,
            grid = crossing(penalty = 10 ^ seq(-8, -.5, .5)))
</code></pre><p>Visualize the results.</p><pre><code class=language-r>autoplot(en_tune)
</code></pre><p><img src=unnamed-chunk-34-1.png alt></p><p>Collect metrics.</p><pre><code class=language-r>en_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))
</code></pre><pre><code>## # A tibble: 16 × 7
##         penalty .metric  .estimator  mean     n std_err .config              
##           &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1 0.000316     accuracy binary     0.807     4  0.0131 Preprocessor1_Model10
##  2 0.00000001   accuracy binary     0.805     4  0.0143 Preprocessor1_Model01
##  3 0.0000000316 accuracy binary     0.805     4  0.0143 Preprocessor1_Model02
##  4 0.0000001    accuracy binary     0.805     4  0.0143 Preprocessor1_Model03
##  5 0.000000316  accuracy binary     0.805     4  0.0143 Preprocessor1_Model04
##  6 0.000001     accuracy binary     0.805     4  0.0143 Preprocessor1_Model05
##  7 0.00000316   accuracy binary     0.805     4  0.0143 Preprocessor1_Model06
##  8 0.00001      accuracy binary     0.805     4  0.0143 Preprocessor1_Model07
##  9 0.0000316    accuracy binary     0.805     4  0.0143 Preprocessor1_Model08
## 10 0.0001       accuracy binary     0.805     4  0.0143 Preprocessor1_Model09
## 11 0.00316      accuracy binary     0.805     4  0.0174 Preprocessor1_Model12
## 12 0.001        accuracy binary     0.800     4  0.0164 Preprocessor1_Model11
## 13 0.01         accuracy binary     0.796     4  0.0190 Preprocessor1_Model13
## 14 0.0316       accuracy binary     0.781     4  0.0238 Preprocessor1_Model14
## 15 0.1          accuracy binary     0.777     4  0.0191 Preprocessor1_Model15
## 16 0.316        accuracy binary     0.599     4  0.0338 Preprocessor1_Model16
</code></pre><h2 id=fit-model-2>Fit model</h2><p>Use best config to fit model to training data.</p><pre><code class=language-r>en_fit &lt;- en_wf %&gt;%
  finalize_workflow(select_best(en_tune)) %&gt;%
  fit(train)
</code></pre><p>Check out accuracy on testing dataset to see if we overfitted.</p><pre><code class=language-r>en_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)
</code></pre><pre><code>## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.825
</code></pre><p>Check out important features (aka predictors).</p><pre><code class=language-r>library(broom)
en_fit$fit$fit$fit %&gt;%
  tidy() %&gt;%
  filter(lambda &gt;= select_best(en_tune)$penalty) %&gt;%
  filter(lambda == min(lambda),
         term != &quot;(Intercept)&quot;) %&gt;%
  mutate(term = fct_reorder(term, estimate)) %&gt;%
  ggplot(aes(estimate, term, fill = estimate &gt; 0)) +
  geom_col() +
  theme(legend.position = &quot;none&quot;)
</code></pre><p><img src=unnamed-chunk-38-1.png alt></p><h2 id=make-predictions-2>Make predictions</h2><p>Now we&rsquo;re ready to predict survival for the holdout dataset and submit to Kaggle.</p><pre><code class=language-r>en_fit %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/elasticnet.csv&quot;)
</code></pre><p>I got and accuracy of 0.76794.</p><h1 id=stacked-ensemble-modelling>Stacked ensemble modelling</h1><p>Tune again with a probability-based metric.</p><pre><code class=language-r>xg_tune &lt;- xg_wf %&gt;%
  tune_grid(train_5fold,
            metrics = metric_set(roc_auc),
            control = control,
            grid = crossing(trees = seq(500, 2500, 500),
                            mtry = c(3, 5, 8), # finalize(mtry(), train)
                            tree_depth = c(5, 10, 15),
                            learn_rate = c(0.01, 0.005)))

rf_tune &lt;- rf_wf %&gt;%
  tune_grid(train_5fold,
            metrics = metric_set(roc_auc),
            control = control,
            grid = crossing(trees = seq(500, 2500, 500),
                            mtry = c(3, 5, 8), # finalize(mtry(), train)
                            min_n = seq(1, 20, 5)))

en_tune &lt;- en_wf %&gt;%
  tune_grid(train_5fold,
            metrics = metric_set(roc_auc),
            control = control,
            grid = crossing(penalty = 10 ^ seq(-8, -.5, .5)))
</code></pre><p>Get best config.</p><pre><code class=language-r>xg_best &lt;- xg_tune %&gt;% filter_parameters(parameters = select_best(xg_tune))
rf_best &lt;- rf_tune %&gt;% filter_parameters(parameters = select_best(rf_tune))
en_best &lt;- en_tune %&gt;% filter_parameters(parameters = select_best(en_tune))
</code></pre><p>Do the stacked ensemble modelling (for some reasons I didn&rsquo;t understand, I could not stack elastic net).</p><pre><code class=language-r>blended &lt;- stacks() %&gt;% # initialize
  add_candidates(xg_best) %&gt;% # add gradient boosting
  add_candidates(rf_best) %&gt;% # add random forest
#  add_candidates(en_best) %&gt;% # add elastic net
  blend_predictions() %&gt;% # fit regularized model
  fit_members() # fit candidate members with non-zero stacking coef with full training dataset
</code></pre><p>Perf on testing dataset?</p><pre><code class=language-r>test %&gt;%
  bind_cols(predict(blended, .)) %&gt;%
  accuracy(survived, .pred_class)
</code></pre><pre><code>## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.803
</code></pre><p>Now predict.</p><pre><code class=language-r>holdout %&gt;%
  bind_cols(predict(blended, .)) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/stacked.csv&quot;)
</code></pre><p>I got an 0.77033 accuracy.</p><h1 id=logistic-regression>Logistic regression</h1><p>And what about a good old-fashioned logistic regression (not a ML algo)?</p><p>First a recipe.</p><pre><code class=language-r>logistic_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) %&gt;% # all factors var are split into binary terms (factor disj coding)
  step_normalize(all_predictors()) # normalize
</code></pre><p>Then specify a logistic regression.</p><pre><code class=language-r>logistic_model &lt;- logistic_reg() %&gt;% # no param to be tuned
  set_engine(&quot;glm&quot;) %&gt;% # elastic net
  set_mode(&quot;classification&quot;) # binary response
</code></pre><p>Now set our workflow.</p><pre><code class=language-r>logistic_wf &lt;- 
  workflow() %&gt;% 
  add_model(logistic_model) %&gt;% 
  add_recipe(logistic_rec)
</code></pre><p>Fit model.</p><pre><code class=language-r>logistic_fit &lt;- logistic_wf %&gt;%
  fit(train)
</code></pre><p>Inspect results.</p><pre><code class=language-r>tidy(logistic_fit, exponentiate = TRUE) %&gt;%
  filter(p.value &lt; 0.05)
</code></pre><pre><code>## # A tibble: 7 × 5
##   term         estimate std.error statistic  p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 age             0.712     0.146     -2.33 1.96e- 2
## 2 class_first     2.88      0.153      6.94 4.06e-12
## 3 class_second    1.83      0.116      5.20 2.01e- 7
## 4 alone_yes       1.45      0.135      2.77 5.67e- 3
## 5 port_X1         1.34      0.109      2.67 7.49e- 3
## 6 title_Mr.       0.307     0.253     -4.68 2.90e- 6
## 7 title_Other     0.668     0.141     -2.86 4.21e- 3
</code></pre><p>Check out accuracy on testing dataset to see if we overfitted.</p><pre><code class=language-r>logistic_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)
</code></pre><pre><code>## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.825
</code></pre><p>Confusion matrix.</p><pre><code class=language-r>logistic_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  conf_mat(survived, .pred_class)
</code></pre><pre><code>##           Truth
## Prediction  no yes
##        no  133  22
##        yes  17  51
</code></pre><p>Custom metrics.</p><pre><code class=language-r>custom_metrics &lt;- metric_set(sens, precision, recall, f_meas)
logistic_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  custom_metrics(truth = survived, estimate = .pred_class)
</code></pre><pre><code>## # A tibble: 4 × 3
##   .metric   .estimator .estimate
##   &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
## 1 sens      binary         0.887
## 2 precision binary         0.858
## 3 recall    binary         0.887
## 4 f_meas    binary         0.872
</code></pre><p>ROC curve.</p><pre><code class=language-r>logistic_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  roc_curve(truth = survived, estimate = .pred_yes, event_level = &quot;second&quot;) %&gt;%
  autoplot()
</code></pre><p><img src=unnamed-chunk-53-1.png alt></p><p>Check out important features (aka predictors).</p><pre><code class=language-r>library(broom)
logistic_fit %&gt;%
  tidy() %&gt;%
  mutate(term = fct_reorder(term, estimate)) %&gt;%
  ggplot(aes(estimate, term, fill = estimate &gt; 0)) +
  geom_col() +
  theme(legend.position = &quot;none&quot;)
</code></pre><p><img src=unnamed-chunk-54-1.png alt></p><p>Now we&rsquo;re ready to predict survival for the holdout dataset and submit to Kaggle.</p><pre><code class=language-r>logistic_fit %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/logistic.csv&quot;)
</code></pre><p>I got and accuracy of 0.76555. Oldies but goodies!</p><h1 id=conclusions>Conclusions</h1><p>I covered three ML algorithms (gradient boosting, random forest and elastic net) and logistic regression with the awesome <code>tidymodels</code> metapackage in <code>R</code>. My scores at predicting Titanic survivors were ok I guess. Some folks on Kaggle got a perfect accuracy, so there is always room for improvement. Maybe better tuning, better features (or predictors) or other algorithms would increase accuracy. Of course, I forgot to use <code>set.seed()</code> so results are not exactly reproducible.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>This post was also published on <a href=https://www.r-bloggers.com>https://www.r-bloggers.com</a>. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><div class=article-tags><a class="badge badge-light" href=/tags/machine-learning/>machine learning</a>
<a class="badge badge-light" href=/tags/tidymodels/>tidymodels</a>
<a class="badge badge-light" href=/tags/r/>R</a>
<a class="badge badge-light" href=/tags/rstats/>rstats</a></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu2cb62cde7ffbf991a23ad99f81001b35_47904_270x270_fill_q90_lanczos_center.jpg alt><div class=media-body><h5 class=card-title><a href=https://oliviergimenez.github.io></a></h5><h6 class=card-subtitle>Statistics, ecology & social sciences</h6><p class=card-text>I&rsquo;m a scientist working at the interface of animal ecology, statistical modeling and social sciences.</p><ul class=network-icon aria-hidden=true><li><a href=https://github.com/oliviergimenez target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://twitter.com/oaggimenez target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://www.youtube.com/channel/UCFtTq-4WwH0LIczQ__KFC1A/videos target=_blank rel=noopener><i class="fab fa-youtube"></i></a></li><li><a href="https://scholar.google.fr/citations?user=5NkQUA8AAAAJ&hl=fr" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://figshare.com/authors/Olivier_Gimenez/3914297 target=_blank rel=noopener><i class="ai ai-figshare"></i></a></li><li><a href=https://www.linkedin.com/in/olivier-gimenez-545451115/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div><section id=comments><div id=disqus_thread></div><script>let disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='https://'+"https-oliviergimenez-github-io"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/blog/twitter-social-network/>Quick and dirty analysis of a Twitter social network</a></li><li><a href=/blog/articlenina/>New paper!</a></li><li><a href=/blog/inla_workshop/>Workshop on spatio-temporal models with INLA</a></li><li><a href=/blog/crashcourse_netlogor/>Crash course on individual-based models using NetLogoR</a></li><li><a href=/blog/run_openbugs_parallel/>Running OpenBUGS in parallel</a></li></ul></div></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/academic.min.37431be2d92d7fb0160054761ab79602.js></script><div class=container><footer class=site-footer><p class=powered-by>© Olivier Gimenez 2021 &#183;
Built w/ <a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic</a> for <a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>, adapted from <a href=https://juliasilge.com/ target=_blank rel=noopener>Julia Silge's blog</a> & hosted on <a href=https://pages.github.com/ target=_blank rel=noopener>Github Pages</a>. Purple photo <a href="https://unsplash.com/@isaacquesada?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Isaac Quesada</a> on <a href="https://unsplash.com/s/photos/purple?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>