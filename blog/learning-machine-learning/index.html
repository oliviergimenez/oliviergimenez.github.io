<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="I would like to familiarize myself with machine learning (ML) techniques in R. So I have been reading and learning by doing. I thought I&rsquo;d share my experience for others who&rsquo;d like to give it a try1.">

  
  <link rel="alternate" hreflang="en-us" href="https://oliviergimenez.github.io/blog/learning-machine-learning/">

  


  
  
  
  <meta name="theme-color" content="#8486B2">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/tomorrow.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/tomorrow.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i%7CRaleway:400,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-96999184-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-96999184-1');
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu_a98e7b6589a8d25.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu_ceadb92754828f6f.png">

  <link rel="canonical" href="https://oliviergimenez.github.io/blog/learning-machine-learning/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@oaggimenez">
  <meta property="twitter:creator" content="@oaggimenez">
  
  <meta property="og:site_name" content="Olivier Gimenez">
  <meta property="og:url" content="https://oliviergimenez.github.io/blog/learning-machine-learning/">
  <meta property="og:title" content="Experimenting with machine learning in R with tidymodels and the Kaggle titanic dataset | Olivier Gimenez">
  <meta property="og:description" content="I would like to familiarize myself with machine learning (ML) techniques in R. So I have been reading and learning by doing. I thought I&rsquo;d share my experience for others who&rsquo;d like to give it a try1."><meta property="og:image" content="https://oliviergimenez.github.io/blog/learning-machine-learning/featured.png">
  <meta property="twitter:image" content="https://oliviergimenez.github.io/blog/learning-machine-learning/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2021-08-13T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2021-08-13T00:00:00&#43;00:00">
  

  



  


  


  





  <title>Experimenting with machine learning in R with tidymodels and the Kaggle titanic dataset | Olivier Gimenez</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Olivier Gimenez</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Olivier Gimenez</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/about"><span>About</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>People</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/people/current/"><span>Current</span></a>
            
              <a class="dropdown-item" href="/people/alumni/"><span>Former</span></a>
            
          </div>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/project_landing"><span>Projects</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Publications</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/publication/papers/"><span>Papers</span></a>
            
              <a class="dropdown-item" href="/publication/books/"><span>Books and book chapter</span></a>
            
          </div>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Talks & workshops</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/talks/talk/"><span>Talks</span></a>
            
              <a class="dropdown-item" href="/talks/workshop/"><span>Workshops</span></a>
            
          </div>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/coding"><span>Codes</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      

      

    </ul>

  </div>
</nav>


  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Experimenting with machine learning in R with tidymodels and the Kaggle titanic dataset</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Aug 13, 2021
  </span>
  

  

  

  
  
  

  
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 514px;">
  <div style="position: relative">
    <img src="/blog/learning-machine-learning/featured_hu_58555903faca55ec.png" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p>I would like to familiarize myself with machine learning (ML) techniques in <code>R</code>. So I have been reading and learning by doing. I thought I&rsquo;d share my experience for others who&rsquo;d like to give it a try<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<h1 id="first-version-august-13-2021-updated-august-23-2021">First version August 13, 2021, updated August 23, 2021</h1>
<p>Since my first post, I’ve been reading notebooks shared by folks who
ranked high in the challenge, and added two features that they used.
Eventually, these new predictors did not help (I must be doing something
wrong). I also explored some other ML algorithms. Last, I tuned the
parameters more efficiently with a clever grid-search algorithm. All in
all, I slightly improved my score, but most importantly, I now have a
clean template for further use.</p>
<h1 id="motivation">Motivation</h1>
<p>All material available from GitHub at

<a href="https://github.com/oliviergimenez/learning-machine-learning" target="_blank" rel="noopener">https://github.com/oliviergimenez/learning-machine-learning</a>.</p>
<p>The two great books I’m using are:</p>
<ul>
<li>
<p>
<a href="https://www.statlearning.com/" target="_blank" rel="noopener">An Introduction to Statistical Learning with Applications in
R</a> by Gareth James, Daniela Witten,
Trevor Hastie and Robert Tibshirani</p>
</li>
<li>
<p>
<a href="https://www.tmwr.org/" target="_blank" rel="noopener">Tidy models in R</a> by Max Kuhn and Julia
Silge</p>
</li>
</ul>
<p>I also recommend checking out the material (codes, screencasts) shared
by 
<a href="http://varianceexplained.org/r/sliced-ml/" target="_blank" rel="noopener">David Robinson</a> and

<a href="https://juliasilge.com/" target="_blank" rel="noopener">Julia Silge</a> from whom I picked some useful
tricks that I put to use below.</p>
<p>To try things, I’ve joined the

<a href="https://en.wikipedia.org/wiki/Kaggle" target="_blank" rel="noopener">Kaggle</a> online community which
gathers folks with lots of experience in ML from whom you can learn.
Kaggle also hosts public datasets that can be used for playing around.</p>
<p>I use the <code>tidymodels</code> metapackage that contains a suite of packages for
modeling and machine learning using <code>tidyverse</code> principles. Check out
all possibilities 
<a href="https://www.tidymodels.org/find/" target="_blank" rel="noopener">here</a>, and parsnip
models in particular 
<a href="https://www.tidymodels.org/find/parsnip/" target="_blank" rel="noopener">there</a>.</p>
<p>Let’s start with the famous 
<a href="https://www.kaggle.com/c/titanic/overview" target="_blank" rel="noopener">Titanic
dataset</a>. We need to predict
if a passenger survived the sinking of the Titanic (1) or not (0). A
dataset is provided for training our models (train.csv). Another dataset
is provided (test.csv) for which we do not know the answer. We will
predict survival for each passenger, submit our answer to Kaggle and see
how well we did compared to other folks. The metric for comparison is
the percentage of passengers we correctly predict – aka as accuracy.</p>
<p>First things first, let’s load some packages to get us started.</p>
<pre><code>library(tidymodels) # metapackage for ML 
library(tidyverse) # metapackage for data manipulation and visulaisation
library(stacks) # stack ML models for better perfomance
theme_set(theme_light())
doParallel::registerDoParallel(cores = 4) # parallel computations
</code></pre>
<h1 id="data">Data</h1>
<p>Read in training data.</p>
<pre><code>rawdata &lt;- read_csv(&quot;dat/titanic/train.csv&quot;)
glimpse(rawdata)

## Rows: 891
## Columns: 12
## $ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20…
## $ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, …
## $ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, …
## $ Name        &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bradley (Florence Brig…
## $ Sex         &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;,…
## $ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55, 2, NA, …
## $ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, …
## $ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, …
## $ Ticket      &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;113803&quot;, &quot;373450&quot;, &quot;330…
## $ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 1…
## $ Cabin       &lt;chr&gt; NA, &quot;C85&quot;, NA, &quot;C123&quot;, NA, NA, &quot;E46&quot;, NA, NA, NA, &quot;G6&quot;, &quot;C103&quot;, NA, N…
## $ Embarked    &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;,…

naniar::miss_var_summary(rawdata)

## # A tibble: 12 × 3
##    variable    n_miss pct_miss
##    &lt;chr&gt;        &lt;int&gt;    &lt;dbl&gt;
##  1 Cabin          687   77.1  
##  2 Age            177   19.9  
##  3 Embarked         2    0.224
##  4 PassengerId      0    0    
##  5 Survived         0    0    
##  6 Pclass           0    0    
##  7 Name             0    0    
##  8 Sex              0    0    
##  9 SibSp            0    0    
## 10 Parch            0    0    
## 11 Ticket           0    0    
## 12 Fare             0    0
</code></pre>
<p>After some data exploration (not shown), I decided to take care of
missing values, gather the two family variables in a single variable,
and create a variable title.</p>
<pre><code># Get most frequent port of embarkation
uniqx &lt;- unique(na.omit(rawdata$Embarked))
mode_embarked &lt;- as.character(fct_drop(uniqx[which.max(tabulate(match(rawdata$Embarked, uniqx)))]))


# Build function for data cleaning and handling NAs
process_data &lt;- function(tbl){
  
  tbl %&gt;%
    mutate(class = case_when(Pclass == 1 ~ &quot;first&quot;,
                             Pclass == 2 ~ &quot;second&quot;,
                             Pclass == 3 ~ &quot;third&quot;),
           class = as_factor(class),
           gender = factor(Sex),
           fare = Fare,
           age = Age,
           ticket = Ticket,
           alone = if_else(SibSp + Parch == 0, &quot;yes&quot;, &quot;no&quot;), # alone variable
           alone = as_factor(alone),
           port = factor(Embarked), # rename embarked as port
           title = str_extract(Name, &quot;[A-Za-z]+\\.&quot;), # title variable
           title = fct_lump(title, 4)) %&gt;% # keep only most frequent levels of title
    mutate(port = ifelse(is.na(port), mode_embarked, port), # deal w/ NAs in port (replace by mode)
           port = as_factor(port)) %&gt;%
    group_by(title) %&gt;%
    mutate(median_age_title = median(age, na.rm = T)) %&gt;%
    ungroup() %&gt;%
    mutate(age = if_else(is.na(age), median_age_title, age)) %&gt;% # deal w/ NAs in age (replace by median in title)
    mutate(ticketfreq = ave(1:nrow(.), FUN = length),
           fareadjusted = fare / ticketfreq) %&gt;%
    mutate(familyage = SibSp + Parch + 1 + age/70)
    
}

# Process the data
dataset &lt;- rawdata %&gt;%
  process_data() %&gt;%
  mutate(survived = as_factor(if_else(Survived == 1, &quot;yes&quot;, &quot;no&quot;))) %&gt;%
  mutate(survived = relevel(survived, ref = &quot;yes&quot;)) %&gt;% # first event is survived = yes
  select(survived, class, gender, age, alone, port, title, fareadjusted, familyage) 

# Have a look again
glimpse(dataset)

## Rows: 891
## Columns: 9
## $ survived     &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, y…
## $ class        &lt;fct&gt; third, first, third, first, third, third, first, third, third, secon…
## $ gender       &lt;fct&gt; male, female, female, female, male, male, male, male, female, female…
## $ age          &lt;dbl&gt; 22, 38, 26, 35, 35, 30, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55, 2, 30,…
## $ alone        &lt;fct&gt; no, no, yes, no, yes, yes, yes, no, no, no, no, yes, yes, no, yes, y…
## $ port         &lt;fct&gt; 3, 1, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3, 3, 2,…
## $ title        &lt;fct&gt; Mr., Mrs., Miss., Mrs., Mr., Mr., Mr., Master., Mrs., Mrs., Miss., M…
## $ fareadjusted &lt;dbl&gt; 0.008136925, 0.080003704, 0.008894501, 0.059595960, 0.009034792, 0.0…
## $ familyage    &lt;dbl&gt; 2.314286, 2.542857, 1.371429, 2.500000, 1.500000, 1.428571, 1.771429…

naniar::miss_var_summary(dataset)

## # A tibble: 9 × 3
##   variable     n_miss pct_miss
##   &lt;chr&gt;         &lt;int&gt;    &lt;dbl&gt;
## 1 survived          0        0
## 2 class             0        0
## 3 gender            0        0
## 4 age               0        0
## 5 alone             0        0
## 6 port              0        0
## 7 title             0        0
## 8 fareadjusted      0        0
## 9 familyage         0        0
</code></pre>
<p>Let’s apply the same treatment to the test dataset.</p>
<pre><code>rawdata &lt;- read_csv(&quot;dat/titanic/test.csv&quot;) 
holdout &lt;- rawdata %&gt;%
  process_data() %&gt;%
  select(PassengerId, class, gender, age, alone, port, title, fareadjusted, familyage) 

glimpse(holdout)

## Rows: 418
## Columns: 9
## $ PassengerId  &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905…
## $ class        &lt;fct&gt; third, third, second, third, third, third, third, second, third, thi…
## $ gender       &lt;fct&gt; male, female, male, male, female, male, female, male, female, male, …
## $ age          &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0, 28.5, 46…
## $ alone        &lt;fct&gt; yes, no, yes, yes, no, yes, yes, no, yes, no, yes, yes, no, no, no, …
## $ port         &lt;fct&gt; 2, 3, 2, 3, 3, 3, 2, 3, 1, 3, 3, 3, 3, 3, 3, 1, 2, 1, 3, 1, 1, 3, 3,…
## $ title        &lt;fct&gt; Mr., Mrs., Mr., Mr., Mrs., Mr., Miss., Mr., Mrs., Mr., Mr., Mr., Mrs…
## $ fareadjusted &lt;dbl&gt; 0.018730144, 0.016746411, 0.023175837, 0.020723684, 0.029395933, 0.0…
## $ familyage    &lt;dbl&gt; 1.492857, 2.671429, 1.885714, 1.385714, 3.314286, 1.200000, 1.428571…

naniar::miss_var_summary(holdout)

## # A tibble: 9 × 3
##   variable     n_miss pct_miss
##   &lt;chr&gt;         &lt;int&gt;    &lt;dbl&gt;
## 1 fareadjusted      1    0.239
## 2 PassengerId       0    0    
## 3 class             0    0    
## 4 gender            0    0    
## 5 age               0    0    
## 6 alone             0    0    
## 7 port              0    0    
## 8 title             0    0    
## 9 familyage         0    0
</code></pre>
<h1 id="exploratory-data-analysis">Exploratory data analysis</h1>
<pre><code>skimr::skim(dataset)
</code></pre>
<table>
<caption>Data summary</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">Name</td>
<td style="text-align: left;">dataset</td>
</tr>
<tr class="even">
<td style="text-align: left;">Number of rows</td>
<td style="text-align: left;">891</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Number of columns</td>
<td style="text-align: left;">9</td>
</tr>
<tr class="even">
<td style="text-align: left;">_______________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Column type frequency:</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">factor</td>
<td style="text-align: left;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">numeric</td>
<td style="text-align: left;">3</td>
</tr>
<tr class="even">
<td style="text-align: left;">________________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Group variables</td>
<td style="text-align: left;">None</td>
</tr>
</tbody>
</table>
<p>Data summary</p>
<p><strong>Variable type: factor</strong></p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: left;">ordered</th>
<th style="text-align: right;">n_unique</th>
<th style="text-align: left;">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">survived</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">FALSE</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">no: 549, yes: 342</td>
</tr>
<tr class="even">
<td style="text-align: left;">class</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">FALSE</td>
<td style="text-align: right;">3</td>
<td style="text-align: left;">thi: 491, fir: 216, sec: 184</td>
</tr>
<tr class="odd">
<td style="text-align: left;">gender</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">FALSE</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">mal: 577, fem: 314</td>
</tr>
<tr class="even">
<td style="text-align: left;">alone</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">FALSE</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">yes: 537, no: 354</td>
</tr>
<tr class="odd">
<td style="text-align: left;">port</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">FALSE</td>
<td style="text-align: right;">4</td>
<td style="text-align: left;">3: 644, 1: 168, 2: 77, S: 2</td>
</tr>
<tr class="even">
<td style="text-align: left;">title</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">FALSE</td>
<td style="text-align: right;">5</td>
<td style="text-align: left;">Mr.: 517, Mis: 182, Mrs: 125, Mas: 40</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
<th style="text-align: right;">p0</th>
<th style="text-align: right;">p25</th>
<th style="text-align: right;">p50</th>
<th style="text-align: right;">p75</th>
<th style="text-align: right;">p100</th>
<th style="text-align: left;">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">age</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">29.39</td>
<td style="text-align: right;">13.26</td>
<td style="text-align: right;">0.42</td>
<td style="text-align: right;">21.00</td>
<td style="text-align: right;">30.00</td>
<td style="text-align: right;">35.00</td>
<td style="text-align: right;">80.00</td>
<td style="text-align: left;">▂▇▃▁▁</td>
</tr>
<tr class="even">
<td style="text-align: left;">fareadjusted</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.02</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0.58</td>
<td style="text-align: left;">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td style="text-align: left;">familyage</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2.32</td>
<td style="text-align: right;">1.57</td>
<td style="text-align: right;">1.07</td>
<td style="text-align: right;">1.41</td>
<td style="text-align: right;">1.57</td>
<td style="text-align: right;">2.62</td>
<td style="text-align: right;">11.43</td>
<td style="text-align: left;">▇▁▁▁▁</td>
</tr>
</tbody>
</table>
<p>Let’s explore the data.</p>
<pre><code>dataset %&gt;%
  count(survived)

## # A tibble: 2 × 2
##   survived     n
##   &lt;fct&gt;    &lt;int&gt;
## 1 yes        342
## 2 no         549

dataset %&gt;%
  group_by(gender) %&gt;%
  summarize(n = n(),
            n_surv = sum(survived == &quot;yes&quot;),
            pct_surv = n_surv / n)

## # A tibble: 2 × 4
##   gender     n n_surv pct_surv
##   &lt;fct&gt;  &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;
## 1 female   314    233    0.742
## 2 male     577    109    0.189

dataset %&gt;%
  group_by(title) %&gt;%
  summarize(n = n(),
            n_surv = sum(survived == &quot;yes&quot;),
            pct_surv = n_surv / n) %&gt;%
  arrange(desc(pct_surv))

## # A tibble: 5 × 4
##   title       n n_surv pct_surv
##   &lt;fct&gt;   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;
## 1 Mrs.      125     99    0.792
## 2 Miss.     182    127    0.698
## 3 Master.    40     23    0.575
## 4 Other      27     12    0.444
## 5 Mr.       517     81    0.157

dataset %&gt;%
  group_by(class, gender) %&gt;%
  summarize(n = n(),
            n_surv = sum(survived == &quot;yes&quot;),
            pct_surv = n_surv / n) %&gt;%
  arrange(desc(pct_surv))

## # A tibble: 6 × 5
## # Groups:   class [3]
##   class  gender     n n_surv pct_surv
##   &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;
## 1 first  female    94     91    0.968
## 2 second female    76     70    0.921
## 3 third  female   144     72    0.5  
## 4 first  male     122     45    0.369
## 5 second male     108     17    0.157
## 6 third  male     347     47    0.135
</code></pre>
<p>Some informative graphs.</p>
<pre><code>dataset %&gt;%
  group_by(class, gender) %&gt;%
  summarize(n = n(),
            n_surv = sum(survived == &quot;yes&quot;),
            pct_surv = n_surv / n) %&gt;%
    mutate(class = fct_reorder(class, pct_surv)) %&gt;%
    ggplot(aes(pct_surv, class, fill = class, color = class)) +
    geom_col(position = position_dodge()) +
    scale_x_continuous(labels = percent) +
    labs(x = &quot;% in category that survived&quot;, fill = NULL, color = NULL, y = NULL) +
  facet_wrap(~gender)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-7-1.png" alt=""></p>
<pre><code>dataset %&gt;%
  mutate(age = cut(age, breaks = c(0, 20, 40, 60, 80))) %&gt;%
  group_by(age, gender) %&gt;%
  summarize(n = n(),
            n_surv = sum(survived == &quot;yes&quot;),
            pct_surv = n_surv / n) %&gt;%
    mutate(age = fct_reorder(age, pct_surv)) %&gt;%
    ggplot(aes(pct_surv, age, fill = age, color = age)) +
    geom_col(position = position_dodge()) +
    scale_x_continuous(labels = percent) +
    labs(x = &quot;% in category that survived&quot;, fill = NULL, color = NULL, y = NULL) +
  facet_wrap(~gender)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-7-2.png" alt=""></p>
<pre><code>dataset %&gt;%
    ggplot(aes(fareadjusted, group = survived, color = survived, fill = survived)) +
    geom_histogram(alpha = .4, position = position_dodge()) +
    labs(x = &quot;fare&quot;, y = NULL, color = &quot;survived?&quot;, fill = &quot;survived?&quot;)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-7-3.png" alt=""></p>
<pre><code>dataset %&gt;%
    ggplot(aes(familyage, group = survived, color = survived, fill = survived)) +
    geom_histogram(alpha = .4, position = position_dodge()) +
    labs(x = &quot;family aged&quot;, y = NULL, color = &quot;survived?&quot;, fill = &quot;survived?&quot;)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-7-4.png" alt=""></p>
<h1 id="trainingtesting-datasets">Training/testing datasets</h1>
<p>Split our dataset in two, one dataset for training and the other one for
testing. We will use an additionnal splitting step for cross-validation.</p>
<pre><code>set.seed(2021)
spl &lt;- initial_split(dataset, strata = &quot;survived&quot;)
train &lt;- training(spl)
test &lt;- testing(spl)

train_5fold &lt;- train %&gt;%
  vfold_cv(5)
</code></pre>
<h1 id="gradient-boosting-algorithms---xgboost">Gradient boosting algorithms - xgboost</h1>
<p>Let’s start with 
<a href="https://en.wikipedia.org/wiki/XGBoost" target="_blank" rel="noopener">gradient boosting
methods</a> which are very popular
in the ML community.</p>
<h2 id="tuning">Tuning</h2>
<p>Set up defaults.</p>
<pre><code>mset &lt;- metric_set(accuracy) # metric is accuracy
control &lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
</code></pre>
<p>First a recipe.</p>
<pre><code>xg_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
</code></pre>
<p>Then specify a gradient boosting model.</p>
<pre><code>xg_model &lt;- boost_tree(mode = &quot;classification&quot;, # binary response
                       trees = tune(),
                       mtry = tune(),
                       tree_depth = tune(),
                       learn_rate = tune(),
                       loss_reduction = tune(),
                       min_n = tune()) # parameters to be tuned
</code></pre>
<p>Now set our workflow.</p>
<pre><code>xg_wf &lt;- 
  workflow() %&gt;% 
  add_model(xg_model) %&gt;% 
  add_recipe(xg_rec)
</code></pre>
<p>Use cross-validation to evaluate our model with different param config.</p>
<pre><code>xg_tune &lt;- xg_wf %&gt;%
  tune_grid(train_5fold,
            metrics = mset,
            control = control,
            grid = crossing(trees = 1000,
                            mtry = c(3, 5, 8), # finalize(mtry(), train)
                            tree_depth = c(5, 10, 15),
                            learn_rate = c(0.01, 0.005),
                            loss_reduction = c(0.01, 0.1, 1),
                            min_n = c(2, 10, 25)))
</code></pre>
<p>Visualize the results.</p>
<pre><code>autoplot(xg_tune) + theme_light()
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-14-1.png" alt=""></p>
<p>Collect metrics.</p>
<pre><code>xg_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))

## # A tibble: 162 × 12
##     mtry trees min_n tree_depth learn_rate loss_reduction .metric  .estimator  mean     n
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;
##  1     3  1000     2          5       0.01           0.01 accuracy binary     0.849     5
##  2     8  1000     2          5       0.01           0.01 accuracy binary     0.847     5
##  3     8  1000     2          5       0.01           0.1  accuracy binary     0.846     5
##  4     3  1000     2         15       0.01           0.1  accuracy binary     0.844     5
##  5     5  1000     2         10       0.01           1    accuracy binary     0.844     5
##  6     3  1000     2          5       0.01           0.1  accuracy binary     0.844     5
##  7     5  1000     2         10       0.01           0.1  accuracy binary     0.843     5
##  8     3  1000     2         10       0.01           0.1  accuracy binary     0.843     5
##  9     5  1000     2          5       0.01           0.01 accuracy binary     0.843     5
## 10     5  1000     2          5       0.01           0.1  accuracy binary     0.843     5
## # … with 152 more rows, and 2 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt;
</code></pre>
<p>The tuning takes some time. There are other ways to explore the
parameter space more efficiently. For example, we will use the function

<a href="https://dials.tidymodels.org/reference/grid_max_entropy.html" target="_blank" rel="noopener"><code>dials::grid_max_entropy()</code></a>
in the last section about ensemble modelling. Here, I will use

<a href="https://search.r-project.org/CRAN/refmans/finetune/html/tune_race_anova.html" target="_blank" rel="noopener"><code>finetune::tune_race_anova</code></a>.</p>
<pre><code>library(finetune)
xg_tune &lt;-
  xg_wf %&gt;%
  tune_race_anova(
    train_5fold,
    grid = 50,
    param_info = xg_model %&gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
</code></pre>
<p>Visualize the results.</p>
<pre><code>autoplot(xg_tune)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-17-1.png" alt=""></p>
<p>Collect metrics.</p>
<pre><code>xg_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))

## # A tibble: 50 × 12
##     mtry trees min_n tree_depth learn_rate loss_reduction .metric  .estimator  mean     n
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;
##  1     6   856     4         13   1.12e- 2       2.49e- 8 accuracy binary     0.837     5
##  2    10  1952     6          7   3.36e- 2       2.07e+ 0 accuracy binary     0.829     5
##  3     3   896     2          5   1.73e- 5       6.97e- 8 accuracy binary     0.826     5
##  4    14  1122     4          6   1.16e- 6       3.44e+ 0 accuracy binary     0.815     4
##  5     7   939     8          4   2.88e- 5       1.50e- 4 accuracy binary     0.813     4
##  6     7    17    10          7   4.54e- 6       6.38e- 3 accuracy binary     0.813     4
##  7     8    92     9         11   3.60e- 3       3.01e-10 accuracy binary     0.811     4
##  8    13  1407    15          4   1.48e- 2       9.68e- 4 accuracy binary     0.807     3
##  9     4   658    11          9   9.97e-10       1.27e- 5 accuracy binary     0.805     3
## 10     2   628    14          9   1.84e- 6       9.44e- 5 accuracy binary     0.798     3
## # … with 40 more rows, and 2 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt;
</code></pre>
<h2 id="fit-model">Fit model</h2>
<p>Use best config to fit model to training data.</p>
<pre><code>xg_fit &lt;- xg_wf %&gt;%
  finalize_workflow(select_best(xg_tune)) %&gt;%
  fit(train)

## [23:39:25] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
</code></pre>
<p>Check out accuracy on testing dataset to see if we overfitted.</p>
<pre><code>xg_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.812
</code></pre>
<p>Check out important features (aka predictors).</p>
<pre><code>importances &lt;- xgboost::xgb.importance(model = extract_fit_engine(xg_fit))
importances %&gt;%
  mutate(Feature = fct_reorder(Feature, Gain)) %&gt;%
  ggplot(aes(Gain, Feature)) +
  geom_col()
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-21-1.png" alt=""></p>
<h2 id="make-predictions">Make predictions</h2>
<p>Now we’re ready to predict survival for the holdout dataset and submit
to Kaggle. Note that I use the whole dataset, not just the training
dataset.</p>
<pre><code>xg_wf %&gt;%
  finalize_workflow(select_best(xg_tune)) %&gt;%
  fit(dataset) %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/xgboost.csv&quot;)

## [23:39:28] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
</code></pre>
<p>I got and accuracy of 0.74162. Cool. Let’s train a random forest model
now.</p>
<h1 id="random-forests">Random forests</h1>
<p>Let’s continue with 
<a href="https://en.wikipedia.org/wiki/Random_forest" target="_blank" rel="noopener">random forest
methods</a>.</p>
<h2 id="tuning-1">Tuning</h2>
<p>First a recipe.</p>
<pre><code>rf_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
</code></pre>
<p>Then specify a random forest model.</p>
<pre><code>rf_model &lt;- rand_forest(mode = &quot;classification&quot;, # binary response
                        engine = &quot;ranger&quot;, # by default
                        mtry = tune(),
                        trees = tune(),
                        min_n = tune()) # parameters to be tuned
</code></pre>
<p>Now set our workflow.</p>
<pre><code>rf_wf &lt;- 
  workflow() %&gt;% 
  add_model(rf_model) %&gt;% 
  add_recipe(rf_rec)
</code></pre>
<p>Use cross-validation to evaluate our model with different param config.</p>
<pre><code>rf_tune &lt;-
  rf_wf %&gt;%
  tune_race_anova(
    train_5fold,
    grid = 50,
    param_info = rf_model %&gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
</code></pre>
<p>Visualize the results.</p>
<pre><code>autoplot(rf_tune)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-27-1.png" alt=""></p>
<p>Collect metrics.</p>
<pre><code>rf_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))

## # A tibble: 50 × 9
##     mtry trees min_n .metric  .estimator  mean     n std_err .config              
##    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1    14  1554     5 accuracy binary     0.837     5 0.00656 Preprocessor1_Model49
##  2     4   774    18 accuracy binary     0.837     5 0.0133  Preprocessor1_Model50
##  3     5  1736     8 accuracy binary     0.834     5 0.0111  Preprocessor1_Model46
##  4     8  1322     5 accuracy binary     0.832     5 0.00713 Preprocessor1_Model41
##  5     2  1078    30 accuracy binary     0.831     5 0.00727 Preprocessor1_Model12
##  6     4  1892    14 accuracy binary     0.831     5 0.00886 Preprocessor1_Model39
##  7     8   962     7 accuracy binary     0.829     5 0.00742 Preprocessor1_Model43
##  8     7   946     4 accuracy binary     0.826     5 0.00452 Preprocessor1_Model23
##  9     7  1262     3 accuracy binary     0.826     5 0.00710 Preprocessor1_Model47
## 10     9   544     9 accuracy binary     0.825     5 0.00673 Preprocessor1_Model11
## # … with 40 more rows
</code></pre>
<h2 id="fit-model-1">Fit model</h2>
<p>Use best config to fit model to training data.</p>
<pre><code>rf_fit &lt;- rf_wf %&gt;%
  finalize_workflow(select_best(rf_tune)) %&gt;%
  fit(train)
</code></pre>
<p>Check out accuracy on testing dataset to see if we overfitted.</p>
<pre><code>rf_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.786
</code></pre>
<p>Check out important features (aka predictors).</p>
<pre><code>library(vip)
finalize_model(
  x = rf_model,
  parameters = select_best(rf_tune)) %&gt;%
  set_engine(&quot;ranger&quot;, importance = &quot;permutation&quot;) %&gt;%
  fit(survived ~ ., data = juice(prep(rf_rec))) %&gt;%
  vip(geom = &quot;point&quot;)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-31-1.png" alt=""></p>
<h2 id="make-predictions-1">Make predictions</h2>
<p>Now we’re ready to predict survival for the holdout dataset and submit
to Kaggle.</p>
<pre><code>rf_wf %&gt;%
  finalize_workflow(select_best(rf_tune)) %&gt;%
  fit(dataset) %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/randomforest.csv&quot;)
</code></pre>
<p>I got and accuracy of 0.77990, a bit better than gradient boosting.</p>
<p>Let’s continue with 
<a href="https://en.wikipedia.org/wiki/Catboost" target="_blank" rel="noopener">cat boosting
methods</a>.</p>
<h1 id="gradient-boosting-algorithms---catboost">Gradient boosting algorithms - catboost</h1>
<h2 id="tuning-2">Tuning</h2>
<p>Set up defaults.</p>
<pre><code>mset &lt;- metric_set(accuracy) # metric is accuracy
control &lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
</code></pre>
<p>First a recipe.</p>
<pre><code>cb_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
</code></pre>
<p>Then specify a cat boosting model.</p>
<pre><code>library(treesnip)
cb_model &lt;- boost_tree(mode = &quot;classification&quot;,
                       engine = &quot;catboost&quot;,
                       mtry = tune(),
                       trees = tune(),
                       min_n = tune(),
                       tree_depth = tune(),
                       learn_rate = tune()) # parameters to be tuned
</code></pre>
<p>Now set our workflow.</p>
<pre><code>cb_wf &lt;- 
  workflow() %&gt;% 
  add_model(cb_model) %&gt;% 
  add_recipe(cb_rec)
</code></pre>
<p>Use cross-validation to evaluate our model with different param config.</p>
<pre><code>cb_tune &lt;- cb_wf %&gt;%
  tune_race_anova(
    train_5fold,
    grid = 30,
    param_info = cb_model %&gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
</code></pre>
<p>Visualize the results.</p>
<pre><code>autoplot(cb_tune)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-38-1.png" alt=""></p>
<p>Collect metrics.</p>
<pre><code>cb_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))

## # A tibble: 30 × 11
##     mtry trees min_n tree_depth learn_rate .metric  .estimator  mean     n std_err .config 
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   
##  1     1  1787    25          7   6.84e- 3 accuracy binary     0.835     5 0.0125  Preproc…
##  2    12  1885    18         15   2.06e- 3 accuracy binary     0.831     5 0.00602 Preproc…
##  3    13  1278     5          6   2.10e- 2 accuracy binary     0.826     5 0.00431 Preproc…
##  4     3  1681    22          5   5.42e- 3 accuracy binary     0.825     5 0.00507 Preproc…
##  5     9   303     2          8   9.94e- 2 accuracy binary     0.820     4 0.0120  Preproc…
##  6    11  1201    24         12   3.77e- 2 accuracy binary     0.812     3 0.00868 Preproc…
##  7    11   634    35         11   1.30e- 3 accuracy binary     0.805     3 0.0242  Preproc…
##  8     7  1648     4          7   2.38e- 4 accuracy binary     0.737     3 0.0115  Preproc…
##  9     1  1427    27         10   1.74e- 6 accuracy binary     0.378     3 0.0152  Preproc…
## 10     2   940    19          3   3.20e-10 accuracy binary     0.378     3 0.0152  Preproc…
## # … with 20 more rows
</code></pre>
<h2 id="fit-model-2">Fit model</h2>
<p>Use best config to fit model to training data.</p>
<pre><code>cb_fit &lt;- cb_wf %&gt;%
  finalize_workflow(select_best(cb_tune)) %&gt;%
  fit(train)
</code></pre>
<p>Check out accuracy on testing dataset to see if we overfitted.</p>
<pre><code>cb_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.808
</code></pre>
<h2 id="make-predictions-2">Make predictions</h2>
<p>Now we’re ready to predict survival for the holdout dataset and submit
to Kaggle.</p>
<pre><code>cb_wf %&gt;%
  finalize_workflow(select_best(cb_tune)) %&gt;%
  fit(dataset) %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/catboost.csv&quot;)
</code></pre>
<p>I got and accuracy of 0.76076. Cool.</p>
<h1 id="regularization-methods">Regularization methods</h1>
<p>Let’s continue with 
<a href="https://en.wikipedia.org/wiki/Elastic_net_regularization" target="_blank" rel="noopener">elastic net
regularization</a>.</p>
<h2 id="tuning-3">Tuning</h2>
<p>First a recipe.</p>
<pre><code>en_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_normalize(all_numeric_predictors()) %&gt;% # normalize
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
</code></pre>
<p>Then specify a regularization model. We tune parameter mixture, with
ridge regression for mixture = 0, and lasso for mixture = 1.</p>
<pre><code>en_model &lt;- logistic_reg(penalty = tune(), 
                         mixture = tune()) %&gt;% # param to be tuned
  set_engine(&quot;glmnet&quot;) %&gt;% # elastic net
  set_mode(&quot;classification&quot;) # binary response
</code></pre>
<p>Now set our workflow.</p>
<pre><code>en_wf &lt;- 
  workflow() %&gt;% 
  add_model(en_model) %&gt;% 
  add_recipe(en_rec)
</code></pre>
<p>Use cross-validation to evaluate our model with different param config.</p>
<pre><code>en_tune &lt;- en_wf %&gt;%
  tune_grid(train_5fold,
            metrics = mset,
            control = control,
            grid = crossing(penalty = 10 ^ seq(-8, -.5, .5),
                            mixture = seq(0, 1, length.out = 10)))
</code></pre>
<p>Visualize the results.</p>
<pre><code>autoplot(en_tune)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-47-1.png" alt=""></p>
<p>Collect metrics.</p>
<pre><code>en_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))

## # A tibble: 160 × 8
##         penalty mixture .metric  .estimator  mean     n std_err .config               
##           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 
##  1 0.00000001     0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model017
##  2 0.0000000316   0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model018
##  3 0.0000001      0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model019
##  4 0.000000316    0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model020
##  5 0.000001       0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model021
##  6 0.00000316     0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model022
##  7 0.00001        0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model023
##  8 0.0000316      0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model024
##  9 0.0001         0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model025
## 10 0.000316       0.111 accuracy binary     0.831     5  0.0112 Preprocessor1_Model026
## # … with 150 more rows
</code></pre>
<h2 id="fit-model-3">Fit model</h2>
<p>Use best config to fit model to training data.</p>
<pre><code>en_fit &lt;- en_wf %&gt;%
  finalize_workflow(select_best(en_tune)) %&gt;%
  fit(train)
</code></pre>
<p>Check out accuracy on testing dataset to see if we overfitted.</p>
<pre><code>en_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.826
</code></pre>
<p>Check out important features (aka predictors).</p>
<pre><code>library(broom)
en_fit$fit$fit$fit %&gt;%
  tidy() %&gt;%
  filter(lambda &gt;= select_best(en_tune)$penalty) %&gt;%
  filter(lambda == min(lambda),
         term != &quot;(Intercept)&quot;) %&gt;%
  mutate(term = fct_reorder(term, estimate)) %&gt;%
  ggplot(aes(estimate, term, fill = estimate &gt; 0)) +
  geom_col() +
  theme(legend.position = &quot;none&quot;)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-51-1.png" alt=""></p>
<h2 id="make-predictions-3">Make predictions</h2>
<p>Now we’re ready to predict survival for the holdout dataset and submit
to Kaggle.</p>
<pre><code>en_wf %&gt;%
  finalize_workflow(select_best(en_tune)) %&gt;%
  fit(dataset) %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/regularization.csv&quot;)
</code></pre>
<p>I got and accuracy of 0.76315.</p>
<h1 id="logistic-regression">Logistic regression</h1>
<p>And what about a good old-fashioned logistic regression (not a ML algo)?</p>
<p>First a recipe.</p>
<pre><code>logistic_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_normalize(all_numeric_predictors()) %&gt;% # normalize
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
</code></pre>
<p>Then specify a logistic regression.</p>
<pre><code>logistic_model &lt;- logistic_reg() %&gt;% # no param to be tuned
  set_engine(&quot;glm&quot;) %&gt;% # elastic net
  set_mode(&quot;classification&quot;) # binary response
</code></pre>
<p>Now set our workflow.</p>
<pre><code>logistic_wf &lt;- 
  workflow() %&gt;% 
  add_model(logistic_model) %&gt;% 
  add_recipe(logistic_rec)
</code></pre>
<p>Fit model.</p>
<pre><code>logistic_fit &lt;- logistic_wf %&gt;%
  fit(train)
</code></pre>
<p>Inspect significant features (aka predictors).</p>
<pre><code>tidy(logistic_fit, exponentiate = TRUE) %&gt;%
  filter(p.value &lt; 0.05)

## # A tibble: 6 × 5
##   term         estimate std.error statistic       p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;
## 1 age             1.35      0.152      1.98 0.0473       
## 2 familyage       2.97      0.223      4.88 0.00000105   
## 3 class_first     0.136     0.367     -5.43 0.0000000559 
## 4 class_second    0.386     0.298     -3.19 0.00145      
## 5 title_Mr.      51.0       0.684      5.75 0.00000000912
## 6 title_Other    54.7       0.991      4.04 0.0000538
</code></pre>
<p>Same thing, but graphically.</p>
<pre><code>library(broom)
logistic_fit %&gt;%
  tidy() %&gt;%
  mutate(term = fct_reorder(term, estimate)) %&gt;%
  ggplot(aes(estimate, term, fill = estimate &gt; 0)) +
  geom_col() +
  theme(legend.position = &quot;none&quot;)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-58-1.png" alt=""></p>
<p>Check out accuracy on testing dataset to see if we overfitted.</p>
<pre><code>logistic_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.821
</code></pre>
<p>Confusion matrix.</p>
<pre><code>logistic_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  conf_mat(survived, .pred_class)

##           Truth
## Prediction yes  no
##        yes  59  13
##        no   27 125
</code></pre>
<p>ROC curve.</p>
<pre><code>logistic_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  roc_curve(truth = survived, estimate = .pred_yes) %&gt;%
  autoplot()
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-61-1.png" alt=""></p>
<p>Now we’re ready to predict survival for the holdout dataset and submit
to Kaggle.</p>
<pre><code>logistic_wf %&gt;%
  fit(dataset) %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/logistic.csv&quot;)
</code></pre>
<p>I got and accuracy of 0.76076. Oldies but goodies!</p>
<h1 id="neural-networks">Neural networks</h1>
<p>We go on with 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network" target="_blank" rel="noopener">neural
networks</a>.</p>
<h2 id="tuning-4">Tuning</h2>
<p>Set up defaults.</p>
<pre><code>mset &lt;- metric_set(accuracy) # metric is accuracy
control &lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
</code></pre>
<p>First a recipe.</p>
<pre><code>nn_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_normalize(all_numeric_predictors()) %&gt;%
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
</code></pre>
<p>Then specify a neural network.</p>
<pre><code>nn_model &lt;- mlp(epochs = tune(), 
                hidden_units = tune(), 
                dropout = tune()) %&gt;% # param to be tuned
  set_mode(&quot;classification&quot;) %&gt;% # binary response var
  set_engine(&quot;keras&quot;, verbose = 0)
</code></pre>
<p>Now set our workflow.</p>
<pre><code>nn_wf &lt;- 
  workflow() %&gt;% 
  add_model(nn_model) %&gt;% 
  add_recipe(nn_rec)
</code></pre>
<p>Use cross-validation to evaluate our model with different param config.</p>
<pre><code>nn_tune &lt;- nn_wf %&gt;%
  tune_race_anova(
    train_5fold,
    grid = 30,
    param_info = nn_model %&gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
</code></pre>
<p>Visualize the results.</p>
<pre><code>autoplot(nn_tune)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-68-1.png" alt=""></p>
<p>Collect metrics.</p>
<pre><code>nn_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))

## # A tibble: 30 × 9
##    hidden_units dropout epochs .metric  .estimator  mean     n std_err .config             
##           &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
##  1           10 0.484      405 accuracy binary     0.838     5 0.0150  Preprocessor1_Model…
##  2            7 0.0597     220 accuracy binary     0.834     5 0.00881 Preprocessor1_Model…
##  3            4 0.536      629 accuracy binary     0.834     5 0.0154  Preprocessor1_Model…
##  4            9 0.198      768 accuracy binary     0.832     5 0.0146  Preprocessor1_Model…
##  5            6 0.752      822 accuracy binary     0.832     5 0.0112  Preprocessor1_Model…
##  6            9 0.406      293 accuracy binary     0.831     5 0.0145  Preprocessor1_Model…
##  7            4 0.00445    871 accuracy binary     0.831     5 0.0150  Preprocessor1_Model…
##  8            4 0.293      353 accuracy binary     0.831     5 0.0117  Preprocessor1_Model…
##  9           10 0.675      935 accuracy binary     0.831     5 0.0141  Preprocessor1_Model…
## 10            7 0.128      580 accuracy binary     0.831     5 0.0151  Preprocessor1_Model…
## # … with 20 more rows
</code></pre>
<h2 id="fit-model-4">Fit model</h2>
<p>Use best config to fit model to training data.</p>
<pre><code>nn_fit &lt;- nn_wf %&gt;%
  finalize_workflow(select_best(nn_tune)) %&gt;%
  fit(train)
</code></pre>
<p>Check out accuracy on testing dataset to see if we overfitted.</p>
<pre><code>nn_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.817
</code></pre>
<h2 id="make-predictions-4">Make predictions</h2>
<p>Now we’re ready to predict survival for the holdout dataset and submit
to Kaggle.</p>
<pre><code>nn_wf %&gt;%
  finalize_workflow(select_best(nn_tune)) %&gt;%
  fit(train) %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/nn.csv&quot;)
</code></pre>
<p>I got and accuracy of 0.78708. My best score so far.</p>
<h1 id="support-vector-machines">Support vector machines</h1>
<p>We go on with 
<a href="https://en.wikipedia.org/wiki/Support-vector_machine" target="_blank" rel="noopener">support vector
machines</a>.</p>
<h2 id="tuning-5">Tuning</h2>
<p>Set up defaults.</p>
<pre><code>mset &lt;- metric_set(accuracy) # metric is accuracy
control &lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
</code></pre>
<p>First a recipe.</p>
<pre><code>svm_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  # remove any zero variance predictors
  step_zv(all_predictors()) %&gt;% 
  # remove any linear combinations
  step_lincomb(all_numeric()) %&gt;%
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
</code></pre>
<p>Then specify a svm.</p>
<pre><code>svm_model &lt;- svm_rbf(cost = tune(), 
                     rbf_sigma = tune()) %&gt;% # param to be tuned
  set_mode(&quot;classification&quot;) %&gt;% # binary response var
  set_engine(&quot;kernlab&quot;)
</code></pre>
<p>Now set our workflow.</p>
<pre><code>svm_wf &lt;- 
  workflow() %&gt;% 
  add_model(svm_model) %&gt;% 
  add_recipe(svm_rec)
</code></pre>
<p>Use cross-validation to evaluate our model with different param config.</p>
<pre><code>svm_tune &lt;- svm_wf %&gt;%
  tune_race_anova(
    train_5fold,
    grid = 30,
    param_info = svm_model %&gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
</code></pre>
<p>Visualize the results.</p>
<pre><code>autoplot(svm_tune)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-78-1.png" alt=""></p>
<p>Collect metrics.</p>
<pre><code>svm_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))

## # A tibble: 30 × 8
##        cost    rbf_sigma .metric  .estimator  mean     n std_err .config              
##       &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1 17.7     0.00183      accuracy binary     0.829     5 0.0101  Preprocessor1_Model05
##  2  0.741   0.0506       accuracy binary     0.823     5 0.0129  Preprocessor1_Model14
##  3  1.77    0.0429       accuracy binary     0.820     5 0.0143  Preprocessor1_Model08
##  4  0.229   0.0197       accuracy binary     0.808     3 0.00626 Preprocessor1_Model23
##  5  1.15    0.00285      accuracy binary     0.795     3 0.00867 Preprocessor1_Model28
##  6  0.00182 0.00000203   accuracy binary     0.613     3 0.0171  Preprocessor1_Model01
##  7  0.0477  0.714        accuracy binary     0.613     3 0.0171  Preprocessor1_Model02
##  8  6.60    0.0000294    accuracy binary     0.613     3 0.0171  Preprocessor1_Model03
##  9  0.00254 0.000636     accuracy binary     0.613     3 0.0171  Preprocessor1_Model04
## 10  0.00544 0.0000000647 accuracy binary     0.613     3 0.0171  Preprocessor1_Model06
## # … with 20 more rows
</code></pre>
<h2 id="fit-model-5">Fit model</h2>
<p>Use best config to fit model to training data.</p>
<pre><code>svm_fit &lt;- svm_wf %&gt;%
  finalize_workflow(select_best(svm_tune)) %&gt;%
  fit(train)
</code></pre>
<p>Check out accuracy on testing dataset to see if we overfitted.</p>
<pre><code>svm_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.826
</code></pre>
<h2 id="make-predictions-5">Make predictions</h2>
<p>Now we’re ready to predict survival for the holdout dataset and submit
to Kaggle.</p>
<pre><code>svm_wf %&gt;%
  finalize_workflow(select_best(svm_tune)) %&gt;%
  fit(dataset) %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/svm.csv&quot;)
</code></pre>
<p>I got and accuracy of 0.77511.</p>
<h1 id="decision-trees">Decision trees</h1>
<p>We go on with 
<a href="https://en.wikipedia.org/wiki/Decision_tree" target="_blank" rel="noopener">decision
trees</a>.</p>
<h2 id="tuning-6">Tuning</h2>
<p>Set up defaults.</p>
<pre><code>mset &lt;- metric_set(accuracy) # metric is accuracy
control &lt;- control_grid(save_workflow = TRUE,
                        save_pred = TRUE,
                        extract = extract_model) # grid for tuning
</code></pre>
<p>First a recipe.</p>
<pre><code>dt_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_zv(all_predictors()) %&gt;% 
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
</code></pre>
<p>Then specify a decision tree model.</p>
<pre><code>library(baguette)
dt_model &lt;- bag_tree(cost_complexity = tune(),
                     tree_depth = tune(),
                     min_n = tune()) %&gt;% # param to be tuned
  set_engine(&quot;rpart&quot;, times = 25) %&gt;% # nb bootstraps
  set_mode(&quot;classification&quot;) # binary response var
</code></pre>
<p>Now set our workflow.</p>
<pre><code>dt_wf &lt;- 
  workflow() %&gt;% 
  add_model(dt_model) %&gt;% 
  add_recipe(dt_rec)
</code></pre>
<p>Use cross-validation to evaluate our model with different param config.</p>
<pre><code>dt_tune &lt;- dt_wf %&gt;%
  tune_race_anova(
    train_5fold,
    grid = 30,
    param_info = dt_model %&gt;% parameters(),
    metrics = metric_set(accuracy),
    control = control_race(verbose_elim = TRUE))
</code></pre>
<p>Visualize the results.</p>
<pre><code>autoplot(dt_tune)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-88-1.png" alt=""></p>
<p>Collect metrics.</p>
<pre><code>dt_tune %&gt;%
  collect_metrics() %&gt;%
  arrange(desc(mean))

## # A tibble: 30 × 9
##    cost_complexity tree_depth min_n .metric  .estimator  mean     n std_err .config        
##              &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          
##  1        3.02e- 7         13     7 accuracy binary     0.832     5 0.0130  Preprocessor1_…
##  2        3.43e- 3         14     4 accuracy binary     0.829     5 0.0145  Preprocessor1_…
##  3        5.75e- 5          5     7 accuracy binary     0.828     5 0.0126  Preprocessor1_…
##  4        2.13e- 6         14     5 accuracy binary     0.828     5 0.0108  Preprocessor1_…
##  5        1.34e- 5          5    35 accuracy binary     0.823     5 0.00654 Preprocessor1_…
##  6        4.47e- 5         13    10 accuracy binary     0.823     5 0.0120  Preprocessor1_…
##  7        1.42e- 2          4    25 accuracy binary     0.822     5 0.0121  Preprocessor1_…
##  8        4.54e-10         10    36 accuracy binary     0.822     5 0.0147  Preprocessor1_…
##  9        8.10e- 8         11    32 accuracy binary     0.822     5 0.0143  Preprocessor1_…
## 10        3.43e- 4         11    21 accuracy binary     0.820     5 0.0207  Preprocessor1_…
## # … with 20 more rows
</code></pre>
<h2 id="fit-model-6">Fit model</h2>
<p>Use best config to fit model to training data.</p>
<pre><code>dt_fit &lt;- dt_wf %&gt;%
  finalize_workflow(select_best(dt_tune)) %&gt;%
  fit(train)
</code></pre>
<p>Check out accuracy on testing dataset to see if we overfitted.</p>
<pre><code>dt_fit %&gt;%
  augment(test, type.predict = &quot;response&quot;) %&gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.808
</code></pre>
<h2 id="make-predictions-6">Make predictions</h2>
<p>Now we’re ready to predict survival for the holdout dataset and submit
to Kaggle.</p>
<pre><code>dt_wf %&gt;%
  finalize_workflow(select_best(dt_tune)) %&gt;%
  fit(dataset) %&gt;%
  augment(holdout) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/dt.csv&quot;)
</code></pre>
<p>I got and accuracy of 0.76794.</p>
<h1 id="stacked-ensemble-modelling">Stacked ensemble modelling</h1>
<p>Let’s do some ensemble modelling with all algo but logistic and
catboost. Tune again with a probability-based metric. Start with
xgboost.</p>
<pre><code>library(finetune)
library(stacks)
# xgboost
xg_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_dummy(all_nominal_predictors()) # all factors var are split into binary terms (factor disj coding)
xg_model &lt;- boost_tree(mode = &quot;classification&quot;, # binary response
                       trees = tune(),
                       mtry = tune(),
                       tree_depth = tune(),
                       learn_rate = tune(),
                       loss_reduction = tune(),
                       min_n = tune()) # parameters to be tuned
xg_wf &lt;- 
  workflow() %&gt;% 
  add_model(xg_model) %&gt;% 
  add_recipe(xg_rec)
xg_grid &lt;- grid_latin_hypercube(
  trees(),
  finalize(mtry(), train),
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  min_n(),
  size = 30)
xg_tune &lt;- xg_wf %&gt;%
  tune_grid(
    resamples = train_5fold,
    grid = xg_grid,
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
</code></pre>
<p>Then random forests.</p>
<pre><code># random forest
rf_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% 
  step_dummy(all_nominal_predictors()) 
rf_model &lt;- rand_forest(mode = &quot;classification&quot;, # binary response
                        engine = &quot;ranger&quot;, # by default
                        mtry = tune(),
                        trees = tune(),
                        min_n = tune()) # parameters to be tuned
rf_wf &lt;- 
  workflow() %&gt;% 
  add_model(rf_model) %&gt;% 
  add_recipe(rf_rec)
rf_grid &lt;- grid_latin_hypercube(
  finalize(mtry(), train),
  trees(),
  min_n(),
  size = 30)
rf_tune &lt;- rf_wf %&gt;%
  tune_grid(
    resamples = train_5fold,
    grid = rf_grid,
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
</code></pre>
<p>Regularisation methods (between ridge and lasso).</p>
<pre><code># regularization methods
en_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_normalize(all_numeric_predictors()) %&gt;% # normalize
  step_dummy(all_nominal_predictors()) 
en_model &lt;- logistic_reg(penalty = tune(), 
                         mixture = tune()) %&gt;% # param to be tuned
  set_engine(&quot;glmnet&quot;) %&gt;% # elastic net
  set_mode(&quot;classification&quot;) # binary response
en_wf &lt;- 
  workflow() %&gt;% 
  add_model(en_model) %&gt;% 
  add_recipe(en_rec)
en_grid &lt;- grid_latin_hypercube(
  penalty(),
  mixture(),
  size = 30)
en_tune &lt;- en_wf %&gt;%
  tune_grid(
    resamples = train_5fold,
    grid = en_grid,
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
</code></pre>
<p>Neural networks (takes time, so pick only a few values for illustration
purpose).</p>
<pre><code># neural networks
nn_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% # replace missing value by median
  step_normalize(all_numeric_predictors()) %&gt;%
  step_dummy(all_nominal_predictors()) 
nn_model &lt;- mlp(epochs = tune(), 
                hidden_units = 2, 
                dropout = tune()) %&gt;% # param to be tuned
  set_mode(&quot;classification&quot;) %&gt;% # binary response var
  set_engine(&quot;keras&quot;, verbose = 0)
nn_wf &lt;- 
  workflow() %&gt;% 
  add_model(nn_model) %&gt;% 
  add_recipe(nn_rec)
# nn_grid &lt;- grid_latin_hypercube(
#   epochs(),
#   hidden_units(),
#   dropout(),
#   size = 10)
nn_tune &lt;- nn_wf %&gt;%
  tune_grid(
    resamples = train_5fold,
    grid = crossing(dropout = c(0.1, 0.2), epochs = c(250, 500, 1000)), # nn_grid
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
#autoplot(nn_tune)
</code></pre>
<p>Support vector machines.</p>
<pre><code># support vector machines
svm_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% 
  step_normalize(all_numeric_predictors()) %&gt;%
  step_dummy(all_nominal_predictors()) 
svm_model &lt;- svm_rbf(cost = tune(), 
                     rbf_sigma = tune()) %&gt;% # param to be tuned
  set_mode(&quot;classification&quot;) %&gt;% # binary response var
  set_engine(&quot;kernlab&quot;)
svm_wf &lt;- 
  workflow() %&gt;% 
  add_model(svm_model) %&gt;% 
  add_recipe(svm_rec)
svm_grid &lt;- grid_latin_hypercube(
  cost(),
  rbf_sigma(),
  size = 30)
svm_tune &lt;- svm_wf %&gt;%
  tune_grid(
    resamples = train_5fold,
    grid = svm_grid,
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
</code></pre>
<p>Last, decision trees.</p>
<pre><code># decision trees
dt_rec &lt;- recipe(survived ~ ., data = train) %&gt;%
  step_impute_median(all_numeric()) %&gt;% 
  step_zv(all_predictors()) %&gt;% 
  step_dummy(all_nominal_predictors()) 
library(baguette)
dt_model &lt;- bag_tree(cost_complexity = tune(),
                     tree_depth = tune(),
                     min_n = tune()) %&gt;% # param to be tuned
  set_engine(&quot;rpart&quot;, times = 25) %&gt;% # nb bootstraps
  set_mode(&quot;classification&quot;) # binary response var
dt_wf &lt;- 
  workflow() %&gt;% 
  add_model(dt_model) %&gt;% 
  add_recipe(dt_rec)
dt_grid &lt;- grid_latin_hypercube(
  cost_complexity(),
  tree_depth(),
  min_n(),
  size = 30)
dt_tune &lt;- dt_wf %&gt;%
  tune_grid(
    resamples = train_5fold,
    grid = dt_grid,
    metrics = metric_set(roc_auc),
    control = control_stack_grid())
</code></pre>
<p>Get best config.</p>
<pre><code>xg_best &lt;- xg_tune %&gt;% filter_parameters(parameters = select_best(xg_tune))
rf_best &lt;- rf_tune %&gt;% filter_parameters(parameters = select_best(rf_tune))
en_best &lt;- en_tune %&gt;% filter_parameters(parameters = select_best(en_tune))
nn_best &lt;- nn_tune %&gt;% filter_parameters(parameters = select_best(nn_tune))
svm_best &lt;- svm_tune %&gt;% filter_parameters(parameters = select_best(svm_tune))
dt_best &lt;- dt_tune %&gt;% filter_parameters(parameters = select_best(dt_tune))
</code></pre>
<p>Do the stacked ensemble modelling.</p>
<p>Pile all models together.</p>
<pre><code>blended &lt;- stacks() %&gt;% # initialize
  add_candidates(en_best) %&gt;% # add regularization model
  add_candidates(xg_best) %&gt;% # add gradient boosting
  add_candidates(rf_best) %&gt;% # add random forest
  add_candidates(nn_best) %&gt;% # add neural network
  add_candidates(svm_best) %&gt;% # add svm
  add_candidates(dt_best) # add decision trees
blended

## # A data stack with 6 model definitions and 6 candidate members:
## #   en_best: 1 model configuration
## #   xg_best: 1 model configuration
## #   rf_best: 1 model configuration
## #   nn_best: 1 model configuration
## #   svm_best: 1 model configuration
## #   dt_best: 1 model configuration
## # Outcome: survived (factor)
</code></pre>
<p>Fit regularized model.</p>
<pre><code>blended_fit &lt;- blended %&gt;%
  blend_predictions() # fit regularized model
</code></pre>
<p>Visualise penalized model. Note that neural networks are dropped,
despite achieving best score when used in isolation. I’ll have to dig
into that.</p>
<pre><code>autoplot(blended_fit)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-102-1.png" alt=""></p>
<pre><code>autoplot(blended_fit, type = &quot;members&quot;)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-102-2.png" alt=""></p>
<pre><code>autoplot(blended_fit, type = &quot;weights&quot;)
</code></pre>
<p><img src="/blog/learning-machine-learning/unnamed-chunk-102-3.png" alt=""></p>
<p>Fit candidate members with non-zero stacking coef with full training
dataset.</p>
<pre><code>blended_regularized &lt;- blended_fit %&gt;%
  fit_members() 
blended_regularized

## # A tibble: 3 × 3
##   member                type         weight
##   &lt;chr&gt;                 &lt;chr&gt;         &lt;dbl&gt;
## 1 .pred_no_en_best_1_12 logistic_reg  2.67 
## 2 .pred_no_dt_best_1_11 bag_tree      1.95 
## 3 .pred_no_rf_best_1_05 rand_forest   0.922
</code></pre>
<p>Perf on testing dataset?</p>
<pre><code>test %&gt;%
  bind_cols(predict(blended_regularized, .)) %&gt;%
  accuracy(survived, .pred_class)

## # A tibble: 1 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.826
</code></pre>
<p>Now predict.</p>
<pre><code>holdout %&gt;%
  bind_cols(predict(blended_regularized, .)) %&gt;%
  select(PassengerId, Survived = .pred_class) %&gt;%
  mutate(Survived = if_else(Survived == &quot;yes&quot;, 1, 0)) %&gt;%
  write_csv(&quot;output/titanic/stacked.csv&quot;)
</code></pre>
<p>I got an 0.76076 accuracy.</p>
<h1 id="conclusions">Conclusions</h1>
<p>I covered several ML algorithms and logistic regression with the awesome
<code>tidymodels</code> metapackage in <code>R</code>. My scores at predicting Titanic
survivors were ok I guess. Some folks on Kaggle got a perfect accuracy,
so there is always room for improvement. Maybe better tuning, better
features (or predictors) or other algorithms would increase accuracy. Of
course, I forgot to use <code>set.seed()</code> so results are not exactly
reproducible.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>This post was also published on 
<a href="https://www.r-bloggers.com" target="_blank" rel="noopener">https://www.r-bloggers.com</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/machine-learning/">machine learning</a>
  
  <a class="badge badge-light" href="/tags/tidymodels/">tidymodels</a>
  
  <a class="badge badge-light" href="/tags/r/">R</a>
  
  <a class="badge badge-light" href="/tags/rstats/">rstats</a>
  
</div>














  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/authors/authors/avatar_hu_6b27bb3302803439.jpg" alt="">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://oliviergimenez.github.io/"></a></h5>
        <h6 class="card-subtitle">Statistics, ecology &amp; social sciences</h6>
        <p class="card-text">I&rsquo;m a scientist working at the interface of animal ecology, statistical modeling and social sciences.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/oliviergimenez" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://bsky.app/profile/oaggimenez.bsky.social" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.youtube.com/channel/UCFtTq-4WwH0LIczQ__KFC1A/videos" target="_blank" rel="noopener">
        <i class="fab fa-youtube"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.fr/citations?user=5NkQUA8AAAAJ&amp;hl=fr" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://figshare.com/authors/Olivier_Gimenez/3914297" target="_blank" rel="noopener">
        <i class="ai ai-figshare"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/olivier-gimenez-545451115/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  







<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "https-oliviergimenez-github-io" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>






  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/blog/twitter-social-network/">Quick and dirty analysis of a Twitter social network</a></li>
      
      <li><a href="/blog/articlenina/">New paper!</a></li>
      
      <li><a href="/blog/inla_workshop/">Workshop on spatio-temporal models with INLA</a></li>
      
      <li><a href="/blog/crashcourse_netlogor/">Crash course on individual-based models using NetLogoR</a></li>
      
      <li><a href="/blog/run_openbugs_parallel/">Running OpenBUGS in parallel</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.625093851a77bbf06d06d7f8a6cf3f7b.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">

  <p class="powered-by">
    © Olivier Gimenez 2025 &middot; 

    Built w/ <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic</a> for <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>, adapted from <a href="https://juliasilge.com/" target="_blank" rel="noopener">Julia Silge's blog</a> & hosted on <a href="https://pages.github.com/" target="_blank" rel="noopener">Github Pages</a>. Purple photo <a href="https://unsplash.com/@isaacquesada?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Isaac Quesada</a> on <a href="https://unsplash.com/s/photos/purple?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
